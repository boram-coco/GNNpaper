[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GNNpaper",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nNov 11, 2023\n\n\n[FRAUD] 그래프 그림 그리기(graft_정리)\n\n\n김보람 \n\n\n\n\nNov 10, 2023\n\n\n[FRAUD] 그래프 그림 그리기(graft)\n\n\n김보람 \n\n\n\n\nNov 9, 2023\n\n\n[Essays] graft\n\n\n신록예찬 \n\n\n\n\nNov 9, 2023\n\n\n[Essays] graft\n\n\n신록예찬 \n\n\n\n\nNov 6, 2023\n\n\n[FRAUD] : &lt;mean_\n\n\n김보람 \n\n\n\n\nNov 2, 2023\n\n\n[FRAUD] 데이터정리\n\n\n김보람 \n\n\n\n\nNov 1, 2023\n\n\n[FRAUD] 그래프 그림 그리기\n\n\n김보람 \n\n\n\n\nOct 20, 2023\n\n\n[FRAUD] 책_코드\n\n\n김보람 \n\n\n\n\nOct 20, 2023\n\n\n[FRAUD] 책_코드\n\n\n김보람 \n\n\n\n\nOct 20, 2023\n\n\n[FRAUD] 데이터정리 시도(GCN_엣지손x)\n\n\n김보람 \n\n\n\n\nOct 11, 2023\n\n\n[FRAUD] 데이터정리 시도(df50 X범주_auto_best)\n\n\n김보람 \n\n\n\n\nOct 11, 2023\n\n\n[FRAUD] 데이터정리 시도(df50 X범주_auto)\n\n\n김보람 \n\n\n\n\nOct 11, 2023\n\n\n[FRAUD] 데이터정리 시도(GCNConv3추가,dropout조정)\n\n\n김보람 \n\n\n\n\nOct 10, 2023\n\n\n[FRAUD] 데이터정리 시도(GCN_X범주)\n\n\n김보람 \n\n\n\n\nOct 2, 2023\n\n\n[FRAUD] 데이터정리 시도(GCN)\n\n\n김보람 \n\n\n\n\nSep 30, 2023\n\n\n[FRAUD] 데이터정리 시도(9.30_df50 autogluon_best)\n\n\n김보람 \n\n\n\n\nSep 20, 2023\n\n\n[FRAUD] 데이터정리 시도(9.20_df50 autogluon, seed)\n\n\n김보람 \n\n\n\n\nSep 20, 2023\n\n\n[FRAUD] 데이터정리 시도(9.20_df50 autogluon)\n\n\n김보람 \n\n\n\n\nSep 18, 2023\n\n\n[FRAUD] 데이터정리 시도(df50다른것들 시드고정)\n\n\n김보람 \n\n\n\n\nSep 18, 2023\n\n\n[FRAUD] 데이터정리 시도(9.18_df50 정리중(~ing))\n\n\n김보람 \n\n\n\n\nSep 13, 2023\n\n\n[FRAUD] 데이터 (9.13_df50 edge다르게)\n\n\n김보람 \n\n\n\n\nSep 5, 2023\n\n\n[FRAUD] df02_f1 0.259\n\n\n김보람 \n\n\n\n\nSep 5, 2023\n\n\n[FRAUD] df02 accuracy 0.9707 f1은 망함\n\n\n김보람 \n\n\n\n\nSep 5, 2023\n\n\n[FRAUD] df02 accuracy 0.9707 f1은 망함\n\n\n김보람 \n\n\n\n\nAug 25, 2023\n\n\n[FRAUD] 데이터정리 시도(8.25_df02 커널 죽음)\n\n\n김보람 \n\n\n\n\nAug 23, 2023\n\n\n[FRAUD] 데이터정리 시도(8.23_df50 다시)\n\n\n김보람 \n\n\n\n\nAug 22, 2023\n\n\n[FRAUD] 데이터정리 시도(matrix로 lesson6따라하기 - 실패ㅎ)\n\n\n김보람 \n\n\n\n\nAug 22, 2023\n\n\n[FRAUD] 데이터정리 시도(8.22 df02시도)\n\n\n김보람 \n\n\n\n\nAug 18, 2023\n\n\n[FRAUD] 데이터 정리 시도(8.18-망함 df50을 tr/test로 분리했다가 다시 합쳐봄..)\n\n\n김보람 \n\n\n\n\nAug 18, 2023\n\n\n[FRAUD] 데이터정리 시도(8.18-df50com으로 93퍼 accuracy)\n\n\n김보람 \n\n\n\n\nAug 14, 2023\n\n\n[FRAUD] 데이터정리 시도(8.14-망함 tr/test_mask 만들어봄)\n\n\n김보람 \n\n\n\n\nAug 10, 2023\n\n\n[FRAUD] 데이터정리 시도(1, tr로만 96퍼 accuracy)\n\n\n김보람 \n\n\n\n\nAug 10, 2023\n\n\n[FRAUD] 데이터정리\n\n\n김보람 \n\n\n\n\nAug 10, 2023\n\n\n[FRAUD] 그래프자료로 데이터정리\n\n\n김보람 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/231101.html",
    "href": "posts/231101.html",
    "title": "[FRAUD] 그래프 그림 그리기",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\nimport xgboost as xgb\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# gnn\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric\nfrom torch_geometric.nn import GCNConv\n\n\n\ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef compute_time_difference(group):\n    n = len(group)\n    result = []\n    for i in range(n):\n        for j in range(n):\n            time_difference = abs(group.iloc[i].trans_date_trans_time.value - group.iloc[j].trans_date_trans_time.value)\n            result.append([group.iloc[i].name, group.iloc[j].name, time_difference])\n    return result\n\ndef mask(df):\n    df_tr,df_test = sklearn.model_selection.train_test_split(df, random_state=42)\n    N = len(df)\n    train_mask = [i in df_tr.index for i in range(N)]\n    test_mask = [i in df_test.index for i in range(N)]\n    train_mask = np.array(train_mask)\n    test_mask = np.array(test_mask)\n    return train_mask, test_mask\n\ndef edge_index_selected(edge_index):\n    theta = edge_index[:,2].mean()\n    edge_index[:,2] = (np.exp(-edge_index[:,2]/theta) != 1)*(np.exp(-edge_index[:,2]/theta))\n    edge_index = edge_index.tolist()\n    mean_ = np.array(edge_index)[:,2].mean()\n    selected_edges = [(int(row[0]), int(row[1])) for row in edge_index if row[2] &gt; mean_]\n    edge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\n    return edge_index_selected\n\n\n\n\nfraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\n\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50 = df50.reset_index()\ndf50.shape\n\n(12012, 23)\n\n\n\n\n\n\nmask(df50)\n\n(array([False,  True,  True, ...,  True, False,  True]),\n array([ True, False, False, ..., False,  True, False]))\n\n\n\ntrain_mask, test_mask = mask(df50)\n\n\n\n\n\n\n# groups = df50.groupby('cc_num')\n# edge_index_list_plus = [compute_time_difference(group) for _, group in groups]\n# edge_index_list_plus_flat = [item for sublist in edge_index_list_plus for item in sublist]\n# edge_index_list_plus_nparr = np.array(edge_index_list_plus_flat)\n# np.save('edge_index_list_plus50.npy', edge_index_list_plus_nparr)\n\n\nedge_index = np.load('edge_index_list_plus50.npy').astype(np.float64)\nedge_index.shape\n\n(200706, 3)\n\n\n\nedge_index_selected = edge_index_selected(edge_index)\n\n\nedge_index_selected\n\ntensor([[ 1023,  1023,  1023,  ..., 11944, 11944, 11944],\n        [ 1024,  1028,  1031,  ...,  4257,  9241,  9782]])\n\n\n\nnp.array(edge_index_selected)\n\narray([[ 1023,  1023,  1023, ..., 11944, 11944, 11944],\n       [ 1024,  1028,  1031, ...,  4257,  9241,  9782]])\n\n\n\n\n\n\n\n\n\nx = torch.tensor(df50['amt'], dtype=torch.float).reshape(-1,1)\ny = torch.tensor(df50['is_fraud'],dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index = edge_index_selected, y=y, train_mask = train_mask, test_mask = test_mask)\ndata\n\n\n\ntorch.manual_seed(202250926)\nclass GCN2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 32)\n        self.conv2 = GCNConv(32,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\nX = (data.x[data.train_mask]).numpy()\nXX = (data.x[data.test_mask]).numpy()\ny = (data.y[data.train_mask]).numpy()\nyy = (data.y[data.test_mask]).numpy()\n\nmodel = GCN2()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(400):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\nmodel.eval()\n\npred = model(data).argmax(dim=1)\nyyhat = pred[data.test_mask]\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results1= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석1'])\n_results1\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석1\n0.902098\n0.862478\n0.95913\n0.90824\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\n\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\n\n\npos = nx.spring_layout(G)\nlabels = {i: f\"{i}\\n{data.y[i].item()}\" for i in range(data.num_nodes)}\nnx.draw(G, pos, with_labels=True, node_color='lightblue', labels=labels)\nplt.show()\n\n\n\n\n\nanomaly_nodes = [i for i in range(data.num_nodes) if data.y[i].item() == 1]\n\n# 중요한 노드에 대한 레이블 설정\nlabels = {i: f\"{i}\\n{data.y[i].item()}\" for i in anomaly_nodes}\n\n# 중요한 노드 강조\nnode_color = ['lightblue' if i not in anomaly_nodes else 'red' for i in range(data.num_nodes)]\n\n# 그래프 시각화\nnx.draw(G, pos, with_labels=True, node_color=node_color, labels=labels)\nplt.show()\n\n\n\n\n\nG\n\n&lt;networkx.classes.graph.Graph at 0x7f3eaffc0880&gt;\n\n\n흠…..\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# PyTorch Geometric 그래프를 NetworkX 그래프로 변환\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\n\n# 노드 A와 B의 인덱스\nnode_A_index = 1023\nnode_B_index = 1024\n\n# 노드 A와 B 간의 경로 찾기\nshortest_path = nx.shortest_path(G, source=node_A_index, target=node_B_index)\n\n# 노드 A와 B 간의 경로로 이루어진 부분 그래프 추출\nsubgraph = G.subgraph(shortest_path)\n\n# 그래프 시각화\npos = nx.spring_layout(subgraph)  # 그래프 레이아웃 설정\n\n# 노드 그리기\nnx.draw_networkx_nodes(subgraph, pos, node_size=200)\n\n# 엣지 그리기\nnx.draw_networkx_edges(subgraph, pos)\n\n# 그래프 출력\nplt.axis('off')  # 축 숨기기\nplt.show()\n\n\n\n\n\nsubgraph = nx.ego_graph(G, 1023, radius=1)\n\n# 그래프 시각화\npos = nx.spring_layout(subgraph)  # 그래프 레이아웃 설정\n\n# 노드 그리기\nnx.draw_networkx_nodes(subgraph, pos, node_size=200)\n\n# 엣지 그리기\nnx.draw_networkx_edges(subgraph, pos)\n\n# 그래프 출력\nplt.axis('off')  # 축 숨기기\nplt.show()\n\n\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# PyTorch Geometric 그래프를 NetworkX 그래프로 변환\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\n\n# 노드 1023과 연결된 모든 노드와 엣지를 포함하는 부분 그래프 추출\nsubgraph = nx.ego_graph(G, 1023, radius=1)\n\n# 그래프 시각화\npos = nx.spring_layout(subgraph)  # 그래프 레이아웃 설정\n\n# 노드 그리기\nnx.draw_networkx_nodes(subgraph, pos, node_size=200)\n\n# 엣지 그리기\nnx.draw_networkx_edges(subgraph, pos)\n\n# 노드 인덱스 표시\nlabels = {node: str(node) for node in subgraph.nodes()}\nnx.draw_networkx_labels(subgraph, pos, labels, font_size=10)\n\n# 그래프 출력\nplt.axis('off')  # 축 숨기기\nplt.show()\n\n\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# PyTorch Geometric 그래프를 NetworkX 그래프로 변환\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\n\nsubgraph = nx.ego_graph(G, 9782, radius=1)\n\n# 그래프 시각화\npos = nx.spring_layout(subgraph)  # 그래프 레이아웃 설정\n\n# 노드 그리기\nnx.draw_networkx_nodes(subgraph, pos, node_size=200)\n\n# 엣지 그리기\nnx.draw_networkx_edges(subgraph, pos)\n\n# 노드 인덱스 표시\nlabels = {node: str(node) for node in subgraph.nodes()}\nnx.draw_networkx_labels(subgraph, pos, labels, font_size=10)\n\n# 그래프 출력\nplt.axis('off')  # 축 숨기기\nplt.show()\n\n\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# PyTorch Geometric 그래프를 NetworkX 그래프로 변환\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\n\n# 노드 1023과 연결된 모든 노드와 엣지를 포함하는 부분 그래프 추출\nsubgraph1 = nx.ego_graph(G, 1023, radius=1)\nsubgraph2 = nx.ego_graph(G, 9782, radius=1)\n\n# 그래프 시각화\npos1 = nx.spring_layout(subgraph1)  # 그래프 레이아웃 설정\npos2 = nx.spring_layout(subgraph2)\n\n# 노드 그리기\nnx.draw_networkx_nodes(subgraph1, pos1, node_size=200, node_color='b', label='Node 1023')\nnx.draw_networkx_nodes(subgraph2, pos2, node_size=200, node_color='g', label='Node 1024')\n\n# 엣지 그리기\nnx.draw_networkx_edges(subgraph1, pos1)\nnx.draw_networkx_edges(subgraph2, pos2)\n\n# 노드 인덱스 표시\nlabels1 = {node: str(node) for node in subgraph1.nodes()}\nlabels2 = {node: str(node) for node in subgraph2.nodes()}\nnx.draw_networkx_labels(subgraph1, pos1, labels1, font_size=10)\nnx.draw_networkx_labels(subgraph2, pos2, labels2, font_size=10)\n\n# 그래프 출력\nplt.axis('off')  # 축 숨기기\n\n# 레이블 표시\nplt.legend(loc='best')\n\nplt.show()\n\n\n\n\n\n# 노드 1023와 9782 간의 연결 확인\nare_connected = G.has_edge(1023, 9782)\n\nif are_connected:\n    print(\"Node 1023 and Node 9782 are connected.\")\nelse:\n    print(\"Node 1023 and Node 9782 are not connected.\")\n\nNode 1023 and Node 9782 are not connected.\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# PyTorch Geometric 그래프를 NetworkX 그래프로 변환\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\n\n# 노드 1023과 연결된 모든 노드와 엣지를 포함하는 부분 그래프 추출\nsubgraph1 = nx.ego_graph(G, 1023, radius=1)\nsubgraph2 = nx.ego_graph(G, 9782, radius=1)\n\n# 왼쪽 그래프 레이아웃 설정\npos1 = nx.spring_layout(subgraph1, pos=None, seed=42)  # 그래프 레이아웃 설정\n\n# 오른쪽 그래프 레이아웃 설정\npos2 = nx.spring_layout(subgraph2, pos=None, seed=43)\n\n# 노드 그리기\nnx.draw_networkx_nodes(subgraph1, pos1, node_size=200, node_color='b', label='Node 1023')\nnx.draw_networkx_nodes(subgraph2, pos2, node_size=200, node_color='g', label='Node 1024')\n\n# 엣지 그리기\nnx.draw_networkx_edges(subgraph1, pos1)\nnx.draw_networkx_edges(subgraph2, pos2)\n\n# 노드 인덱스 표시\nlabels1 = {node: str(node) for node in subgraph1.nodes()}\nlabels2 = {node: str(node) for node in subgraph2.nodes()}\nnx.draw_networkx_labels(subgraph1, pos1, labels1, font_size=10)\nnx.draw_networkx_labels(subgraph2, pos2, labels2, font_size=10)\n\n# 그래프 출력\nplt.axis('off')  # 축 숨기기\n\n# 레이블 표시\nplt.legend(loc='best')\n\nplt.show()\n\n\n\n\n\n\nlen(df50['cc_num'].unique())\n\n932\n\n\n\ndf50['cc_num'].value_counts()\n\n4.302480e+15    43\n1.800650e+14    36\n2.131740e+14    35\n2.720430e+15    34\n2.242540e+15    33\n                ..\n3.885950e+13     1\n4.026220e+12     1\n6.526450e+15     1\n4.972230e+15     1\n6.535330e+15     1\nName: cc_num, Length: 932, dtype: int64\n\n\n\ncc_num_counts = df50['cc_num'].value_counts()\nmean_counts = cc_num_counts.mean()\nmean_counts\n\n12.888412017167383\n\n\n\ncounts = df50['cc_num'].value_counts()\ncc_num_with_13_counts = counts[counts == 13].index\ncc_num_with_13_counts\n\nFloat64Index([3585740000000000.0, 4839040000000000.0,  370349000000000.0,\n              3583090000000000.0, 3534720000000000.0,  342351000000000.0,\n               213161000000000.0, 6011860000000000.0, 2288810000000000.0,\n                  560881000000.0,  346273000000000.0,    4760120000000.0,\n              2356280000000000.0,     501803000000.0, 2297450000000000.0,\n              4933460000000000.0, 3565940000000000.0, 3597980000000000.0,\n                36913600000000.0,     630423000000.0,    4746000000000.0,\n              3536820000000000.0, 2720890000000000.0,          4.537e+18,\n                 4755700000000.0,   38057500000000.0,         4.2929e+18,\n              6011230000000000.0, 4060580000000000.0, 3546900000000000.0,\n               345060000000000.0,   30596500000000.0, 3597340000000000.0,\n              4124540000000000.0, 4204240000000000.0,  375237000000000.0,\n                36153900000000.0, 4334230000000000.0, 6012000000000000.0,\n              4134460000000000.0, 4653880000000000.0, 4509140000000000.0,\n              3565420000000000.0],\n             dtype='float64')\n\n\n\ndf50[df50['cc_num'] ==  4.537e+18].index\n\nInt64Index([ 6298,  6714,  7704,  7823,  7910,  7990,  9145,  9806, 10338,\n            10910, 11621, 11847, 11926],\n           dtype='int64')\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\nselected_indices = df50[df50['cc_num'] ==  3585740000000000.0].index\nis_fraud_values = df50.loc[selected_indices, 'is_fraud'].tolist()\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\nsubgraph = G.subgraph(selected_indices)\nnode_colors = ['r' if node in selected_indices and is_fraud_values[selected_indices.get_loc(node)] == 1 else 'k' for node in subgraph.nodes()]\npos = nx.spring_layout(subgraph, seed=42)\nplt.figure(figsize=(10, 5))\nnx.draw_networkx_nodes(subgraph, pos, node_size=200, node_color=node_colors)\nnx.draw_networkx_edges(subgraph, pos, edge_color='gray')\nnx.draw_networkx_labels(subgraph, pos, font_size=10)\nplt.axis('off')\nplt.show()\n\n\n\n\n\nindex = df50[df50['cc_num'] == 3585740000000000.0].index\nis_fraud_value = df50.loc[index, 'is_fraud']\nis_fraud_value\n\n4437     1\n4443     1\n4446     1\n4449     1\n4451     1\n4452     1\n4455     1\n4465     1\n6914     0\n6974     0\n8624     0\n8830     0\n11375    0\nName: is_fraud, dtype: int64\n\n\n\ndf50[df50['cc_num'] == 4.302480e+15].index\n\nInt64Index([ 2293,  2294,  2295,  2296,  2297,  2298,  2299,  2300,  4709,\n             4711,  4718,  4720,  4728,  4729,  4730,  4733,  4734,  4736,\n             4739,  4742,  6081,  6751,  6855,  7025,  7554,  7817,  7946,\n             8173,  9152,  9166,  9548,  9708,  9773,  9807, 10198, 10828,\n            10909, 10953, 10962, 11273, 11330, 11562, 11946],\n           dtype='int64')\n\n\n\nindex = df50[df50['cc_num'] == 4.302480e+15].index\nis_fraud_value = df50.loc[index, 'is_fraud']\nis_fraud_value\n\n2293     1\n2294     1\n2295     1\n2296     1\n2297     1\n2298     1\n2299     1\n2300     1\n4709     1\n4711     1\n4718     1\n4720     1\n4728     1\n4729     1\n4730     1\n4733     1\n4734     1\n4736     1\n4739     1\n4742     1\n6081     0\n6751     0\n6855     0\n7025     0\n7554     0\n7817     0\n7946     0\n8173     0\n9152     0\n9166     0\n9548     0\n9708     0\n9773     0\n9807     0\n10198    0\n10828    0\n10909    0\n10953    0\n10962    0\n11273    0\n11330    0\n11562    0\n11946    0\nName: is_fraud, dtype: int64\n\n\n- cc_num=4.302480e+15 인 그래프\n해당 노드의 is_fraud=1 이면 빨간색으로 표시했다\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# 4.50~에 해당하는 행의 인덱스를 찾음\nselected_indices = df50[df50['cc_num'] == 4.302480e+15].index\n\n# 선택된 인덱스에 대한 is_fraud 값을 가져옴\nis_fraud_values = df50.loc[selected_indices, 'is_fraud'].tolist()\n\n# PyTorch Geometric 그래프를 NetworkX 그래프로 변환\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\n\n# 서브그래프를 추출\nsubgraph = G.subgraph(selected_indices)\n\n# 노드와 엣지 색상을 설정\nnode_colors = ['r' if node in selected_indices and is_fraud_values[selected_indices.get_loc(node)] == 1 else 'k' for node in subgraph.nodes()]\n\n# 그래프 레이아웃 설정\npos = nx.spring_layout(subgraph, seed=42)\n\n# 그래프 그리기\nplt.figure(figsize=(10, 5))\nnx.draw_networkx_nodes(subgraph, pos, node_size=200, node_color=node_colors)\nnx.draw_networkx_edges(subgraph, pos, edge_color='gray')\nnx.draw_networkx_labels(subgraph, pos, font_size=10)\n\n# 축 숨기기\nplt.axis('off')\n\n# 그래프 출력\nplt.show()\n\n\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# 4.30~에 해당하는 행의 인덱스를 찾음\nselected_indices1 = df50[df50['cc_num'] == 4.302480e+15].index\nselected_indices2 = df50[df50['cc_num'] == 2.242540e+15].index\n\n# 선택된 인덱스에 대한 is_fraud 값을 가져옴\nis_fraud_values1 = df50.loc[selected_indices1, 'is_fraud'].tolist()\nis_fraud_values2 = df50.loc[selected_indices2, 'is_fraud'].tolist()\n\n# PyTorch Geometric 그래프를 NetworkX 그래프로 변환\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\n\n# 첫 번째 서브그래프 추출\nsubgraph1 = G.subgraph(selected_indices1)\n\n# 노드와 엣지 색상 설정 (서브그래프 1)\nnode_colors1 = ['r' if node in selected_indices1 and is_fraud_values1[selected_indices1.get_loc(node)] == 1 else 'k' for node in subgraph1.nodes()]\n\n# 그래프 레이아웃 설정 (서브그래프 1)\npos1 = nx.spring_layout(subgraph1, seed=42)\n\n# 두 번째 서브그래프 추출\nsubgraph2 = G.subgraph(selected_indices2)\n\n# 노드와 엣지 색상 설정 (서브그래프 2)\nnode_colors2 = ['r' if node in selected_indices2 and is_fraud_values2[selected_indices2.get_loc(node)] == 1 else 'k' for node in subgraph2.nodes()]\n\n# 그래프 레이아웃 설정 (서브그래프 2)\npos2 = nx.spring_layout(subgraph2, seed=43)\n\n# 그래프 그리기\nplt.figure(figsize=(15, 5))\n\n# 첫 번째 하위 그래프\nplt.subplot(1, 2, 1)\nplt.title('cc_num=4.302480e+15')\nnx.draw_networkx_nodes(subgraph1, pos1, node_size=200, node_color=node_colors1)\nnx.draw_networkx_edges(subgraph1, pos1, edge_color='gray')\nnx.draw_networkx_labels(subgraph1, pos1, font_size=10)\nplt.axis('off')\n\n# 두 번째 하위 그래프\nplt.subplot(1, 2, 2)\nplt.title('cc_num=2.242540e+15')\nnx.draw_networkx_nodes(subgraph2, pos2, node_size=200, node_color=node_colors2)\nnx.draw_networkx_edges(subgraph2, pos2, edge_color='gray')\nnx.draw_networkx_labels(subgraph2, pos2, font_size=10)\nplt.axis('off')\n\n# 그래프 출력\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# 4.30~에 해당하는 행의 인덱스를 찾음\nselected_indices1 = df50[df50['cc_num'] == 4.302480e+15].index\nselected_indices2 = df50[df50['cc_num'] == 2.242540e+15].index\n\n# 선택된 인덱스에 대한 is_fraud 값을 가져옴\nis_fraud_values1 = df50.loc[selected_indices1, 'is_fraud'].tolist()\nis_fraud_values2 = df50.loc[selected_indices2, 'is_fraud'].tolist()\n\n# PyTorch Geometric 그래프를 NetworkX 그래프로 변환\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\n\n# 첫 번째 서브그래프 추출\nsubgraph1 = G.subgraph(selected_indices1)\n\n# 노드와 엣지 색상 설정 (서브그래프 1)\nnode_colors1 = ['r' if node in selected_indices1 and is_fraud_values1[selected_indices1.get_loc(node)] == 1 else 'k' for node in subgraph1.nodes()]\n\n# 엣지 색상 설정 (파란색)\nedge_colors1 = ['b' if (u, v) in subgraph1.edges() else 'gray' for u, v in subgraph1.edges()]\n\n# 그래프 레이아웃 설정 (서브그래프 1)\npos1 = nx.spring_layout(subgraph1, seed=42)\n\n# 두 번째 서브그래프 추출\nsubgraph2 = G.subgraph(selected_indices2)\n\n# 노드와 엣지 색상 설정 (서브그래프 2)\nnode_colors2 = ['r' if node in selected_indices2 and is_fraud_values2[selected_indices2.get_loc(node)] == 1 else 'k' for node in subgraph2.nodes()]\n\n# 엣지 색상 설정 (초록색)\nedge_colors2 = ['g' if (u, v) in subgraph2.edges() else 'gray' for u, v in subgraph2.edges()]\n\n# 그래프 레이아웃 설정 (서브그래프 2)\npos2 = nx.spring_layout(subgraph2, seed=43)\n\n# 그래프 그리기\nplt.figure(figsize=(10, 5))\n\n# 첫 번째 하위 그래프\nplt.title('cc_num=4.302480e+15')\nnx.draw_networkx_nodes(subgraph1, pos1, node_size=200, node_color=node_colors1)\nnx.draw_networkx_edges(subgraph1, pos1, edge_color=edge_colors1)\nnx.draw_networkx_labels(subgraph1, pos1, font_size=10)\n\n# 두 번째 하위 그래프\nnx.draw_networkx_nodes(subgraph2, pos2, node_size=200, node_color=node_colors2)\nnx.draw_networkx_edges(subgraph2, pos2, edge_color=edge_colors2)\nnx.draw_networkx_labels(subgraph2, pos2, font_size=10)\n\n# 축 숨기기\nplt.axis('off')\n\n# 그래프 출력\nplt.show()\n\n\n\n\n\ndf50[df50['cc_num'] == 6.535330e+15].index\n\nInt64Index([11740], dtype='int64')\n\n\n\ndf50[df50['cc_num'] == 4.503100e+18].index\n\nInt64Index([5381, 5382, 7996, 8217, 9176, 10248, 10517, 10613, 10950, 11321], dtype='int64')\n\n\n\ndf50[df50['is_fraud'] == 1]['cc_num'].value_counts()\n\n2.131740e+14    24\n3.506040e+15    23\n1.800680e+14    22\n6.011380e+15    22\n1.800850e+14    21\n                ..\n6.011110e+15     2\n4.809700e+12     2\n3.576430e+15     2\n4.503100e+18     2\n3.546670e+15     2\nName: cc_num, Length: 596, dtype: int64\n\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# PyTorch Geometric 그래프를 NetworkX 그래프로 변환\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\n\nsubgraph = nx.ego_graph(G, 2293, radius=1)\n\n# 그래프 시각화\npos = nx.spring_layout(subgraph)  # 그래프 레이아웃 설정\n\n# 노드 그리기\nnx.draw_networkx_nodes(subgraph, pos, node_size=200)\n\n# 엣지 그리기\nnx.draw_networkx_edges(subgraph, pos)\n\n# 노드 인덱스 표시\nlabels = {node: str(node) for node in subgraph.nodes()}\nnx.draw_networkx_labels(subgraph, pos, labels, font_size=10)\n\n# 그래프 출력\nplt.axis('off')  # 축 숨기기\nplt.show()\n\n\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# PyTorch Geometric 그래프를 NetworkX 그래프로 변환\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\n\n# 노드 1023과 연결된 모든 노드와 엣지를 포함하는 부분 그래프 추출\nsubgraph1 = nx.ego_graph(G, 2293, radius=1)\nsubgraph2 = nx.ego_graph(G, 11740, radius=1)\n\n# 왼쪽 그래프 레이아웃 설정\npos1 = nx.spring_layout(subgraph1, pos=None, seed=42)  # 그래프 레이아웃 설정\n\n# 오른쪽 그래프 레이아웃 설정\npos2 = nx.spring_layout(subgraph2, pos=None, seed=43)\n\n# 노드 그리기\nnx.draw_networkx_nodes(subgraph1, pos1, node_size=200, node_color='b', label='cc_num=4.302480e+15')\nnx.draw_networkx_nodes(subgraph2, pos2, node_size=200, node_color='g', label='cc_num=6.535330e+15')\n\n# 엣지 그리기\nnx.draw_networkx_edges(subgraph1, pos1)\nnx.draw_networkx_edges(subgraph2, pos2)\n\n# 노드 인덱스 표시\nlabels1 = {node: str(node) for node in subgraph1.nodes()}\nlabels2 = {node: str(node) for node in subgraph2.nodes()}\nnx.draw_networkx_labels(subgraph1, pos1, labels1, font_size=10)\nnx.draw_networkx_labels(subgraph2, pos2, labels2, font_size=10)\n\n# 그래프 출력\nplt.axis('off')  # 축 숨기기\n\n# 레이블 표시\nplt.legend(loc='best')\n\nplt.show()\n\n\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# PyTorch Geometric 그래프를 NetworkX 그래프로 변환\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\n\n# 노드 1023과 연결된 모든 노드와 엣지를 포함하는 부분 그래프 추출\nsubgraph1 = nx.ego_graph(G, 2293, radius=1)\nsubgraph2 = nx.ego_graph(G, 8217, radius=1)\n\n# 왼쪽 그래프 레이아웃 설정\npos1 = nx.spring_layout(subgraph1, pos=None, seed=42)  # 그래프 레이아웃 설정\n\n# 오른쪽 그래프 레이아웃 설정\npos2 = nx.spring_layout(subgraph2, pos=None, seed=43)\n\n# 두 개의 하위 그래프 생성\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n\n# 첫 번째 하위 그래프에 그래프 그리기\nax1.set_title('cc_num=4.302480e+15')\nnx.draw_networkx_nodes(subgraph1, pos1, ax=ax1, node_size=200, node_color='b')\nnx.draw_networkx_edges(subgraph1, pos1, ax=ax1)\nlabels1 = {node: str(node) for node in subgraph1.nodes()}\nnx.draw_networkx_labels(subgraph1, pos1, labels1, font_size=10, ax=ax1)\n\n# 두 번째 하위 그래프에 그래프 그리기\nax2.set_title('cc_num=4.503100e+18')\nnx.draw_networkx_nodes(subgraph2, pos2, ax=ax2, node_size=200, node_color='g')\nnx.draw_networkx_edges(subgraph2, pos2, ax=ax2)\nlabels2 = {node: str(node) for node in subgraph2.nodes()}\nnx.draw_networkx_labels(subgraph2, pos2, labels2, font_size=10, ax=ax2)\n\n# 축 숨기기\nax1.axis('off')\nax2.axis('off')\n\n# 그래프 출력\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# PyTorch Geometric 그래프를 NetworkX 그래프로 변환\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\n\n# 노드 1023과 연결된 모든 노드와 엣지를 포함하는 부분 그래프 추출\nsubgraph1 = nx.ego_graph(G, 2293, radius=1)\nsubgraph2 = nx.ego_graph(G, 8217, radius=1)\n\n# 왼쪽 그래프 레이아웃 설정\npos1 = nx.spring_layout(subgraph1, pos=None, seed=42)  # 그래프 레이아웃 설정\n\n# 오른쪽 그래프 레이아웃 설정\npos2 = nx.spring_layout(subgraph2, pos=None, seed=43)\n\n# 두 개의 하위 그래프 생성\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n\n# 첫 번째 하위 그래프에 그래프 그리기\nax1.set_title('cc_num=4.302480e+15')\nnx.draw_networkx_nodes(subgraph1, pos1, ax=ax1, node_size=200, node_color='b')\nlabels1 = {node: str(node) for node in subgraph1.nodes()}\n\n# 엣지 색상 설정 (is_fraud가 1일 때 빨간색, 그 외에는 검정색)\nedge_colors1 = ['r' if subgraph1[u][v].get('is_fraud', 0) == 1 else 'k' for u, v in subgraph1.edges()]\nnx.draw_networkx_edges(subgraph1, pos1, edgelist=subgraph1.edges(), edge_color=edge_colors1, ax=ax1)\nnx.draw_networkx_labels(subgraph1, pos1, labels1, font_size=10, ax=ax1)\n\n# 두 번째 하위 그래프에 그래프 그리기\nax2.set_title('cc_num=4.503100e+18')\nnx.draw_networkx_nodes(subgraph2, pos2, ax=ax2, node_size=200, node_color='g')\nlabels2 = {node: str(node) for node in subgraph2.nodes()}\n\n# 엣지 색상 설정 (is_fraud가 1일 때 빨간색, 그 외에는 검정색)\nedge_colors2 = ['r' if subgraph2[u][v].get('is_fraud', 0) == 1 else 'k' for u, v in subgraph2.edges()]\nnx.draw_networkx_edges(subgraph2, pos2, edgelist=subgraph2.edges(), edge_color=edge_colors2, ax=ax2)\nnx.draw_networkx_labels(subgraph2, pos2, labels2, font_size=10, ax=ax2)\n\n# 축 숨기기\nax1.axis('off')\nax2.axis('off')\n\n# 그래프 출력\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nfraud값이 1인게 표시가 안되네??\n\n\ndata\n\nData(x=[12012, 1], edge_index=&lt;function edge_index_selected at 0x7f3f8d358ee0&gt;, y=[12012], train_mask=[12012], test_mask=[12012])\n\n\n\ndata.edge_index\n\n&lt;function __main__.edge_index_selected(edge_index)&gt;\n\n\n\nx = torch.tensor(df50['amt'], dtype=torch.float).reshape(-1,1)\ny = torch.tensor(df50['is_fraud'],dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index = edge_index_selected, y=y, train_mask = train_mask, test_mask = test_mask)\ndata\n\nData(x=[12012, 1], edge_index=[2, 93730], y=[12012], train_mask=[12012], test_mask=[12012])\n\n\n\n- ppt 삽입할 그래프.\n- cc_num\n\n거래량이 가장 많은 cc_num:4.302480e+15\n거래량 평균인cc_num: 4.2929e+18\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# 4.50~에 해당하는 행의 인덱스를 찾음\nselected_indices = df50[df50['cc_num'] == 4.302480e+15].index\n\n# 선택된 인덱스에 대한 is_fraud 값을 가져옴\nis_fraud_values = df50.loc[selected_indices, 'is_fraud'].tolist()\n\n# PyTorch Geometric 그래프를 NetworkX 그래프로 변환\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\n\n# 서브그래프를 추출\nsubgraph = G.subgraph(selected_indices)\n\n# 노드와 엣지 색상을 설정\nnode_colors = ['r' if node in selected_indices and is_fraud_values[selected_indices.get_loc(node)] == 1 else 'k' for node in subgraph.nodes()]\n\n# 그래프 레이아웃 설정\npos = nx.spring_layout(subgraph, seed=42)\n\n# 그래프 그리기\nplt.figure(figsize=(10, 5))\nnx.draw_networkx_nodes(subgraph, pos, node_size=200, node_color=node_colors)\nnx.draw_networkx_edges(subgraph, pos, edge_color='gray')\nnx.draw_networkx_labels(subgraph, pos, font_size=10)\n\n# 축 숨기기\nplt.axis('off')\n\n# 그래프 출력\nplt.show()\n\n\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# 4.50~에 해당하는 행의 인덱스를 찾음\nselected_indices = df50[df50['cc_num'] == 4.2929e+18].index\n\n# 선택된 인덱스에 대한 is_fraud 값을 가져옴\nis_fraud_values = df50.loc[selected_indices, 'is_fraud'].tolist()\n\n# PyTorch Geometric 그래프를 NetworkX 그래프로 변환\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\n\n# 서브그래프를 추출\nsubgraph = G.subgraph(selected_indices)\n\n# 노드와 엣지 색상을 설정\nnode_colors = ['r' if node in selected_indices and is_fraud_values[selected_indices.get_loc(node)] == 1 else 'k' for node in subgraph.nodes()]\n\n# 그래프 레이아웃 설정\npos = nx.spring_layout(subgraph, seed=42)\n\n# 그래프 그리기\nplt.figure(figsize=(10, 5))\nnx.draw_networkx_nodes(subgraph, pos, node_size=200, node_color=node_colors)\nnx.draw_networkx_edges(subgraph, pos, edge_color='gray')\nnx.draw_networkx_labels(subgraph, pos, font_size=10)\n\n# 축 숨기기\nplt.axis('off')\n\n# 그래프 출력\nplt.show()\n\n\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# 첫 번째 데이터 선택\nselected_indices1 = df50[df50['cc_num'] == 4.302480e+15].index\nis_fraud_values1 = df50.loc[selected_indices1, 'is_fraud'].tolist()\n\n# 두 번째 데이터 선택\nselected_indices2 = df50[df50['cc_num'] == 4.2929e+18].index\nis_fraud_values2 = df50.loc[selected_indices2, 'is_fraud'].tolist()\n\n# PyTorch Geometric 그래프를 NetworkX 그래프로 변환\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\n\n# 서브그래프 추출\nsubgraph1 = G.subgraph(selected_indices1)\nsubgraph2 = G.subgraph(selected_indices2)\n\n# 노드 색상 설정\nnode_colors1 = ['r' if node in selected_indices1 and is_fraud_values1[selected_indices1.get_loc(node)] == 1 else 'k' for node in subgraph1.nodes()]\nnode_colors2 = ['r' if node in selected_indices2 and is_fraud_values2[selected_indices2.get_loc(node)] == 1 else 'k' for node in subgraph2.nodes()]\n\n# 엣지 색상 설정\nedge_colors1 = ['g' if edge in subgraph1.edges() else 'k' for edge in subgraph1.edges()]\nedge_colors2 = ['b' if edge in subgraph2.edges() else 'k' for edge in subgraph2.edges()]\n\n# 그래프 레이아웃 설정\npos1 = nx.spring_layout(subgraph1, seed=42)\npos2 = nx.spring_layout(subgraph2, seed=42)\n\n# 그래프 그리기\nplt.figure(figsize=(12, 6))\n\n# 그래프 1 그리기\nnx.draw_networkx_nodes(subgraph1, pos1, node_size=200, node_color=node_colors1)\nnx.draw_networkx_edges(subgraph1, pos1, edge_color=edge_colors1)\n\n# 그래프 2 그리기\nnx.draw_networkx_nodes(subgraph2, pos2, node_size=200, node_color=node_colors2)\nnx.draw_networkx_edges(subgraph2, pos2, edge_color=edge_colors2)\n\n# 축 숨기기\nplt.axis('off')\n\n# 그래프 출력\nplt.show()\n\n\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\n\n# 스타일 설정 (ggplot 스타일 사용)\nstyle.use('ggplot')\n\n# 첫 번째 데이터 선택\nselected_indices1 = df50[df50['cc_num'] == 4.302480e+15].index\nis_fraud_values1 = df50.loc[selected_indices1, 'is_fraud'].tolist()\n\n# 두 번째 데이터 선택\nselected_indices2 = df50[df50['cc_num'] == 4.2929e+18].index\nis_fraud_values2 = df50.loc[selected_indices2, 'is_fraud'].tolist()\n\n# PyTorch Geometric 그래프를 NetworkX 그래프로 변환\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\n\n# 서브그래프 추출\nsubgraph1 = G.subgraph(selected_indices1)\nsubgraph2 = G.subgraph(selected_indices2)\n\n# 노드 색상 설정\nnode_colors1 = ['r' if node in selected_indices1 and is_fraud_values1[selected_indices1.get_loc(node)] == 1 else 'k' for node in subgraph1.nodes()]\nnode_colors2 = ['r' if node in selected_indices2 and is_fraud_values2[selected_indices2.get_loc(node)] == 1 else 'k' for node in subgraph2.nodes()]\n\n# 엣지 색상 설정\nedge_colors1 = ['g' if edge in subgraph1.edges() else 'k' for edge in subgraph1.edges()]\nedge_colors2 = ['b' if edge in subgraph2.edges() else 'k' for edge in subgraph2.edges()]\n\n# 그래프 레이아웃 설정 (kamada_kawai 레이아웃 사용)\npos1 = nx.kamada_kawai_layout(subgraph1)\npos2 = nx.kamada_kawai_layout(subgraph2)\n\n# 그래프 그리기\nplt.figure(figsize=(12, 6))\n\n# 그래프 1 그리기\nnx.draw_networkx_nodes(subgraph1, pos1, node_size=200, node_color=node_colors1, node_shape='o')  # 노드 모양: 원 (circle)\nnx.draw_networkx_edges(subgraph1, pos1, edge_color=edge_colors1)\n\n# 그래프 2 그리기\nnx.draw_networkx_nodes(subgraph2, pos2, node_size=200, node_color=node_colors2, node_shape='D')  # 노드 모양: 다이아몬드 (diamond)\nnx.draw_networkx_edges(subgraph2, pos2, edge_color=edge_colors2)\n\n# 범례 추가\nlegend_elements = [\n    plt.Line2D([0], [0], marker='o', color='g', markerfacecolor='black', markersize=10, label='cc_num 1'),\n    plt.Line2D([0], [0], marker='D', color='b', markerfacecolor='black', markersize=10, label='cc_num 2'),\n    plt.Line2D([0], [0], marker='o', color='black', markerfacecolor='r', markersize=10, label='fraud=1'),\n]\nplt.legend(handles=legend_elements, loc='lower center', ncol=3, frameon=True)\n\n# 그래프 출력\nplt.axis('off')\nplt.show()\n\n\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\n\n# 스타일 설정 (ggplot 스타일 사용)\nstyle.use('ggplot')\n\n# 첫 번째 데이터 선택\nselected_indices1 = df50[df50['cc_num'] == 4.302480e+15].index\nis_fraud_values1 = df50.loc[selected_indices1, 'is_fraud'].tolist()\n\n# 두 번째 데이터 선택\nselected_indices2 = df50[df50['cc_num'] == 4.2929e+18].index\nis_fraud_values2 = df50.loc[selected_indices2, 'is_fraud'].tolist()\n\n# PyTorch Geometric 그래프를 NetworkX 그래프로 변환\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\n\n# 서브그래프 추출\nsubgraph1 = G.subgraph(selected_indices1)\nsubgraph2 = G.subgraph(selected_indices2)\n\n# 노드 색상 설정\nnode_colors1 = ['r' if node in selected_indices1 and is_fraud_values1[selected_indices1.get_loc(node)] == 1 else 'k' for node in subgraph1.nodes()]\nnode_colors2 = ['r' if node in selected_indices2 and is_fraud_values2[selected_indices2.get_loc(node)] == 1 else 'k' for node in subgraph2.nodes()]\n\n# 엣지 색상 설정\nedge_colors1 = ['g' if edge in subgraph1.edges() else 'k' for edge in subgraph1.edges()]\nedge_colors2 = ['b' if edge in subgraph2.edges() else 'k' for edge in subgraph2.edges()]\n\n# 그래프 레이아웃 설정 (kamada_kawai 레이아웃 사용)\npos1 = nx.kamada_kawai_layout(subgraph1)\npos2 = nx.kamada_kawai_layout(subgraph2)\n\n# 그래프 그리기\nplt.figure(figsize=(12, 6))\n\n# 그래프 1 그리기\nnx.draw_networkx_nodes(subgraph1, pos1, node_size=200, node_color=node_colors1, node_shape='o')  # 노드 모양: 원 (circle)\nnx.draw_networkx_edges(subgraph1, pos1, edge_color=edge_colors1)\n\n# 그래프 2 그리기\nnx.draw_networkx_nodes(subgraph2, pos2, node_size=200, node_color=node_colors2, node_shape='D')  # 노드 모양: 다이아몬드 (diamond)\nnx.draw_networkx_edges(subgraph2, pos2, edge_color=edge_colors2)\n\n# 범례 추가\nlegend_elements = [\n    plt.Line2D([0], [0], marker='o', color='g', markerfacecolor='black', markersize=10, label='cc_num 1'),\n    plt.Line2D([0], [0], marker='D', color='b', markerfacecolor='black', markersize=10, label='cc_num 2'),\n    plt.Line2D([0], [0], marker='o', color='black', markerfacecolor='r', markersize=10, label='fraud=1'),\n]\nlegend = plt.legend(handles=legend_elements, loc='lower center', ncol=3, frameon=True)\nlegend.set_bbox_to_anchor((0.5, -0.1))  # 범례 위치 조정\n\n# 그래프 출력\nplt.axis('off')\nplt.show()"
  },
  {
    "objectID": "posts/231101.html#데이터정리",
    "href": "posts/231101.html#데이터정리",
    "title": "[FRAUD] 그래프 그림 그리기",
    "section": "",
    "text": "_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50 = df50.reset_index()\ndf50.shape\n\n(12012, 23)\n\n\n\n\n\n\nmask(df50)\n\n(array([False,  True,  True, ...,  True, False,  True]),\n array([ True, False, False, ..., False,  True, False]))\n\n\n\ntrain_mask, test_mask = mask(df50)\n\n\n\n\n\n\n# groups = df50.groupby('cc_num')\n# edge_index_list_plus = [compute_time_difference(group) for _, group in groups]\n# edge_index_list_plus_flat = [item for sublist in edge_index_list_plus for item in sublist]\n# edge_index_list_plus_nparr = np.array(edge_index_list_plus_flat)\n# np.save('edge_index_list_plus50.npy', edge_index_list_plus_nparr)\n\n\nedge_index = np.load('edge_index_list_plus50.npy').astype(np.float64)\nedge_index.shape\n\n(200706, 3)\n\n\n\nedge_index_selected = edge_index_selected(edge_index)\n\n\nedge_index_selected\n\ntensor([[ 1023,  1023,  1023,  ..., 11944, 11944, 11944],\n        [ 1024,  1028,  1031,  ...,  4257,  9241,  9782]])\n\n\n\nnp.array(edge_index_selected)\n\narray([[ 1023,  1023,  1023, ..., 11944, 11944, 11944],\n       [ 1024,  1028,  1031, ...,  4257,  9241,  9782]])"
  },
  {
    "objectID": "posts/231101.html#분석-1gcn-amt",
    "href": "posts/231101.html#분석-1gcn-amt",
    "title": "[FRAUD] 그래프 그림 그리기",
    "section": "",
    "text": "x = torch.tensor(df50['amt'], dtype=torch.float).reshape(-1,1)\ny = torch.tensor(df50['is_fraud'],dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index = edge_index_selected, y=y, train_mask = train_mask, test_mask = test_mask)\ndata\n\n\n\ntorch.manual_seed(202250926)\nclass GCN2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 32)\n        self.conv2 = GCNConv(32,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\nX = (data.x[data.train_mask]).numpy()\nXX = (data.x[data.test_mask]).numpy()\ny = (data.y[data.train_mask]).numpy()\nyy = (data.y[data.test_mask]).numpy()\n\nmodel = GCN2()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(400):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\nmodel.eval()\n\npred = model(data).argmax(dim=1)\nyyhat = pred[data.test_mask]\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results1= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석1'])\n_results1\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석1\n0.902098\n0.862478\n0.95913\n0.90824\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\n\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\n\n\npos = nx.spring_layout(G)\nlabels = {i: f\"{i}\\n{data.y[i].item()}\" for i in range(data.num_nodes)}\nnx.draw(G, pos, with_labels=True, node_color='lightblue', labels=labels)\nplt.show()\n\n\n\n\n\nanomaly_nodes = [i for i in range(data.num_nodes) if data.y[i].item() == 1]\n\n# 중요한 노드에 대한 레이블 설정\nlabels = {i: f\"{i}\\n{data.y[i].item()}\" for i in anomaly_nodes}\n\n# 중요한 노드 강조\nnode_color = ['lightblue' if i not in anomaly_nodes else 'red' for i in range(data.num_nodes)]\n\n# 그래프 시각화\nnx.draw(G, pos, with_labels=True, node_color=node_color, labels=labels)\nplt.show()\n\n\n\n\n\nG\n\n&lt;networkx.classes.graph.Graph at 0x7f3eaffc0880&gt;\n\n\n흠…..\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# PyTorch Geometric 그래프를 NetworkX 그래프로 변환\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\n\n# 노드 A와 B의 인덱스\nnode_A_index = 1023\nnode_B_index = 1024\n\n# 노드 A와 B 간의 경로 찾기\nshortest_path = nx.shortest_path(G, source=node_A_index, target=node_B_index)\n\n# 노드 A와 B 간의 경로로 이루어진 부분 그래프 추출\nsubgraph = G.subgraph(shortest_path)\n\n# 그래프 시각화\npos = nx.spring_layout(subgraph)  # 그래프 레이아웃 설정\n\n# 노드 그리기\nnx.draw_networkx_nodes(subgraph, pos, node_size=200)\n\n# 엣지 그리기\nnx.draw_networkx_edges(subgraph, pos)\n\n# 그래프 출력\nplt.axis('off')  # 축 숨기기\nplt.show()\n\n\n\n\n\nsubgraph = nx.ego_graph(G, 1023, radius=1)\n\n# 그래프 시각화\npos = nx.spring_layout(subgraph)  # 그래프 레이아웃 설정\n\n# 노드 그리기\nnx.draw_networkx_nodes(subgraph, pos, node_size=200)\n\n# 엣지 그리기\nnx.draw_networkx_edges(subgraph, pos)\n\n# 그래프 출력\nplt.axis('off')  # 축 숨기기\nplt.show()\n\n\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# PyTorch Geometric 그래프를 NetworkX 그래프로 변환\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\n\n# 노드 1023과 연결된 모든 노드와 엣지를 포함하는 부분 그래프 추출\nsubgraph = nx.ego_graph(G, 1023, radius=1)\n\n# 그래프 시각화\npos = nx.spring_layout(subgraph)  # 그래프 레이아웃 설정\n\n# 노드 그리기\nnx.draw_networkx_nodes(subgraph, pos, node_size=200)\n\n# 엣지 그리기\nnx.draw_networkx_edges(subgraph, pos)\n\n# 노드 인덱스 표시\nlabels = {node: str(node) for node in subgraph.nodes()}\nnx.draw_networkx_labels(subgraph, pos, labels, font_size=10)\n\n# 그래프 출력\nplt.axis('off')  # 축 숨기기\nplt.show()\n\n\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# PyTorch Geometric 그래프를 NetworkX 그래프로 변환\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\n\nsubgraph = nx.ego_graph(G, 9782, radius=1)\n\n# 그래프 시각화\npos = nx.spring_layout(subgraph)  # 그래프 레이아웃 설정\n\n# 노드 그리기\nnx.draw_networkx_nodes(subgraph, pos, node_size=200)\n\n# 엣지 그리기\nnx.draw_networkx_edges(subgraph, pos)\n\n# 노드 인덱스 표시\nlabels = {node: str(node) for node in subgraph.nodes()}\nnx.draw_networkx_labels(subgraph, pos, labels, font_size=10)\n\n# 그래프 출력\nplt.axis('off')  # 축 숨기기\nplt.show()\n\n\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# PyTorch Geometric 그래프를 NetworkX 그래프로 변환\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\n\n# 노드 1023과 연결된 모든 노드와 엣지를 포함하는 부분 그래프 추출\nsubgraph1 = nx.ego_graph(G, 1023, radius=1)\nsubgraph2 = nx.ego_graph(G, 9782, radius=1)\n\n# 그래프 시각화\npos1 = nx.spring_layout(subgraph1)  # 그래프 레이아웃 설정\npos2 = nx.spring_layout(subgraph2)\n\n# 노드 그리기\nnx.draw_networkx_nodes(subgraph1, pos1, node_size=200, node_color='b', label='Node 1023')\nnx.draw_networkx_nodes(subgraph2, pos2, node_size=200, node_color='g', label='Node 1024')\n\n# 엣지 그리기\nnx.draw_networkx_edges(subgraph1, pos1)\nnx.draw_networkx_edges(subgraph2, pos2)\n\n# 노드 인덱스 표시\nlabels1 = {node: str(node) for node in subgraph1.nodes()}\nlabels2 = {node: str(node) for node in subgraph2.nodes()}\nnx.draw_networkx_labels(subgraph1, pos1, labels1, font_size=10)\nnx.draw_networkx_labels(subgraph2, pos2, labels2, font_size=10)\n\n# 그래프 출력\nplt.axis('off')  # 축 숨기기\n\n# 레이블 표시\nplt.legend(loc='best')\n\nplt.show()\n\n\n\n\n\n# 노드 1023와 9782 간의 연결 확인\nare_connected = G.has_edge(1023, 9782)\n\nif are_connected:\n    print(\"Node 1023 and Node 9782 are connected.\")\nelse:\n    print(\"Node 1023 and Node 9782 are not connected.\")\n\nNode 1023 and Node 9782 are not connected.\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# PyTorch Geometric 그래프를 NetworkX 그래프로 변환\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\n\n# 노드 1023과 연결된 모든 노드와 엣지를 포함하는 부분 그래프 추출\nsubgraph1 = nx.ego_graph(G, 1023, radius=1)\nsubgraph2 = nx.ego_graph(G, 9782, radius=1)\n\n# 왼쪽 그래프 레이아웃 설정\npos1 = nx.spring_layout(subgraph1, pos=None, seed=42)  # 그래프 레이아웃 설정\n\n# 오른쪽 그래프 레이아웃 설정\npos2 = nx.spring_layout(subgraph2, pos=None, seed=43)\n\n# 노드 그리기\nnx.draw_networkx_nodes(subgraph1, pos1, node_size=200, node_color='b', label='Node 1023')\nnx.draw_networkx_nodes(subgraph2, pos2, node_size=200, node_color='g', label='Node 1024')\n\n# 엣지 그리기\nnx.draw_networkx_edges(subgraph1, pos1)\nnx.draw_networkx_edges(subgraph2, pos2)\n\n# 노드 인덱스 표시\nlabels1 = {node: str(node) for node in subgraph1.nodes()}\nlabels2 = {node: str(node) for node in subgraph2.nodes()}\nnx.draw_networkx_labels(subgraph1, pos1, labels1, font_size=10)\nnx.draw_networkx_labels(subgraph2, pos2, labels2, font_size=10)\n\n# 그래프 출력\nplt.axis('off')  # 축 숨기기\n\n# 레이블 표시\nplt.legend(loc='best')\n\nplt.show()\n\n\n\n\n\n\nlen(df50['cc_num'].unique())\n\n932\n\n\n\ndf50['cc_num'].value_counts()\n\n4.302480e+15    43\n1.800650e+14    36\n2.131740e+14    35\n2.720430e+15    34\n2.242540e+15    33\n                ..\n3.885950e+13     1\n4.026220e+12     1\n6.526450e+15     1\n4.972230e+15     1\n6.535330e+15     1\nName: cc_num, Length: 932, dtype: int64\n\n\n\ncc_num_counts = df50['cc_num'].value_counts()\nmean_counts = cc_num_counts.mean()\nmean_counts\n\n12.888412017167383\n\n\n\ncounts = df50['cc_num'].value_counts()\ncc_num_with_13_counts = counts[counts == 13].index\ncc_num_with_13_counts\n\nFloat64Index([3585740000000000.0, 4839040000000000.0,  370349000000000.0,\n              3583090000000000.0, 3534720000000000.0,  342351000000000.0,\n               213161000000000.0, 6011860000000000.0, 2288810000000000.0,\n                  560881000000.0,  346273000000000.0,    4760120000000.0,\n              2356280000000000.0,     501803000000.0, 2297450000000000.0,\n              4933460000000000.0, 3565940000000000.0, 3597980000000000.0,\n                36913600000000.0,     630423000000.0,    4746000000000.0,\n              3536820000000000.0, 2720890000000000.0,          4.537e+18,\n                 4755700000000.0,   38057500000000.0,         4.2929e+18,\n              6011230000000000.0, 4060580000000000.0, 3546900000000000.0,\n               345060000000000.0,   30596500000000.0, 3597340000000000.0,\n              4124540000000000.0, 4204240000000000.0,  375237000000000.0,\n                36153900000000.0, 4334230000000000.0, 6012000000000000.0,\n              4134460000000000.0, 4653880000000000.0, 4509140000000000.0,\n              3565420000000000.0],\n             dtype='float64')\n\n\n\ndf50[df50['cc_num'] ==  4.537e+18].index\n\nInt64Index([ 6298,  6714,  7704,  7823,  7910,  7990,  9145,  9806, 10338,\n            10910, 11621, 11847, 11926],\n           dtype='int64')\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\nselected_indices = df50[df50['cc_num'] ==  3585740000000000.0].index\nis_fraud_values = df50.loc[selected_indices, 'is_fraud'].tolist()\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\nsubgraph = G.subgraph(selected_indices)\nnode_colors = ['r' if node in selected_indices and is_fraud_values[selected_indices.get_loc(node)] == 1 else 'k' for node in subgraph.nodes()]\npos = nx.spring_layout(subgraph, seed=42)\nplt.figure(figsize=(10, 5))\nnx.draw_networkx_nodes(subgraph, pos, node_size=200, node_color=node_colors)\nnx.draw_networkx_edges(subgraph, pos, edge_color='gray')\nnx.draw_networkx_labels(subgraph, pos, font_size=10)\nplt.axis('off')\nplt.show()\n\n\n\n\n\nindex = df50[df50['cc_num'] == 3585740000000000.0].index\nis_fraud_value = df50.loc[index, 'is_fraud']\nis_fraud_value\n\n4437     1\n4443     1\n4446     1\n4449     1\n4451     1\n4452     1\n4455     1\n4465     1\n6914     0\n6974     0\n8624     0\n8830     0\n11375    0\nName: is_fraud, dtype: int64\n\n\n\ndf50[df50['cc_num'] == 4.302480e+15].index\n\nInt64Index([ 2293,  2294,  2295,  2296,  2297,  2298,  2299,  2300,  4709,\n             4711,  4718,  4720,  4728,  4729,  4730,  4733,  4734,  4736,\n             4739,  4742,  6081,  6751,  6855,  7025,  7554,  7817,  7946,\n             8173,  9152,  9166,  9548,  9708,  9773,  9807, 10198, 10828,\n            10909, 10953, 10962, 11273, 11330, 11562, 11946],\n           dtype='int64')\n\n\n\nindex = df50[df50['cc_num'] == 4.302480e+15].index\nis_fraud_value = df50.loc[index, 'is_fraud']\nis_fraud_value\n\n2293     1\n2294     1\n2295     1\n2296     1\n2297     1\n2298     1\n2299     1\n2300     1\n4709     1\n4711     1\n4718     1\n4720     1\n4728     1\n4729     1\n4730     1\n4733     1\n4734     1\n4736     1\n4739     1\n4742     1\n6081     0\n6751     0\n6855     0\n7025     0\n7554     0\n7817     0\n7946     0\n8173     0\n9152     0\n9166     0\n9548     0\n9708     0\n9773     0\n9807     0\n10198    0\n10828    0\n10909    0\n10953    0\n10962    0\n11273    0\n11330    0\n11562    0\n11946    0\nName: is_fraud, dtype: int64\n\n\n- cc_num=4.302480e+15 인 그래프\n해당 노드의 is_fraud=1 이면 빨간색으로 표시했다\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# 4.50~에 해당하는 행의 인덱스를 찾음\nselected_indices = df50[df50['cc_num'] == 4.302480e+15].index\n\n# 선택된 인덱스에 대한 is_fraud 값을 가져옴\nis_fraud_values = df50.loc[selected_indices, 'is_fraud'].tolist()\n\n# PyTorch Geometric 그래프를 NetworkX 그래프로 변환\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\n\n# 서브그래프를 추출\nsubgraph = G.subgraph(selected_indices)\n\n# 노드와 엣지 색상을 설정\nnode_colors = ['r' if node in selected_indices and is_fraud_values[selected_indices.get_loc(node)] == 1 else 'k' for node in subgraph.nodes()]\n\n# 그래프 레이아웃 설정\npos = nx.spring_layout(subgraph, seed=42)\n\n# 그래프 그리기\nplt.figure(figsize=(10, 5))\nnx.draw_networkx_nodes(subgraph, pos, node_size=200, node_color=node_colors)\nnx.draw_networkx_edges(subgraph, pos, edge_color='gray')\nnx.draw_networkx_labels(subgraph, pos, font_size=10)\n\n# 축 숨기기\nplt.axis('off')\n\n# 그래프 출력\nplt.show()\n\n\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# 4.30~에 해당하는 행의 인덱스를 찾음\nselected_indices1 = df50[df50['cc_num'] == 4.302480e+15].index\nselected_indices2 = df50[df50['cc_num'] == 2.242540e+15].index\n\n# 선택된 인덱스에 대한 is_fraud 값을 가져옴\nis_fraud_values1 = df50.loc[selected_indices1, 'is_fraud'].tolist()\nis_fraud_values2 = df50.loc[selected_indices2, 'is_fraud'].tolist()\n\n# PyTorch Geometric 그래프를 NetworkX 그래프로 변환\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\n\n# 첫 번째 서브그래프 추출\nsubgraph1 = G.subgraph(selected_indices1)\n\n# 노드와 엣지 색상 설정 (서브그래프 1)\nnode_colors1 = ['r' if node in selected_indices1 and is_fraud_values1[selected_indices1.get_loc(node)] == 1 else 'k' for node in subgraph1.nodes()]\n\n# 그래프 레이아웃 설정 (서브그래프 1)\npos1 = nx.spring_layout(subgraph1, seed=42)\n\n# 두 번째 서브그래프 추출\nsubgraph2 = G.subgraph(selected_indices2)\n\n# 노드와 엣지 색상 설정 (서브그래프 2)\nnode_colors2 = ['r' if node in selected_indices2 and is_fraud_values2[selected_indices2.get_loc(node)] == 1 else 'k' for node in subgraph2.nodes()]\n\n# 그래프 레이아웃 설정 (서브그래프 2)\npos2 = nx.spring_layout(subgraph2, seed=43)\n\n# 그래프 그리기\nplt.figure(figsize=(15, 5))\n\n# 첫 번째 하위 그래프\nplt.subplot(1, 2, 1)\nplt.title('cc_num=4.302480e+15')\nnx.draw_networkx_nodes(subgraph1, pos1, node_size=200, node_color=node_colors1)\nnx.draw_networkx_edges(subgraph1, pos1, edge_color='gray')\nnx.draw_networkx_labels(subgraph1, pos1, font_size=10)\nplt.axis('off')\n\n# 두 번째 하위 그래프\nplt.subplot(1, 2, 2)\nplt.title('cc_num=2.242540e+15')\nnx.draw_networkx_nodes(subgraph2, pos2, node_size=200, node_color=node_colors2)\nnx.draw_networkx_edges(subgraph2, pos2, edge_color='gray')\nnx.draw_networkx_labels(subgraph2, pos2, font_size=10)\nplt.axis('off')\n\n# 그래프 출력\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# 4.30~에 해당하는 행의 인덱스를 찾음\nselected_indices1 = df50[df50['cc_num'] == 4.302480e+15].index\nselected_indices2 = df50[df50['cc_num'] == 2.242540e+15].index\n\n# 선택된 인덱스에 대한 is_fraud 값을 가져옴\nis_fraud_values1 = df50.loc[selected_indices1, 'is_fraud'].tolist()\nis_fraud_values2 = df50.loc[selected_indices2, 'is_fraud'].tolist()\n\n# PyTorch Geometric 그래프를 NetworkX 그래프로 변환\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\n\n# 첫 번째 서브그래프 추출\nsubgraph1 = G.subgraph(selected_indices1)\n\n# 노드와 엣지 색상 설정 (서브그래프 1)\nnode_colors1 = ['r' if node in selected_indices1 and is_fraud_values1[selected_indices1.get_loc(node)] == 1 else 'k' for node in subgraph1.nodes()]\n\n# 엣지 색상 설정 (파란색)\nedge_colors1 = ['b' if (u, v) in subgraph1.edges() else 'gray' for u, v in subgraph1.edges()]\n\n# 그래프 레이아웃 설정 (서브그래프 1)\npos1 = nx.spring_layout(subgraph1, seed=42)\n\n# 두 번째 서브그래프 추출\nsubgraph2 = G.subgraph(selected_indices2)\n\n# 노드와 엣지 색상 설정 (서브그래프 2)\nnode_colors2 = ['r' if node in selected_indices2 and is_fraud_values2[selected_indices2.get_loc(node)] == 1 else 'k' for node in subgraph2.nodes()]\n\n# 엣지 색상 설정 (초록색)\nedge_colors2 = ['g' if (u, v) in subgraph2.edges() else 'gray' for u, v in subgraph2.edges()]\n\n# 그래프 레이아웃 설정 (서브그래프 2)\npos2 = nx.spring_layout(subgraph2, seed=43)\n\n# 그래프 그리기\nplt.figure(figsize=(10, 5))\n\n# 첫 번째 하위 그래프\nplt.title('cc_num=4.302480e+15')\nnx.draw_networkx_nodes(subgraph1, pos1, node_size=200, node_color=node_colors1)\nnx.draw_networkx_edges(subgraph1, pos1, edge_color=edge_colors1)\nnx.draw_networkx_labels(subgraph1, pos1, font_size=10)\n\n# 두 번째 하위 그래프\nnx.draw_networkx_nodes(subgraph2, pos2, node_size=200, node_color=node_colors2)\nnx.draw_networkx_edges(subgraph2, pos2, edge_color=edge_colors2)\nnx.draw_networkx_labels(subgraph2, pos2, font_size=10)\n\n# 축 숨기기\nplt.axis('off')\n\n# 그래프 출력\nplt.show()\n\n\n\n\n\ndf50[df50['cc_num'] == 6.535330e+15].index\n\nInt64Index([11740], dtype='int64')\n\n\n\ndf50[df50['cc_num'] == 4.503100e+18].index\n\nInt64Index([5381, 5382, 7996, 8217, 9176, 10248, 10517, 10613, 10950, 11321], dtype='int64')\n\n\n\ndf50[df50['is_fraud'] == 1]['cc_num'].value_counts()\n\n2.131740e+14    24\n3.506040e+15    23\n1.800680e+14    22\n6.011380e+15    22\n1.800850e+14    21\n                ..\n6.011110e+15     2\n4.809700e+12     2\n3.576430e+15     2\n4.503100e+18     2\n3.546670e+15     2\nName: cc_num, Length: 596, dtype: int64\n\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# PyTorch Geometric 그래프를 NetworkX 그래프로 변환\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\n\nsubgraph = nx.ego_graph(G, 2293, radius=1)\n\n# 그래프 시각화\npos = nx.spring_layout(subgraph)  # 그래프 레이아웃 설정\n\n# 노드 그리기\nnx.draw_networkx_nodes(subgraph, pos, node_size=200)\n\n# 엣지 그리기\nnx.draw_networkx_edges(subgraph, pos)\n\n# 노드 인덱스 표시\nlabels = {node: str(node) for node in subgraph.nodes()}\nnx.draw_networkx_labels(subgraph, pos, labels, font_size=10)\n\n# 그래프 출력\nplt.axis('off')  # 축 숨기기\nplt.show()\n\n\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# PyTorch Geometric 그래프를 NetworkX 그래프로 변환\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\n\n# 노드 1023과 연결된 모든 노드와 엣지를 포함하는 부분 그래프 추출\nsubgraph1 = nx.ego_graph(G, 2293, radius=1)\nsubgraph2 = nx.ego_graph(G, 11740, radius=1)\n\n# 왼쪽 그래프 레이아웃 설정\npos1 = nx.spring_layout(subgraph1, pos=None, seed=42)  # 그래프 레이아웃 설정\n\n# 오른쪽 그래프 레이아웃 설정\npos2 = nx.spring_layout(subgraph2, pos=None, seed=43)\n\n# 노드 그리기\nnx.draw_networkx_nodes(subgraph1, pos1, node_size=200, node_color='b', label='cc_num=4.302480e+15')\nnx.draw_networkx_nodes(subgraph2, pos2, node_size=200, node_color='g', label='cc_num=6.535330e+15')\n\n# 엣지 그리기\nnx.draw_networkx_edges(subgraph1, pos1)\nnx.draw_networkx_edges(subgraph2, pos2)\n\n# 노드 인덱스 표시\nlabels1 = {node: str(node) for node in subgraph1.nodes()}\nlabels2 = {node: str(node) for node in subgraph2.nodes()}\nnx.draw_networkx_labels(subgraph1, pos1, labels1, font_size=10)\nnx.draw_networkx_labels(subgraph2, pos2, labels2, font_size=10)\n\n# 그래프 출력\nplt.axis('off')  # 축 숨기기\n\n# 레이블 표시\nplt.legend(loc='best')\n\nplt.show()\n\n\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# PyTorch Geometric 그래프를 NetworkX 그래프로 변환\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\n\n# 노드 1023과 연결된 모든 노드와 엣지를 포함하는 부분 그래프 추출\nsubgraph1 = nx.ego_graph(G, 2293, radius=1)\nsubgraph2 = nx.ego_graph(G, 8217, radius=1)\n\n# 왼쪽 그래프 레이아웃 설정\npos1 = nx.spring_layout(subgraph1, pos=None, seed=42)  # 그래프 레이아웃 설정\n\n# 오른쪽 그래프 레이아웃 설정\npos2 = nx.spring_layout(subgraph2, pos=None, seed=43)\n\n# 두 개의 하위 그래프 생성\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n\n# 첫 번째 하위 그래프에 그래프 그리기\nax1.set_title('cc_num=4.302480e+15')\nnx.draw_networkx_nodes(subgraph1, pos1, ax=ax1, node_size=200, node_color='b')\nnx.draw_networkx_edges(subgraph1, pos1, ax=ax1)\nlabels1 = {node: str(node) for node in subgraph1.nodes()}\nnx.draw_networkx_labels(subgraph1, pos1, labels1, font_size=10, ax=ax1)\n\n# 두 번째 하위 그래프에 그래프 그리기\nax2.set_title('cc_num=4.503100e+18')\nnx.draw_networkx_nodes(subgraph2, pos2, ax=ax2, node_size=200, node_color='g')\nnx.draw_networkx_edges(subgraph2, pos2, ax=ax2)\nlabels2 = {node: str(node) for node in subgraph2.nodes()}\nnx.draw_networkx_labels(subgraph2, pos2, labels2, font_size=10, ax=ax2)\n\n# 축 숨기기\nax1.axis('off')\nax2.axis('off')\n\n# 그래프 출력\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# PyTorch Geometric 그래프를 NetworkX 그래프로 변환\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\n\n# 노드 1023과 연결된 모든 노드와 엣지를 포함하는 부분 그래프 추출\nsubgraph1 = nx.ego_graph(G, 2293, radius=1)\nsubgraph2 = nx.ego_graph(G, 8217, radius=1)\n\n# 왼쪽 그래프 레이아웃 설정\npos1 = nx.spring_layout(subgraph1, pos=None, seed=42)  # 그래프 레이아웃 설정\n\n# 오른쪽 그래프 레이아웃 설정\npos2 = nx.spring_layout(subgraph2, pos=None, seed=43)\n\n# 두 개의 하위 그래프 생성\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n\n# 첫 번째 하위 그래프에 그래프 그리기\nax1.set_title('cc_num=4.302480e+15')\nnx.draw_networkx_nodes(subgraph1, pos1, ax=ax1, node_size=200, node_color='b')\nlabels1 = {node: str(node) for node in subgraph1.nodes()}\n\n# 엣지 색상 설정 (is_fraud가 1일 때 빨간색, 그 외에는 검정색)\nedge_colors1 = ['r' if subgraph1[u][v].get('is_fraud', 0) == 1 else 'k' for u, v in subgraph1.edges()]\nnx.draw_networkx_edges(subgraph1, pos1, edgelist=subgraph1.edges(), edge_color=edge_colors1, ax=ax1)\nnx.draw_networkx_labels(subgraph1, pos1, labels1, font_size=10, ax=ax1)\n\n# 두 번째 하위 그래프에 그래프 그리기\nax2.set_title('cc_num=4.503100e+18')\nnx.draw_networkx_nodes(subgraph2, pos2, ax=ax2, node_size=200, node_color='g')\nlabels2 = {node: str(node) for node in subgraph2.nodes()}\n\n# 엣지 색상 설정 (is_fraud가 1일 때 빨간색, 그 외에는 검정색)\nedge_colors2 = ['r' if subgraph2[u][v].get('is_fraud', 0) == 1 else 'k' for u, v in subgraph2.edges()]\nnx.draw_networkx_edges(subgraph2, pos2, edgelist=subgraph2.edges(), edge_color=edge_colors2, ax=ax2)\nnx.draw_networkx_labels(subgraph2, pos2, labels2, font_size=10, ax=ax2)\n\n# 축 숨기기\nax1.axis('off')\nax2.axis('off')\n\n# 그래프 출력\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nfraud값이 1인게 표시가 안되네??\n\n\ndata\n\nData(x=[12012, 1], edge_index=&lt;function edge_index_selected at 0x7f3f8d358ee0&gt;, y=[12012], train_mask=[12012], test_mask=[12012])\n\n\n\ndata.edge_index\n\n&lt;function __main__.edge_index_selected(edge_index)&gt;\n\n\n\nx = torch.tensor(df50['amt'], dtype=torch.float).reshape(-1,1)\ny = torch.tensor(df50['is_fraud'],dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index = edge_index_selected, y=y, train_mask = train_mask, test_mask = test_mask)\ndata\n\nData(x=[12012, 1], edge_index=[2, 93730], y=[12012], train_mask=[12012], test_mask=[12012])\n\n\n\n- ppt 삽입할 그래프.\n- cc_num\n\n거래량이 가장 많은 cc_num:4.302480e+15\n거래량 평균인cc_num: 4.2929e+18\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# 4.50~에 해당하는 행의 인덱스를 찾음\nselected_indices = df50[df50['cc_num'] == 4.302480e+15].index\n\n# 선택된 인덱스에 대한 is_fraud 값을 가져옴\nis_fraud_values = df50.loc[selected_indices, 'is_fraud'].tolist()\n\n# PyTorch Geometric 그래프를 NetworkX 그래프로 변환\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\n\n# 서브그래프를 추출\nsubgraph = G.subgraph(selected_indices)\n\n# 노드와 엣지 색상을 설정\nnode_colors = ['r' if node in selected_indices and is_fraud_values[selected_indices.get_loc(node)] == 1 else 'k' for node in subgraph.nodes()]\n\n# 그래프 레이아웃 설정\npos = nx.spring_layout(subgraph, seed=42)\n\n# 그래프 그리기\nplt.figure(figsize=(10, 5))\nnx.draw_networkx_nodes(subgraph, pos, node_size=200, node_color=node_colors)\nnx.draw_networkx_edges(subgraph, pos, edge_color='gray')\nnx.draw_networkx_labels(subgraph, pos, font_size=10)\n\n# 축 숨기기\nplt.axis('off')\n\n# 그래프 출력\nplt.show()\n\n\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# 4.50~에 해당하는 행의 인덱스를 찾음\nselected_indices = df50[df50['cc_num'] == 4.2929e+18].index\n\n# 선택된 인덱스에 대한 is_fraud 값을 가져옴\nis_fraud_values = df50.loc[selected_indices, 'is_fraud'].tolist()\n\n# PyTorch Geometric 그래프를 NetworkX 그래프로 변환\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\n\n# 서브그래프를 추출\nsubgraph = G.subgraph(selected_indices)\n\n# 노드와 엣지 색상을 설정\nnode_colors = ['r' if node in selected_indices and is_fraud_values[selected_indices.get_loc(node)] == 1 else 'k' for node in subgraph.nodes()]\n\n# 그래프 레이아웃 설정\npos = nx.spring_layout(subgraph, seed=42)\n\n# 그래프 그리기\nplt.figure(figsize=(10, 5))\nnx.draw_networkx_nodes(subgraph, pos, node_size=200, node_color=node_colors)\nnx.draw_networkx_edges(subgraph, pos, edge_color='gray')\nnx.draw_networkx_labels(subgraph, pos, font_size=10)\n\n# 축 숨기기\nplt.axis('off')\n\n# 그래프 출력\nplt.show()\n\n\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# 첫 번째 데이터 선택\nselected_indices1 = df50[df50['cc_num'] == 4.302480e+15].index\nis_fraud_values1 = df50.loc[selected_indices1, 'is_fraud'].tolist()\n\n# 두 번째 데이터 선택\nselected_indices2 = df50[df50['cc_num'] == 4.2929e+18].index\nis_fraud_values2 = df50.loc[selected_indices2, 'is_fraud'].tolist()\n\n# PyTorch Geometric 그래프를 NetworkX 그래프로 변환\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\n\n# 서브그래프 추출\nsubgraph1 = G.subgraph(selected_indices1)\nsubgraph2 = G.subgraph(selected_indices2)\n\n# 노드 색상 설정\nnode_colors1 = ['r' if node in selected_indices1 and is_fraud_values1[selected_indices1.get_loc(node)] == 1 else 'k' for node in subgraph1.nodes()]\nnode_colors2 = ['r' if node in selected_indices2 and is_fraud_values2[selected_indices2.get_loc(node)] == 1 else 'k' for node in subgraph2.nodes()]\n\n# 엣지 색상 설정\nedge_colors1 = ['g' if edge in subgraph1.edges() else 'k' for edge in subgraph1.edges()]\nedge_colors2 = ['b' if edge in subgraph2.edges() else 'k' for edge in subgraph2.edges()]\n\n# 그래프 레이아웃 설정\npos1 = nx.spring_layout(subgraph1, seed=42)\npos2 = nx.spring_layout(subgraph2, seed=42)\n\n# 그래프 그리기\nplt.figure(figsize=(12, 6))\n\n# 그래프 1 그리기\nnx.draw_networkx_nodes(subgraph1, pos1, node_size=200, node_color=node_colors1)\nnx.draw_networkx_edges(subgraph1, pos1, edge_color=edge_colors1)\n\n# 그래프 2 그리기\nnx.draw_networkx_nodes(subgraph2, pos2, node_size=200, node_color=node_colors2)\nnx.draw_networkx_edges(subgraph2, pos2, edge_color=edge_colors2)\n\n# 축 숨기기\nplt.axis('off')\n\n# 그래프 출력\nplt.show()\n\n\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\n\n# 스타일 설정 (ggplot 스타일 사용)\nstyle.use('ggplot')\n\n# 첫 번째 데이터 선택\nselected_indices1 = df50[df50['cc_num'] == 4.302480e+15].index\nis_fraud_values1 = df50.loc[selected_indices1, 'is_fraud'].tolist()\n\n# 두 번째 데이터 선택\nselected_indices2 = df50[df50['cc_num'] == 4.2929e+18].index\nis_fraud_values2 = df50.loc[selected_indices2, 'is_fraud'].tolist()\n\n# PyTorch Geometric 그래프를 NetworkX 그래프로 변환\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\n\n# 서브그래프 추출\nsubgraph1 = G.subgraph(selected_indices1)\nsubgraph2 = G.subgraph(selected_indices2)\n\n# 노드 색상 설정\nnode_colors1 = ['r' if node in selected_indices1 and is_fraud_values1[selected_indices1.get_loc(node)] == 1 else 'k' for node in subgraph1.nodes()]\nnode_colors2 = ['r' if node in selected_indices2 and is_fraud_values2[selected_indices2.get_loc(node)] == 1 else 'k' for node in subgraph2.nodes()]\n\n# 엣지 색상 설정\nedge_colors1 = ['g' if edge in subgraph1.edges() else 'k' for edge in subgraph1.edges()]\nedge_colors2 = ['b' if edge in subgraph2.edges() else 'k' for edge in subgraph2.edges()]\n\n# 그래프 레이아웃 설정 (kamada_kawai 레이아웃 사용)\npos1 = nx.kamada_kawai_layout(subgraph1)\npos2 = nx.kamada_kawai_layout(subgraph2)\n\n# 그래프 그리기\nplt.figure(figsize=(12, 6))\n\n# 그래프 1 그리기\nnx.draw_networkx_nodes(subgraph1, pos1, node_size=200, node_color=node_colors1, node_shape='o')  # 노드 모양: 원 (circle)\nnx.draw_networkx_edges(subgraph1, pos1, edge_color=edge_colors1)\n\n# 그래프 2 그리기\nnx.draw_networkx_nodes(subgraph2, pos2, node_size=200, node_color=node_colors2, node_shape='D')  # 노드 모양: 다이아몬드 (diamond)\nnx.draw_networkx_edges(subgraph2, pos2, edge_color=edge_colors2)\n\n# 범례 추가\nlegend_elements = [\n    plt.Line2D([0], [0], marker='o', color='g', markerfacecolor='black', markersize=10, label='cc_num 1'),\n    plt.Line2D([0], [0], marker='D', color='b', markerfacecolor='black', markersize=10, label='cc_num 2'),\n    plt.Line2D([0], [0], marker='o', color='black', markerfacecolor='r', markersize=10, label='fraud=1'),\n]\nplt.legend(handles=legend_elements, loc='lower center', ncol=3, frameon=True)\n\n# 그래프 출력\nplt.axis('off')\nplt.show()\n\n\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\n\n# 스타일 설정 (ggplot 스타일 사용)\nstyle.use('ggplot')\n\n# 첫 번째 데이터 선택\nselected_indices1 = df50[df50['cc_num'] == 4.302480e+15].index\nis_fraud_values1 = df50.loc[selected_indices1, 'is_fraud'].tolist()\n\n# 두 번째 데이터 선택\nselected_indices2 = df50[df50['cc_num'] == 4.2929e+18].index\nis_fraud_values2 = df50.loc[selected_indices2, 'is_fraud'].tolist()\n\n# PyTorch Geometric 그래프를 NetworkX 그래프로 변환\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\n\n# 서브그래프 추출\nsubgraph1 = G.subgraph(selected_indices1)\nsubgraph2 = G.subgraph(selected_indices2)\n\n# 노드 색상 설정\nnode_colors1 = ['r' if node in selected_indices1 and is_fraud_values1[selected_indices1.get_loc(node)] == 1 else 'k' for node in subgraph1.nodes()]\nnode_colors2 = ['r' if node in selected_indices2 and is_fraud_values2[selected_indices2.get_loc(node)] == 1 else 'k' for node in subgraph2.nodes()]\n\n# 엣지 색상 설정\nedge_colors1 = ['g' if edge in subgraph1.edges() else 'k' for edge in subgraph1.edges()]\nedge_colors2 = ['b' if edge in subgraph2.edges() else 'k' for edge in subgraph2.edges()]\n\n# 그래프 레이아웃 설정 (kamada_kawai 레이아웃 사용)\npos1 = nx.kamada_kawai_layout(subgraph1)\npos2 = nx.kamada_kawai_layout(subgraph2)\n\n# 그래프 그리기\nplt.figure(figsize=(12, 6))\n\n# 그래프 1 그리기\nnx.draw_networkx_nodes(subgraph1, pos1, node_size=200, node_color=node_colors1, node_shape='o')  # 노드 모양: 원 (circle)\nnx.draw_networkx_edges(subgraph1, pos1, edge_color=edge_colors1)\n\n# 그래프 2 그리기\nnx.draw_networkx_nodes(subgraph2, pos2, node_size=200, node_color=node_colors2, node_shape='D')  # 노드 모양: 다이아몬드 (diamond)\nnx.draw_networkx_edges(subgraph2, pos2, edge_color=edge_colors2)\n\n# 범례 추가\nlegend_elements = [\n    plt.Line2D([0], [0], marker='o', color='g', markerfacecolor='black', markersize=10, label='cc_num 1'),\n    plt.Line2D([0], [0], marker='D', color='b', markerfacecolor='black', markersize=10, label='cc_num 2'),\n    plt.Line2D([0], [0], marker='o', color='black', markerfacecolor='r', markersize=10, label='fraud=1'),\n]\nlegend = plt.legend(handles=legend_elements, loc='lower center', ncol=3, frameon=True)\nlegend.set_bbox_to_anchor((0.5, -0.1))  # 범례 위치 조정\n\n# 그래프 출력\nplt.axis('off')\nplt.show()"
  },
  {
    "objectID": "posts/230930 데이터(13, df50 auto_best).html",
    "href": "posts/230930 데이터(13, df50 auto_best).html",
    "title": "[FRAUD] 데이터정리 시도(9.30_df50 autogluon_best)",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\nimport xgboost as xgb\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# gnn\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric\nfrom torch_geometric.nn import GCNConv\n\n# autogluon\nfrom autogluon.tabular import TabularDataset, TabularPredictor\n\n\ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef compute_time_difference(group):\n    n = len(group)\n    result = []\n    for i in range(n):\n        for j in range(n):\n            time_difference = abs(group.iloc[i].trans_date_trans_time.value - group.iloc[j].trans_date_trans_time.value)\n            result.append([group.iloc[i].name, group.iloc[j].name, time_difference])\n    return result\n\n\nclass GCN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 16)\n        self.conv2 = GCNConv(16,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\n\nfraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\n\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\ndf50 = df50.reset_index()\n\n\nN = len(df50)\n\n\ndf50 = df50[[\"amt\",\"is_fraud\"]]\n\n\ndf50[\"amt\"].mean()\n\n297.4638911088911\n\n\n\ndf50[\"amt\"].describe()\n\ncount    12012.000000\nmean       297.463891\nstd        384.130842\nmin          1.010000\n25%         19.917500\n50%         84.680000\n75%        468.295000\nmax      12025.300000\nName: amt, dtype: float64\n\n\n\n\n\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.shape, df50_test.shape\n\n((9009, 2), (3003, 2))\n\n\n\ntrain_mask = [i in df50_tr.index for i in range(N)]\ntest_mask = [i in df50_test.index for i in range(N)]\n\n\ntrain_mask = np.array(train_mask)\ntest_mask = np.array(test_mask)\n\n\ntrain_mask.sum(), test_mask.sum()\n\n(9009, 3003)\n\n\n\ntrain_mask.shape, test_mask.shape\n\n((12012,), (12012,))\n\n\n\n\n\n\n\n# groups = df50.groupby('cc_num')\n# edge_index_list_plus = [compute_time_difference(group) for _, group in groups]\n# edge_index_list_plus_flat = [item for sublist in edge_index_list_plus for item in sublist]\n# edge_index_list_plus_nparr = np.array(edge_index_list_plus_flat)\n# np.save('edge_index_list_plus50.npy', edge_index_list_plus_nparr)\n\n\nedge_index = np.load('edge_index_list_plus50.npy')\ntheta = edge_index[:,2].mean()\nedge_index = np.load('edge_index_list_plus50.npy').astype(np.float64)\nedge_index[:,2] = (np.exp(-edge_index[:,2]/theta) != 1)*(np.exp(-edge_index[:,2]/theta))\nedge_index = edge_index.tolist()\nmean_ = np.array(edge_index)[:,2].mean()\nselected_edges = [(int(row[0]), int(row[1])) for row in edge_index if row[2] &gt; mean_]\nedge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\n\n\n\n\n\n\nx = torch.tensor(df50['amt'], dtype=torch.float).reshape(-1,1)\ny = torch.tensor(df50['is_fraud'],dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index = edge_index_selected, y=y, train_mask = train_mask, test_mask = test_mask)\ndata\n\nData(x=[12012, 1], edge_index=[2, 93730], y=[12012], train_mask=[12012], test_mask=[12012])"
  },
  {
    "objectID": "posts/230930 데이터(13, df50 auto_best).html#데이터정리",
    "href": "posts/230930 데이터(13, df50 auto_best).html#데이터정리",
    "title": "[FRAUD] 데이터정리 시도(9.30_df50 autogluon_best)",
    "section": "",
    "text": "_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\ndf50 = df50.reset_index()\n\n\nN = len(df50)\n\n\ndf50 = df50[[\"amt\",\"is_fraud\"]]\n\n\ndf50[\"amt\"].mean()\n\n297.4638911088911\n\n\n\ndf50[\"amt\"].describe()\n\ncount    12012.000000\nmean       297.463891\nstd        384.130842\nmin          1.010000\n25%         19.917500\n50%         84.680000\n75%        468.295000\nmax      12025.300000\nName: amt, dtype: float64\n\n\n\n\n\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.shape, df50_test.shape\n\n((9009, 2), (3003, 2))\n\n\n\ntrain_mask = [i in df50_tr.index for i in range(N)]\ntest_mask = [i in df50_test.index for i in range(N)]\n\n\ntrain_mask = np.array(train_mask)\ntest_mask = np.array(test_mask)\n\n\ntrain_mask.sum(), test_mask.sum()\n\n(9009, 3003)\n\n\n\ntrain_mask.shape, test_mask.shape\n\n((12012,), (12012,))\n\n\n\n\n\n\n\n# groups = df50.groupby('cc_num')\n# edge_index_list_plus = [compute_time_difference(group) for _, group in groups]\n# edge_index_list_plus_flat = [item for sublist in edge_index_list_plus for item in sublist]\n# edge_index_list_plus_nparr = np.array(edge_index_list_plus_flat)\n# np.save('edge_index_list_plus50.npy', edge_index_list_plus_nparr)\n\n\nedge_index = np.load('edge_index_list_plus50.npy')\ntheta = edge_index[:,2].mean()\nedge_index = np.load('edge_index_list_plus50.npy').astype(np.float64)\nedge_index[:,2] = (np.exp(-edge_index[:,2]/theta) != 1)*(np.exp(-edge_index[:,2]/theta))\nedge_index = edge_index.tolist()\nmean_ = np.array(edge_index)[:,2].mean()\nselected_edges = [(int(row[0]), int(row[1])) for row in edge_index if row[2] &gt; mean_]\nedge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\n\n\n\n\n\n\nx = torch.tensor(df50['amt'], dtype=torch.float).reshape(-1,1)\ny = torch.tensor(df50['is_fraud'],dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index = edge_index_selected, y=y, train_mask = train_mask, test_mask = test_mask)\ndata\n\nData(x=[12012, 1], edge_index=[2, 93730], y=[12012], train_mask=[12012], test_mask=[12012])"
  },
  {
    "objectID": "posts/230930 데이터(13, df50 auto_best).html#a.-데이터",
    "href": "posts/230930 데이터(13, df50 auto_best).html#a.-데이터",
    "title": "[FRAUD] 데이터정리 시도(9.30_df50 autogluon_best)",
    "section": "A. 데이터",
    "text": "A. 데이터\n\ntr = TabularDataset(df50_tr)\ntst = TabularDataset(df50_test)"
  },
  {
    "objectID": "posts/230930 데이터(13, df50 auto_best).html#b.-predictor-생성",
    "href": "posts/230930 데이터(13, df50 auto_best).html#b.-predictor-생성",
    "title": "[FRAUD] 데이터정리 시도(9.30_df50 autogluon_best)",
    "section": "B. predictor 생성",
    "text": "B. predictor 생성\n\npredictr = TabularPredictor(\"is_fraud\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20230930_045601/\""
  },
  {
    "objectID": "posts/230930 데이터(13, df50 auto_best).html#c.적합fit",
    "href": "posts/230930 데이터(13, df50 auto_best).html#c.적합fit",
    "title": "[FRAUD] 데이터정리 시도(9.30_df50 autogluon_best)",
    "section": "C.적합(fit)",
    "text": "C.적합(fit)\n\npredictr.fit(tr, presets='best_quality')\n\nPresets specified: ['best_quality']\nStack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=1\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20230930_045601/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.8.18\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   749.06 GB / 982.82 GB (76.2%)\nTrain Data Rows:    9009\nTrain Data Columns: 1\nLabel Column: is_fraud\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [1, 0]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    31104.83 MB\n    Train Data (Original)  Memory Usage: 0.07 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', []) : 1 | ['amt']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', []) : 1 | ['amt']\n    0.0s = Fit runtime\n    1 features in original data used to generate 1 features in processed data.\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.04s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif_BAG_L1 ...\n    0.8782   = Validation score   (accuracy)\n    0.0s     = Training   runtime\n    0.01s    = Validation runtime\nFitting model: KNeighborsDist_BAG_L1 ...\n    0.8641   = Validation score   (accuracy)\n    0.0s     = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMXT_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.885    = Validation score   (accuracy)\n    0.49s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.894    = Validation score   (accuracy)\n    0.57s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: RandomForestGini_BAG_L1 ...\n    0.856    = Validation score   (accuracy)\n    0.34s    = Training   runtime\n    0.19s    = Validation runtime\nFitting model: RandomForestEntr_BAG_L1 ...\n    0.856    = Validation score   (accuracy)\n    0.44s    = Training   runtime\n    0.21s    = Validation runtime\nFitting model: CatBoost_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8947   = Validation score   (accuracy)\n    1.49s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesGini_BAG_L1 ...\n    0.8622   = Validation score   (accuracy)\n    0.33s    = Training   runtime\n    0.2s     = Validation runtime\nFitting model: ExtraTreesEntr_BAG_L1 ...\n    0.8626   = Validation score   (accuracy)\n    0.31s    = Training   runtime\n    0.2s     = Validation runtime\nFitting model: NeuralNetFastAI_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.867    = Validation score   (accuracy)\n    7.47s    = Training   runtime\n    0.09s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8944   = Validation score   (accuracy)\n    0.5s     = Training   runtime\n    0.02s    = Validation runtime\nFitting model: NeuralNetTorch_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8888   = Validation score   (accuracy)\n    14.26s   = Training   runtime\n    0.05s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8941   = Validation score   (accuracy)\n    0.85s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8948   = Validation score   (accuracy)\n    2.14s    = Training   runtime\n    0.01s    = Validation runtime\nAutoGluon training complete, total runtime = 38.84s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230930_045601/\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7f3fabaaf250&gt;\n\n\n\npredictr.leaderboard()\n\n                      model  score_val  pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0       WeightedEnsemble_L2   0.894772       0.019776   4.481569                0.009500           2.139285            2       True         14\n1           CatBoost_BAG_L1   0.894661       0.004185   1.490714                0.004185           1.490714            1       True          7\n2            XGBoost_BAG_L1   0.894439       0.021677   0.498835                0.021677           0.498835            1       True         11\n3      LightGBMLarge_BAG_L1   0.894106       0.006091   0.851570                0.006091           0.851570            1       True         13\n4           LightGBM_BAG_L1   0.893995       0.013288   0.571850                0.013288           0.571850            1       True          4\n5     NeuralNetTorch_BAG_L1   0.888778       0.050316  14.255491                0.050316          14.255491            1       True         12\n6         LightGBMXT_BAG_L1   0.885004       0.034434   0.492139                0.034434           0.492139            1       True          3\n7     KNeighborsUnif_BAG_L1   0.878233       0.010086   0.003595                0.010086           0.003595            1       True          1\n8    NeuralNetFastAI_BAG_L1   0.867022       0.086901   7.465914                0.086901           7.465914            1       True         10\n9     KNeighborsDist_BAG_L1   0.864136       0.008126   0.002526                0.008126           0.002526            1       True          2\n10    ExtraTreesEntr_BAG_L1   0.862582       0.202717   0.313634                0.202717           0.313634            1       True          9\n11    ExtraTreesGini_BAG_L1   0.862249       0.202180   0.328506                0.202180           0.328506            1       True          8\n12  RandomForestGini_BAG_L1   0.856033       0.187426   0.337518                0.187426           0.337518            1       True          5\n13  RandomForestEntr_BAG_L1   0.856033       0.210448   0.443938                0.210448           0.443938            1       True          6\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nWeightedEnsemble_L2\n0.894772\n0.019776\n4.481569\n0.009500\n2.139285\n2\nTrue\n14\n\n\n1\nCatBoost_BAG_L1\n0.894661\n0.004185\n1.490714\n0.004185\n1.490714\n1\nTrue\n7\n\n\n2\nXGBoost_BAG_L1\n0.894439\n0.021677\n0.498835\n0.021677\n0.498835\n1\nTrue\n11\n\n\n3\nLightGBMLarge_BAG_L1\n0.894106\n0.006091\n0.851570\n0.006091\n0.851570\n1\nTrue\n13\n\n\n4\nLightGBM_BAG_L1\n0.893995\n0.013288\n0.571850\n0.013288\n0.571850\n1\nTrue\n4\n\n\n5\nNeuralNetTorch_BAG_L1\n0.888778\n0.050316\n14.255491\n0.050316\n14.255491\n1\nTrue\n12\n\n\n6\nLightGBMXT_BAG_L1\n0.885004\n0.034434\n0.492139\n0.034434\n0.492139\n1\nTrue\n3\n\n\n7\nKNeighborsUnif_BAG_L1\n0.878233\n0.010086\n0.003595\n0.010086\n0.003595\n1\nTrue\n1\n\n\n8\nNeuralNetFastAI_BAG_L1\n0.867022\n0.086901\n7.465914\n0.086901\n7.465914\n1\nTrue\n10\n\n\n9\nKNeighborsDist_BAG_L1\n0.864136\n0.008126\n0.002526\n0.008126\n0.002526\n1\nTrue\n2\n\n\n10\nExtraTreesEntr_BAG_L1\n0.862582\n0.202717\n0.313634\n0.202717\n0.313634\n1\nTrue\n9\n\n\n11\nExtraTreesGini_BAG_L1\n0.862249\n0.202180\n0.328506\n0.202180\n0.328506\n1\nTrue\n8\n\n\n12\nRandomForestGini_BAG_L1\n0.856033\n0.187426\n0.337518\n0.187426\n0.337518\n1\nTrue\n5\n\n\n13\nRandomForestEntr_BAG_L1\n0.856033\n0.210448\n0.443938\n0.210448\n0.443938\n1\nTrue\n6"
  },
  {
    "objectID": "posts/230930 데이터(13, df50 auto_best).html#d.-예측predict",
    "href": "posts/230930 데이터(13, df50 auto_best).html#d.-예측predict",
    "title": "[FRAUD] 데이터정리 시도(9.30_df50 autogluon_best)",
    "section": "D. 예측(predict)",
    "text": "D. 예측(predict)\n\n(tr.is_fraud == predictr.predict(tr)).mean()\n\n0.8967698967698968\n\n\n\n(tst.is_fraud == predictr.predict(tst)).mean()\n\n0.8877788877788878\n\n\n\n뭐지 best 옵션을 줬는데 더 낮아졌다."
  },
  {
    "objectID": "posts/0810.html",
    "href": "posts/0810.html",
    "title": "[FRAUD] 그래프자료로 데이터정리",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\nimport torch\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics \n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부     \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\ndef build_graph_tripartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n                                                       df[\"cc_num\"].values.tolist() +\n                                                       df[\"merchant\"].values.tolist()))}\n    df[\"in_node\"]= df[\"cc_num\"].apply(lambda x: mapping[x])\n    df[\"out_node\"]=df[\"merchant\"].apply(lambda x:mapping[x])\n    \n        \n    G=nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\\\n                        [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")     \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")   \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")  \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n\n    return G\n    \n    \ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef embedding(Graph):\n    # Graph -&gt; X (feature)\n    _edgs = list(Graph.edges)\n    subGraph = Graph.edge_subgraph([_edgs[x] for x in range(len(Graph.edges))]).copy()\n    subGraph.add_nodes_from(list(set(Graph.nodes) - set(subGraph.nodes)))    \n    embedded = AverageEmbedder(Node2Vec(subGraph, weight_key='weight').fit(window=10).wv)\n    X = [embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in range(len(Graph.edges))]\n    # Graph -&gt; y (label)\n    y = np.array(list(nx.get_edge_attributes(Graph, \"label\").values()))\n    return X,y \n\ndef anal(df):\n    Graph = build_graph_bipartite(df)\n    X,XX,y,yy = embedding(Graph)\n    lrnr = RandomForestClassifier(n_estimators=100, random_state=42) \n    lrnr.fit(X,y)\n    yyhat = lrnr.predict(XX)\n    df = pd.DataFrame({\n        'acc':[sklearn.metrics.accuracy_score(yy,yyhat)], \n        'pre':[sklearn.metrics.precision_score(yy,yyhat)], \n        'rec':[sklearn.metrics.recall_score(yy,yyhat)],\n        'f1':[sklearn.metrics.f1_score(yy,yyhat)]}\n    )    \n    return df\n\ndef our_sampling1(df):\n    cus_list = set(df.query('is_fraud==1').cc_num.tolist())\n    return df.query(\"cc_num in @ cus_list\")\n\n- 모든엣지를 고려\n\n# N = 10 \n# edge_index = torch.tensor([[i,j] for i in range(N) for j in range(N)]).T\n# # edge_attr = 그래프의 웨이트 \n\n\n# edge_index\n\ntensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2,\n         2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4,\n         4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7,\n         7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9,\n         9, 9, 9, 9],\n        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3,\n         4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7,\n         8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1,\n         2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5,\n         6, 7, 8, 9]])\n\n\n\nfraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\n# diff = fraudTrain.trans_date_trans_time[101]-fraudTrain.trans_date_trans_time[0]\n\n\n# diff\n\nTimedelta('0 days 01:17:00')\n\n\n\n# diff.total_seconds()\n\n4620.0\n\n\n\n# theta = 86400*1.2\n# theta\n\n103680.0\n\n\n\n# theta = 86400*1.2\n# np.exp(-diff.total_seconds()/theta)\n\n0.9564180361647693\n\n\n\n# !git add .\n# !git commit -m. \n# !git push \n# !quarto publish --no-browser --no-prompt\n\n\n\n\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\nN = len(fraudTrain)\nN\n\n1048575\n\n\n- 시도1\n\nedge_index_list = []\nfor i in range(N):\n    for j in range(N):\n        time_difference = (fraudTrain['trans_date_trans_time'][i] - fraudTrain['trans_date_trans_time'][j]).total_seconds()\n        edge_index_list.append([i, j, time_difference])\n\n\nedge_index = torch.tensor(edge_index_list).T\n\n- 시도2\n\nN = len(fraudTrain)\nedge_index = torch.tensor([[i,j] for i in range(N) for j in range(N)]).T\n# edge_attr = 그래프의 웨이트 \n\n\n너~무 오래걸린다.\n\n- 시도3\n\ndf02을 이용해서 해보자.\n\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\n214520*214520\n\n46018830400\n\n\n\n# N = len(df02)\n# edge_index = torch.tensor([[i,j] for i in range(N) for j in range(N)]).T\n\n- 시도4: df50\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\n12012*12012\n\n144288144\n\n\n\ndf50 = df50.reset_index()\n\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.is_fraud.mean().round(5), df50_test.is_fraud.mean().round(5)\n\n(0.49828, 0.50516)\n\n\n\ndf50_tr.shape, df50_test.shape\n\n((9009, 23), (3003, 23))\n\n\n\nN = len(df50_tr)\nedge_index = torch.tensor([[i,j] for i in range(N) for j in range(N)]).T\nedge_index\n\ntensor([[   0,    0,    0,  ..., 9008, 9008, 9008],\n        [   0,    1,    2,  ..., 9006, 9007, 9008]])\n\n\n\ndf50_tr = df50_tr.reset_index()\n\n\nedge_index_list = []\nfor i in range(N):\n    for j in range(N):\n        time_difference = (df50_tr['trans_date_trans_time'][i] - df50_tr['trans_date_trans_time'][j]).total_seconds()\n        edge_index_list.append([i, j, time_difference])\n\n\nedge_index = np.array(edge_index_list)\n\n\nedge_index.shape\n\n(81162081, 3)\n\n\n\nedge_index[:,2] = np.abs(edge_index[:,2])\n\n\ntheta = edge_index[:,2].mean()\ntheta\n\n12230796.273867842\n\n\n\nedge_index[:,2] = (np.exp(-edge_index[:,2]/theta)!=1) * np.exp(-edge_index[:,2]/theta)\n\n\nedge_index\n\narray([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 1.00000000e+00, 1.90157975e-01],\n       [0.00000000e+00, 2.00000000e+00, 9.79646259e-02],\n       ...,\n       [9.00800000e+03, 9.00600000e+03, 6.60662164e-01],\n       [9.00800000e+03, 9.00700000e+03, 1.49150646e-01],\n       [9.00800000e+03, 9.00800000e+03, 0.00000000e+00]])\n\n\n\neee = edge_index[:,:]\n\n\neee[:,1]\n\narray([0.000e+00, 1.000e+00, 2.000e+00, ..., 9.006e+03, 9.007e+03,\n       9.008e+03])\n\n\n\nedge_index_list_updated = edge_index.tolist()\n\n\nedge_index_list_updated[:5]\n\n[[0.0, 0.0, 0.0],\n [0.0, 1.0, 0.19015797528259762],\n [0.0, 2.0, 0.09796462590589798],\n [0.0, 3.0, 0.1424157407389685],\n [0.0, 4.0, 0.11107338192969567]]\n\n\n\ndf50_tr\n\n\n\n\n\n\n\n\nlevel_0\nindex\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n476\n51331\n2019-01-31 00:44:00\n3.543590e+15\nfraud_Medhurst PLC\nshopping_net\n921.24\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nc8928ba53be26fdd997b26f7130c757e\n1327970678\n40.064488\n-78.210499\n1\n\n\n1\n3671\n625691\n2019-09-23 00:09:00\n2.610530e+15\nfraud_Torphy-Goyette\nshopping_pos\n698.28\nTanya\nDickerson\nF\n...\n36.2416\n-86.6117\n22191\nPrison officer\n1994-07-27\n90453290b765904ed1c3426882a6788b\n1348358993\n35.884288\n-87.513318\n1\n\n\n2\n6641\n896244\n2019-12-25 21:30:00\n6.011330e+15\nfraud_Monahan-Morar\npersonal_care\n220.56\nLauren\nButler\nF\n...\n36.0557\n-96.0602\n413574\nTeacher, special educational needs\n1971-09-01\n4072a3effcf51cf7cf88f69d00642cd9\n1356471044\n35.789798\n-95.859736\n0\n\n\n3\n4288\n717690\n2019-11-02 22:22:00\n6.011380e+15\nfraud_Daugherty, Pouros and Beahan\nshopping_pos\n905.43\nMartin\nDuarte\nM\n...\n44.6001\n-84.2931\n864\nGeneral practice doctor\n1942-05-04\nf2fa1b25eef2f43fa5c09e3e1bfe7f77\n1351894926\n44.652759\n-84.500359\n1\n\n\n4\n4770\n815813\n2019-12-08 02:50:00\n4.430880e+15\nfraud_Hudson-Ratke\ngrocery_pos\n307.98\nAlicia\nMorales\nF\n...\n39.3199\n-106.6596\n61\nPublic relations account executive\n1939-11-04\nf06eff8da349e36e623cff026de8e970\n1354935056\n38.389399\n-106.111026\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9004\n11964\n177703\n2019-04-02 21:48:00\n3.572980e+15\nfraud_Ziemann-Waters\nhealth_fitness\n63.89\nWilliam\nLopez\nM\n...\n41.1832\n-96.9882\n614\nAssociate Professor\n1967-06-20\n5b19aad28d65a6b0a912fa7b9d1896de\n1333403300\n42.067169\n-96.876892\n0\n\n\n9005\n5191\n921796\n2019-12-30 23:29:00\n6.762920e+11\nfraud_Wiza, Schaden and Stark\nmisc_pos\n51.41\nLisa\nFitzpatrick\nF\n...\n41.2336\n-75.2389\n104\nFinancial trader\n1927-08-25\nb2a9e44026fc57e54b4e45ade6017668\n1356910178\n40.502189\n-74.814956\n1\n\n\n9006\n5390\n950365\n2020-01-16 03:15:00\n4.807550e+12\nfraud_Murray-Smitham\ngrocery_pos\n357.62\nKimberly\nCastro\nF\n...\n40.2158\n-83.9579\n133\nProfessor Emeritus\n1954-01-29\n4bfa37c329f327074e7220ea6e5d8f8d\n1358306148\n40.620284\n-84.274495\n1\n\n\n9007\n860\n88685\n2019-02-22 02:19:00\n5.738600e+11\nfraud_McDermott-Weimann\ngrocery_pos\n304.75\nCristian\nJones\nM\n...\n42.0765\n-87.7246\n27020\nTrade mark attorney\n1986-07-23\na1c3025ddb615ab2ef890bf82fc3d66a\n1329877195\n42.722479\n-88.362364\n1\n\n\n9008\n7270\n753787\n2019-11-18 10:58:00\n6.042293e+10\nfraud_Terry, Johns and Bins\nmisc_pos\n1.64\nJeffrey\nPowers\nM\n...\n33.6028\n-81.9748\n46944\nSecondary school teacher\n1942-04-02\nee10d61782bde2b5cabc2ad649e977cc\n1353236287\n34.243599\n-82.971344\n0\n\n\n\n\n9009 rows × 24 columns\n\n\n\n\ncc_num로 그룹별로 묶자.\n\n\ndf50_tr[df50_tr['cc_num']==3.543590e+15]\n\n\n\n\n\n\n\n\nlevel_0\nindex\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n476\n51331\n2019-01-31 00:44:00\n3.543590e+15\nfraud_Medhurst PLC\nshopping_net\n921.24\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nc8928ba53be26fdd997b26f7130c757e\n1327970678\n40.064488\n-78.210499\n1\n\n\n344\n462\n50905\n2019-01-30 16:53:00\n3.543590e+15\nfraud_Lesch Ltd\nshopping_pos\n881.11\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n9f7b7675c4decefd03cce56df045ed1c\n1327942400\n39.591484\n-79.575246\n1\n\n\n1377\n6607\n814736\n2019-12-07 22:17:00\n3.543590e+15\nfraud_Botsford and Sons\nhome\n10.41\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\naa9b533e84970309a4ad60a914a8cd77\n1354918668\n41.287791\n-79.980592\n0\n\n\n1447\n485\n51816\n2019-01-31 12:38:00\n3.543590e+15\nfraud_Ruecker-Mayert\nkids_pets\n21.93\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\ncec656f154e0978b0f26702c29ddeeca\n1328013517\n39.946187\n-78.078864\n1\n\n\n1639\n11176\n12947\n2019-01-08 11:08:00\n3.543590e+15\nfraud_Stroman, Hudson and Erdman\ngas_transport\n76.03\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nc10451edc4e21b865d049312acf18ecd\n1326020892\n39.503960\n-78.471680\n0\n\n\n2046\n8124\n627045\n2019-09-23 12:53:00\n3.543590e+15\nfraud_Botsford Ltd\nshopping_pos\n3.20\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n003d591d208f7ee52277b5cc4fa4a37f\n1348404838\n40.066686\n-79.326630\n0\n\n\n2093\n477\n51367\n2019-01-31 01:36:00\n3.543590e+15\nfraud_Watsica, Haag and Considine\nshopping_pos\n1090.67\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nb42bc0820a78de54845c5138b9c39dd5\n1327973774\n40.923284\n-78.882504\n1\n\n\n2415\n491\n52402\n2019-01-31 22:17:00\n3.543590e+15\nfraud_Metz, Russel and Metz\nkids_pets\n22.35\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n51f9352216e99bbe9e8b03b082305971\n1328048275\n39.979547\n-78.851379\n1\n\n\n2625\n463\n51047\n2019-01-30 19:35:00\n3.543590e+15\nfraud_Ruecker-Mayert\nkids_pets\n22.95\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n8e804422b761537e3a49a237afd1ea9a\n1327952100\n40.051981\n-79.021769\n1\n\n\n2769\n478\n51374\n2019-01-31 01:42:00\n3.543590e+15\nfraud_Schmidt and Sons\nshopping_net\n1043.59\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nbbe4e9e431cba66e6531199ffaf79657\n1327974178\n40.192896\n-79.366393\n1\n\n\n3192\n505\n52522\n2019-01-31 23:57:00\n3.543590e+15\nfraud_Kutch, Steuber and Gerhold\nfood_dining\n116.45\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nfcf46ca0264437bbb938c29eca2c92ad\n1328054256\n40.288401\n-78.286914\n1\n\n\n3670\n11714\n1010269\n2020-02-20 06:02:00\n3.543590e+15\nfraud_Huels-Hahn\ngas_transport\n51.80\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nb72c0124f4c5662db13e1bea2f04784b\n1361340164\n39.672719\n-79.642589\n0\n\n\n3945\n6087\n243892\n2019-05-02 13:38:00\n3.543590e+15\nfraud_Cruickshank-Mills\nentertainment\n5.72\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n990da059d387e5fa7481d76ff5c29199\n1335965925\n40.577553\n-79.315460\n0\n\n\n5017\n484\n51431\n2019-01-31 03:28:00\n3.543590e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n741.98\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n41312d7d5fc76be3782b5e9cef04726f\n1327980509\n41.290570\n-79.682069\n1\n\n\n5505\n8148\n181398\n2019-04-04 23:32:00\n3.543590e+15\nfraud_Feil, Hilpert and Koss\nfood_dining\n89.23\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\ne304fd4ebc897fce190925dadcd2b524\n1333582347\n39.736380\n-79.481667\n0\n\n\n5729\n11116\n329202\n2019-06-06 03:26:00\n3.543590e+15\nfraud_Connelly, Reichert and Fritsch\ngas_transport\n69.36\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nbc0832ac8bac6d26548ab6ab553d5d5e\n1338953171\n40.780469\n-79.668417\n0\n\n\n7605\n481\n51392\n2019-01-31 02:16:00\n3.543590e+15\nfraud_Huels-Hahn\ngas_transport\n12.41\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n5f4379c2fc20457f0f99a126cadda1af\n1327976216\n39.884234\n-79.374966\n1\n\n\n7800\n8609\n55920\n2019-02-03 06:51:00\n3.543590e+15\nfraud_Corwin-Gorczany\nmisc_net\n6.70\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n90f33381b6b6644c6d03c8cdb51d05dc\n1328251865\n40.064532\n-78.920283\n0\n\n\n8100\n10488\n509733\n2019-08-09 11:47:00\n3.543590e+15\nfraud_Kutch and Sons\ngrocery_pos\n108.74\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n40a620cc7c5ba396b1fe112f5361e4a9\n1344512838\n40.057443\n-78.569798\n0\n\n\n8313\n504\n52514\n2019-01-31 23:52:00\n3.543590e+15\nfraud_Douglas, Schneider and Turner\nshopping_pos\n1129.56\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nec208107f178422e0953560343d0cf8b\n1328053975\n40.840340\n-78.027854\n1\n\n\n\n\n20 rows × 24 columns\n\n\n\n\ndf50_grouped=df50_tr.groupby(by='cc_num')\n\n\nedge_index_list = []\nfor i in range(N):\n    for j in range(N):\n        if df50_tr['cc_num'][i] != df50_tr['cc_num'][j]:  # cc_num 값이 같다면\n            time_difference = 0\n        else:\n            time_difference = (df50_tr['trans_date_trans_time'][i] - df50_tr['trans_date_trans_time'][j]).total_seconds()\n        edge_index_list.append([i, j, time_difference])\n\n\nedge_index = np.array(edge_index_list)\n\n\nedge_index.shape\n\n(81162081, 3)\n\n\n\nedge_index\n\narray([[0.000e+00, 0.000e+00, 0.000e+00],\n       [0.000e+00, 1.000e+00, 0.000e+00],\n       [0.000e+00, 2.000e+00, 0.000e+00],\n       ...,\n       [9.008e+03, 9.006e+03, 0.000e+00],\n       [9.008e+03, 9.007e+03, 0.000e+00],\n       [9.008e+03, 9.008e+03, 0.000e+00]])\n\n\n\nedge_index[:,2] = np.abs(edge_index[:,2])\n\n\ntheta = edge_index[:,2].mean()\ntheta\n\n10988.585252761077\n\n\n\nedge_index\n\narray([[0.000e+00, 0.000e+00, 0.000e+00],\n       [0.000e+00, 1.000e+00, 0.000e+00],\n       [0.000e+00, 2.000e+00, 0.000e+00],\n       ...,\n       [9.008e+03, 9.006e+03, 0.000e+00],\n       [9.008e+03, 9.007e+03, 0.000e+00],\n       [9.008e+03, 9.008e+03, 0.000e+00]])\n\n\n\nedge_index[:,2] = (np.exp(-edge_index[:,2]/theta)!=1) * np.exp(-edge_index[:,2]/theta)\n\n\nedge_index\n\narray([[0.000e+00, 0.000e+00, 0.000e+00],\n       [0.000e+00, 1.000e+00, 0.000e+00],\n       [0.000e+00, 2.000e+00, 0.000e+00],\n       ...,\n       [9.008e+03, 9.006e+03, 0.000e+00],\n       [9.008e+03, 9.007e+03, 0.000e+00],\n       [9.008e+03, 9.008e+03, 0.000e+00]])\n\n\n\nedge_index_list_updated = edge_index.tolist()\n\n\nnp.array(edge_index_list_updated)[:,2].mean()\n\n8.344409093328692e-05\n\n\n\nmm = np.array(edge_index_list_updated)[:,2].mean()\n\nedge_index_list_updated가 w\n\nselected_edges = [(int(row[0]), int(row[1])) for row in edge_index_list_updated if row[2] &gt; mm]\n\n\nedge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\n\n\nedge_index_selected.shape\n\ntorch.Size([2, 28472])\n\n\n\nx = df50_tr['amt']\n\n\nx\n\n0       921.24\n1       698.28\n2       220.56\n3       905.43\n4       307.98\n         ...  \n9004     63.89\n9005     51.41\n9006    357.62\n9007    304.75\n9008      1.64\nName: amt, Length: 9009, dtype: float64\n\n\n\na = torch.tensor(x, dtype=torch.float)\n\n\na = a.reshape(-1,1)\na\n\ntensor([[921.2400],\n        [698.2800],\n        [220.5600],\n        ...,\n        [357.6200],\n        [304.7500],\n        [  1.6400]])\n\n\n\ny = df50_tr['is_fraud']\n\n\nb = torch.tensor(y,dtype=torch.int64)\n\n\nb\n\ntensor([1, 1, 0,  ..., 1, 1, 0])\n\n\n\nimport torch_geometric\n\n\ndata = torch_geometric.data.Data(x=a, edge_index = edge_index_selected, y=b)\n\n\ndata\n\nData(x=[9009, 1], edge_index=[2, 28472], y=[9009])\n\n\n\n예시처럼 edge_index 하려햇ㄴ는뎅.. 흠..\n\n- pyg lesson6\n\ngconv = torch_geometric.nn.GCNConv(1,4)\ngconv\n\nGCNConv(1, 4)\n\n\n\ngconv(data.x, data.edge_index)\n\ntensor([[-3.8013e+02,  4.0544e+02, -1.8002e+02,  3.0315e+02],\n        [-3.5198e+02,  3.7541e+02, -1.6669e+02,  2.8070e+02],\n        [-1.4832e+02,  1.5819e+02, -7.0240e+01,  1.1828e+02],\n        ...,\n        [-2.4048e+02,  2.5649e+02, -1.1389e+02,  1.9178e+02],\n        [-5.1728e+02,  5.5172e+02, -2.4497e+02,  4.1253e+02],\n        [-1.1028e+00,  1.1762e+00, -5.2228e-01,  8.7949e-01]],\n       grad_fn=&lt;AddBackward0&gt;)\n\n\n\nlist(gconv.parameters())\n\n[Parameter containing:\n tensor([0., 0., 0., 0.], requires_grad=True),\n Parameter containing:\n tensor([[-0.6724],\n         [ 0.7172],\n         [-0.3185],\n         [ 0.5363]], requires_grad=True)]\n\n\n\n_,W = list(gconv.parameters())\nW\n\nParameter containing:\ntensor([[-0.6724],\n        [ 0.7172],\n        [-0.3185],\n        [ 0.5363]], requires_grad=True)\n\n\n- pyg lesson5\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\n\nclass GCN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(9009, 16)\n        self.conv2 = GCNConv(16,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\n\nmodel = GCN()\n\n\nmodel\n\nGCN(\n  (conv1): GCNConv(9009, 16)\n  (conv2): GCNConv(16, 2)\n)\n\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\n\nGCN(\n  (conv1): GCNConv(9009, 16)\n  (conv2): GCNConv(16, 2)\n)\n\n\n\nfor epoch in range(200):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(data.x, data.y)\n    loss.backward()\n    optimizer.step()\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (9009x1 and 9009x16)\n\n\n\ndf50_test\n\n\ndf50_test \n\nNameError: name 'df50_test' is not defined\n\n\n\ndata\n\nData(x=[9009, 1], edge_index=[2, 28472], y=[9009])\n\n\nx=[9009, 1] : 9009개의 거래량이 있다. 특징은 걍.. 거래량 하나? &lt;- 거래자체에 대한 특징을 더 추가해도 좋을듯.. 흠.\nedge_index=[2, 28472]: 거래끼리의 edge값은 28472.. 일단 고객id가 같지 않으면 엣지값0으로 하고..\n\ndata.train_mask\n\nAttributeError: 'GlobalStorage' object has no attribute 'train_mask'"
  },
  {
    "objectID": "posts/0810.html#해보자",
    "href": "posts/0810.html#해보자",
    "title": "[FRAUD] 그래프자료로 데이터정리",
    "section": "",
    "text": "fraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\nN = len(fraudTrain)\nN\n\n1048575\n\n\n- 시도1\n\nedge_index_list = []\nfor i in range(N):\n    for j in range(N):\n        time_difference = (fraudTrain['trans_date_trans_time'][i] - fraudTrain['trans_date_trans_time'][j]).total_seconds()\n        edge_index_list.append([i, j, time_difference])\n\n\nedge_index = torch.tensor(edge_index_list).T\n\n- 시도2\n\nN = len(fraudTrain)\nedge_index = torch.tensor([[i,j] for i in range(N) for j in range(N)]).T\n# edge_attr = 그래프의 웨이트 \n\n\n너~무 오래걸린다.\n\n- 시도3\n\ndf02을 이용해서 해보자.\n\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\n214520*214520\n\n46018830400\n\n\n\n# N = len(df02)\n# edge_index = torch.tensor([[i,j] for i in range(N) for j in range(N)]).T\n\n- 시도4: df50\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\n12012*12012\n\n144288144\n\n\n\ndf50 = df50.reset_index()\n\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.is_fraud.mean().round(5), df50_test.is_fraud.mean().round(5)\n\n(0.49828, 0.50516)\n\n\n\ndf50_tr.shape, df50_test.shape\n\n((9009, 23), (3003, 23))\n\n\n\nN = len(df50_tr)\nedge_index = torch.tensor([[i,j] for i in range(N) for j in range(N)]).T\nedge_index\n\ntensor([[   0,    0,    0,  ..., 9008, 9008, 9008],\n        [   0,    1,    2,  ..., 9006, 9007, 9008]])\n\n\n\ndf50_tr = df50_tr.reset_index()\n\n\nedge_index_list = []\nfor i in range(N):\n    for j in range(N):\n        time_difference = (df50_tr['trans_date_trans_time'][i] - df50_tr['trans_date_trans_time'][j]).total_seconds()\n        edge_index_list.append([i, j, time_difference])\n\n\nedge_index = np.array(edge_index_list)\n\n\nedge_index.shape\n\n(81162081, 3)\n\n\n\nedge_index[:,2] = np.abs(edge_index[:,2])\n\n\ntheta = edge_index[:,2].mean()\ntheta\n\n12230796.273867842\n\n\n\nedge_index[:,2] = (np.exp(-edge_index[:,2]/theta)!=1) * np.exp(-edge_index[:,2]/theta)\n\n\nedge_index\n\narray([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 1.00000000e+00, 1.90157975e-01],\n       [0.00000000e+00, 2.00000000e+00, 9.79646259e-02],\n       ...,\n       [9.00800000e+03, 9.00600000e+03, 6.60662164e-01],\n       [9.00800000e+03, 9.00700000e+03, 1.49150646e-01],\n       [9.00800000e+03, 9.00800000e+03, 0.00000000e+00]])\n\n\n\neee = edge_index[:,:]\n\n\neee[:,1]\n\narray([0.000e+00, 1.000e+00, 2.000e+00, ..., 9.006e+03, 9.007e+03,\n       9.008e+03])\n\n\n\nedge_index_list_updated = edge_index.tolist()\n\n\nedge_index_list_updated[:5]\n\n[[0.0, 0.0, 0.0],\n [0.0, 1.0, 0.19015797528259762],\n [0.0, 2.0, 0.09796462590589798],\n [0.0, 3.0, 0.1424157407389685],\n [0.0, 4.0, 0.11107338192969567]]\n\n\n\ndf50_tr\n\n\n\n\n\n\n\n\nlevel_0\nindex\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n476\n51331\n2019-01-31 00:44:00\n3.543590e+15\nfraud_Medhurst PLC\nshopping_net\n921.24\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nc8928ba53be26fdd997b26f7130c757e\n1327970678\n40.064488\n-78.210499\n1\n\n\n1\n3671\n625691\n2019-09-23 00:09:00\n2.610530e+15\nfraud_Torphy-Goyette\nshopping_pos\n698.28\nTanya\nDickerson\nF\n...\n36.2416\n-86.6117\n22191\nPrison officer\n1994-07-27\n90453290b765904ed1c3426882a6788b\n1348358993\n35.884288\n-87.513318\n1\n\n\n2\n6641\n896244\n2019-12-25 21:30:00\n6.011330e+15\nfraud_Monahan-Morar\npersonal_care\n220.56\nLauren\nButler\nF\n...\n36.0557\n-96.0602\n413574\nTeacher, special educational needs\n1971-09-01\n4072a3effcf51cf7cf88f69d00642cd9\n1356471044\n35.789798\n-95.859736\n0\n\n\n3\n4288\n717690\n2019-11-02 22:22:00\n6.011380e+15\nfraud_Daugherty, Pouros and Beahan\nshopping_pos\n905.43\nMartin\nDuarte\nM\n...\n44.6001\n-84.2931\n864\nGeneral practice doctor\n1942-05-04\nf2fa1b25eef2f43fa5c09e3e1bfe7f77\n1351894926\n44.652759\n-84.500359\n1\n\n\n4\n4770\n815813\n2019-12-08 02:50:00\n4.430880e+15\nfraud_Hudson-Ratke\ngrocery_pos\n307.98\nAlicia\nMorales\nF\n...\n39.3199\n-106.6596\n61\nPublic relations account executive\n1939-11-04\nf06eff8da349e36e623cff026de8e970\n1354935056\n38.389399\n-106.111026\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9004\n11964\n177703\n2019-04-02 21:48:00\n3.572980e+15\nfraud_Ziemann-Waters\nhealth_fitness\n63.89\nWilliam\nLopez\nM\n...\n41.1832\n-96.9882\n614\nAssociate Professor\n1967-06-20\n5b19aad28d65a6b0a912fa7b9d1896de\n1333403300\n42.067169\n-96.876892\n0\n\n\n9005\n5191\n921796\n2019-12-30 23:29:00\n6.762920e+11\nfraud_Wiza, Schaden and Stark\nmisc_pos\n51.41\nLisa\nFitzpatrick\nF\n...\n41.2336\n-75.2389\n104\nFinancial trader\n1927-08-25\nb2a9e44026fc57e54b4e45ade6017668\n1356910178\n40.502189\n-74.814956\n1\n\n\n9006\n5390\n950365\n2020-01-16 03:15:00\n4.807550e+12\nfraud_Murray-Smitham\ngrocery_pos\n357.62\nKimberly\nCastro\nF\n...\n40.2158\n-83.9579\n133\nProfessor Emeritus\n1954-01-29\n4bfa37c329f327074e7220ea6e5d8f8d\n1358306148\n40.620284\n-84.274495\n1\n\n\n9007\n860\n88685\n2019-02-22 02:19:00\n5.738600e+11\nfraud_McDermott-Weimann\ngrocery_pos\n304.75\nCristian\nJones\nM\n...\n42.0765\n-87.7246\n27020\nTrade mark attorney\n1986-07-23\na1c3025ddb615ab2ef890bf82fc3d66a\n1329877195\n42.722479\n-88.362364\n1\n\n\n9008\n7270\n753787\n2019-11-18 10:58:00\n6.042293e+10\nfraud_Terry, Johns and Bins\nmisc_pos\n1.64\nJeffrey\nPowers\nM\n...\n33.6028\n-81.9748\n46944\nSecondary school teacher\n1942-04-02\nee10d61782bde2b5cabc2ad649e977cc\n1353236287\n34.243599\n-82.971344\n0\n\n\n\n\n9009 rows × 24 columns\n\n\n\n\ncc_num로 그룹별로 묶자.\n\n\ndf50_tr[df50_tr['cc_num']==3.543590e+15]\n\n\n\n\n\n\n\n\nlevel_0\nindex\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n476\n51331\n2019-01-31 00:44:00\n3.543590e+15\nfraud_Medhurst PLC\nshopping_net\n921.24\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nc8928ba53be26fdd997b26f7130c757e\n1327970678\n40.064488\n-78.210499\n1\n\n\n344\n462\n50905\n2019-01-30 16:53:00\n3.543590e+15\nfraud_Lesch Ltd\nshopping_pos\n881.11\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n9f7b7675c4decefd03cce56df045ed1c\n1327942400\n39.591484\n-79.575246\n1\n\n\n1377\n6607\n814736\n2019-12-07 22:17:00\n3.543590e+15\nfraud_Botsford and Sons\nhome\n10.41\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\naa9b533e84970309a4ad60a914a8cd77\n1354918668\n41.287791\n-79.980592\n0\n\n\n1447\n485\n51816\n2019-01-31 12:38:00\n3.543590e+15\nfraud_Ruecker-Mayert\nkids_pets\n21.93\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\ncec656f154e0978b0f26702c29ddeeca\n1328013517\n39.946187\n-78.078864\n1\n\n\n1639\n11176\n12947\n2019-01-08 11:08:00\n3.543590e+15\nfraud_Stroman, Hudson and Erdman\ngas_transport\n76.03\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nc10451edc4e21b865d049312acf18ecd\n1326020892\n39.503960\n-78.471680\n0\n\n\n2046\n8124\n627045\n2019-09-23 12:53:00\n3.543590e+15\nfraud_Botsford Ltd\nshopping_pos\n3.20\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n003d591d208f7ee52277b5cc4fa4a37f\n1348404838\n40.066686\n-79.326630\n0\n\n\n2093\n477\n51367\n2019-01-31 01:36:00\n3.543590e+15\nfraud_Watsica, Haag and Considine\nshopping_pos\n1090.67\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nb42bc0820a78de54845c5138b9c39dd5\n1327973774\n40.923284\n-78.882504\n1\n\n\n2415\n491\n52402\n2019-01-31 22:17:00\n3.543590e+15\nfraud_Metz, Russel and Metz\nkids_pets\n22.35\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n51f9352216e99bbe9e8b03b082305971\n1328048275\n39.979547\n-78.851379\n1\n\n\n2625\n463\n51047\n2019-01-30 19:35:00\n3.543590e+15\nfraud_Ruecker-Mayert\nkids_pets\n22.95\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n8e804422b761537e3a49a237afd1ea9a\n1327952100\n40.051981\n-79.021769\n1\n\n\n2769\n478\n51374\n2019-01-31 01:42:00\n3.543590e+15\nfraud_Schmidt and Sons\nshopping_net\n1043.59\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nbbe4e9e431cba66e6531199ffaf79657\n1327974178\n40.192896\n-79.366393\n1\n\n\n3192\n505\n52522\n2019-01-31 23:57:00\n3.543590e+15\nfraud_Kutch, Steuber and Gerhold\nfood_dining\n116.45\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nfcf46ca0264437bbb938c29eca2c92ad\n1328054256\n40.288401\n-78.286914\n1\n\n\n3670\n11714\n1010269\n2020-02-20 06:02:00\n3.543590e+15\nfraud_Huels-Hahn\ngas_transport\n51.80\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nb72c0124f4c5662db13e1bea2f04784b\n1361340164\n39.672719\n-79.642589\n0\n\n\n3945\n6087\n243892\n2019-05-02 13:38:00\n3.543590e+15\nfraud_Cruickshank-Mills\nentertainment\n5.72\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n990da059d387e5fa7481d76ff5c29199\n1335965925\n40.577553\n-79.315460\n0\n\n\n5017\n484\n51431\n2019-01-31 03:28:00\n3.543590e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n741.98\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n41312d7d5fc76be3782b5e9cef04726f\n1327980509\n41.290570\n-79.682069\n1\n\n\n5505\n8148\n181398\n2019-04-04 23:32:00\n3.543590e+15\nfraud_Feil, Hilpert and Koss\nfood_dining\n89.23\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\ne304fd4ebc897fce190925dadcd2b524\n1333582347\n39.736380\n-79.481667\n0\n\n\n5729\n11116\n329202\n2019-06-06 03:26:00\n3.543590e+15\nfraud_Connelly, Reichert and Fritsch\ngas_transport\n69.36\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nbc0832ac8bac6d26548ab6ab553d5d5e\n1338953171\n40.780469\n-79.668417\n0\n\n\n7605\n481\n51392\n2019-01-31 02:16:00\n3.543590e+15\nfraud_Huels-Hahn\ngas_transport\n12.41\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n5f4379c2fc20457f0f99a126cadda1af\n1327976216\n39.884234\n-79.374966\n1\n\n\n7800\n8609\n55920\n2019-02-03 06:51:00\n3.543590e+15\nfraud_Corwin-Gorczany\nmisc_net\n6.70\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n90f33381b6b6644c6d03c8cdb51d05dc\n1328251865\n40.064532\n-78.920283\n0\n\n\n8100\n10488\n509733\n2019-08-09 11:47:00\n3.543590e+15\nfraud_Kutch and Sons\ngrocery_pos\n108.74\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n40a620cc7c5ba396b1fe112f5361e4a9\n1344512838\n40.057443\n-78.569798\n0\n\n\n8313\n504\n52514\n2019-01-31 23:52:00\n3.543590e+15\nfraud_Douglas, Schneider and Turner\nshopping_pos\n1129.56\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nec208107f178422e0953560343d0cf8b\n1328053975\n40.840340\n-78.027854\n1\n\n\n\n\n20 rows × 24 columns\n\n\n\n\ndf50_grouped=df50_tr.groupby(by='cc_num')\n\n\nedge_index_list = []\nfor i in range(N):\n    for j in range(N):\n        if df50_tr['cc_num'][i] != df50_tr['cc_num'][j]:  # cc_num 값이 같다면\n            time_difference = 0\n        else:\n            time_difference = (df50_tr['trans_date_trans_time'][i] - df50_tr['trans_date_trans_time'][j]).total_seconds()\n        edge_index_list.append([i, j, time_difference])\n\n\nedge_index = np.array(edge_index_list)\n\n\nedge_index.shape\n\n(81162081, 3)\n\n\n\nedge_index\n\narray([[0.000e+00, 0.000e+00, 0.000e+00],\n       [0.000e+00, 1.000e+00, 0.000e+00],\n       [0.000e+00, 2.000e+00, 0.000e+00],\n       ...,\n       [9.008e+03, 9.006e+03, 0.000e+00],\n       [9.008e+03, 9.007e+03, 0.000e+00],\n       [9.008e+03, 9.008e+03, 0.000e+00]])\n\n\n\nedge_index[:,2] = np.abs(edge_index[:,2])\n\n\ntheta = edge_index[:,2].mean()\ntheta\n\n10988.585252761077\n\n\n\nedge_index\n\narray([[0.000e+00, 0.000e+00, 0.000e+00],\n       [0.000e+00, 1.000e+00, 0.000e+00],\n       [0.000e+00, 2.000e+00, 0.000e+00],\n       ...,\n       [9.008e+03, 9.006e+03, 0.000e+00],\n       [9.008e+03, 9.007e+03, 0.000e+00],\n       [9.008e+03, 9.008e+03, 0.000e+00]])\n\n\n\nedge_index[:,2] = (np.exp(-edge_index[:,2]/theta)!=1) * np.exp(-edge_index[:,2]/theta)\n\n\nedge_index\n\narray([[0.000e+00, 0.000e+00, 0.000e+00],\n       [0.000e+00, 1.000e+00, 0.000e+00],\n       [0.000e+00, 2.000e+00, 0.000e+00],\n       ...,\n       [9.008e+03, 9.006e+03, 0.000e+00],\n       [9.008e+03, 9.007e+03, 0.000e+00],\n       [9.008e+03, 9.008e+03, 0.000e+00]])\n\n\n\nedge_index_list_updated = edge_index.tolist()\n\n\nnp.array(edge_index_list_updated)[:,2].mean()\n\n8.344409093328692e-05\n\n\n\nmm = np.array(edge_index_list_updated)[:,2].mean()\n\nedge_index_list_updated가 w\n\nselected_edges = [(int(row[0]), int(row[1])) for row in edge_index_list_updated if row[2] &gt; mm]\n\n\nedge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\n\n\nedge_index_selected.shape\n\ntorch.Size([2, 28472])\n\n\n\nx = df50_tr['amt']\n\n\nx\n\n0       921.24\n1       698.28\n2       220.56\n3       905.43\n4       307.98\n         ...  \n9004     63.89\n9005     51.41\n9006    357.62\n9007    304.75\n9008      1.64\nName: amt, Length: 9009, dtype: float64\n\n\n\na = torch.tensor(x, dtype=torch.float)\n\n\na = a.reshape(-1,1)\na\n\ntensor([[921.2400],\n        [698.2800],\n        [220.5600],\n        ...,\n        [357.6200],\n        [304.7500],\n        [  1.6400]])\n\n\n\ny = df50_tr['is_fraud']\n\n\nb = torch.tensor(y,dtype=torch.int64)\n\n\nb\n\ntensor([1, 1, 0,  ..., 1, 1, 0])\n\n\n\nimport torch_geometric\n\n\ndata = torch_geometric.data.Data(x=a, edge_index = edge_index_selected, y=b)\n\n\ndata\n\nData(x=[9009, 1], edge_index=[2, 28472], y=[9009])\n\n\n\n예시처럼 edge_index 하려햇ㄴ는뎅.. 흠..\n\n- pyg lesson6\n\ngconv = torch_geometric.nn.GCNConv(1,4)\ngconv\n\nGCNConv(1, 4)\n\n\n\ngconv(data.x, data.edge_index)\n\ntensor([[-3.8013e+02,  4.0544e+02, -1.8002e+02,  3.0315e+02],\n        [-3.5198e+02,  3.7541e+02, -1.6669e+02,  2.8070e+02],\n        [-1.4832e+02,  1.5819e+02, -7.0240e+01,  1.1828e+02],\n        ...,\n        [-2.4048e+02,  2.5649e+02, -1.1389e+02,  1.9178e+02],\n        [-5.1728e+02,  5.5172e+02, -2.4497e+02,  4.1253e+02],\n        [-1.1028e+00,  1.1762e+00, -5.2228e-01,  8.7949e-01]],\n       grad_fn=&lt;AddBackward0&gt;)\n\n\n\nlist(gconv.parameters())\n\n[Parameter containing:\n tensor([0., 0., 0., 0.], requires_grad=True),\n Parameter containing:\n tensor([[-0.6724],\n         [ 0.7172],\n         [-0.3185],\n         [ 0.5363]], requires_grad=True)]\n\n\n\n_,W = list(gconv.parameters())\nW\n\nParameter containing:\ntensor([[-0.6724],\n        [ 0.7172],\n        [-0.3185],\n        [ 0.5363]], requires_grad=True)\n\n\n- pyg lesson5\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\n\nclass GCN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(9009, 16)\n        self.conv2 = GCNConv(16,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\n\nmodel = GCN()\n\n\nmodel\n\nGCN(\n  (conv1): GCNConv(9009, 16)\n  (conv2): GCNConv(16, 2)\n)\n\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\n\nGCN(\n  (conv1): GCNConv(9009, 16)\n  (conv2): GCNConv(16, 2)\n)\n\n\n\nfor epoch in range(200):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(data.x, data.y)\n    loss.backward()\n    optimizer.step()\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (9009x1 and 9009x16)\n\n\n\ndf50_test\n\n\ndf50_test \n\nNameError: name 'df50_test' is not defined\n\n\n\ndata\n\nData(x=[9009, 1], edge_index=[2, 28472], y=[9009])\n\n\nx=[9009, 1] : 9009개의 거래량이 있다. 특징은 걍.. 거래량 하나? &lt;- 거래자체에 대한 특징을 더 추가해도 좋을듯.. 흠.\nedge_index=[2, 28472]: 거래끼리의 edge값은 28472.. 일단 고객id가 같지 않으면 엣지값0으로 하고..\n\ndata.train_mask\n\nAttributeError: 'GlobalStorage' object has no attribute 'train_mask'"
  },
  {
    "objectID": "posts/230810 데이터정리.html",
    "href": "posts/230810 데이터정리.html",
    "title": "[FRAUD] 데이터정리",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\nimport torch\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics \n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부     \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\ndef build_graph_tripartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n                                                       df[\"cc_num\"].values.tolist() +\n                                                       df[\"merchant\"].values.tolist()))}\n    df[\"in_node\"]= df[\"cc_num\"].apply(lambda x: mapping[x])\n    df[\"out_node\"]=df[\"merchant\"].apply(lambda x:mapping[x])\n    \n        \n    G=nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\\\n                        [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")     \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")   \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")  \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n\n    return G\n    \n    \ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef embedding(Graph):\n    # Graph -&gt; X (feature)\n    _edgs = list(Graph.edges)\n    subGraph = Graph.edge_subgraph([_edgs[x] for x in range(len(Graph.edges))]).copy()\n    subGraph.add_nodes_from(list(set(Graph.nodes) - set(subGraph.nodes)))    \n    embedded = AverageEmbedder(Node2Vec(subGraph, weight_key='weight').fit(window=10).wv)\n    X = [embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in range(len(Graph.edges))]\n    # Graph -&gt; y (label)\n    y = np.array(list(nx.get_edge_attributes(Graph, \"label\").values()))\n    return X,y \n\ndef anal(df):\n    Graph = build_graph_bipartite(df)\n    X,XX,y,yy = embedding(Graph)\n    lrnr = RandomForestClassifier(n_estimators=100, random_state=42) \n    lrnr.fit(X,y)\n    yyhat = lrnr.predict(XX)\n    df = pd.DataFrame({\n        'acc':[sklearn.metrics.accuracy_score(yy,yyhat)], \n        'pre':[sklearn.metrics.precision_score(yy,yyhat)], \n        'rec':[sklearn.metrics.recall_score(yy,yyhat)],\n        'f1':[sklearn.metrics.f1_score(yy,yyhat)]}\n    )    \n    return df\n\ndef our_sampling1(df):\n    cus_list = set(df.query('is_fraud==1').cc_num.tolist())\n    return df.query(\"cc_num in @ cus_list\")\n\n- 모든엣지를 고려하는 방법\n\nN = 10 \nedge_index = torch.tensor([[i,j] for i in range(N) for j in range(N)]).T\n# edge_attr = 그래프의 웨이트 \n\n\nedge_index\n\ntensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2,\n         2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4,\n         4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7,\n         7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9,\n         9, 9, 9, 9],\n        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3,\n         4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7,\n         8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1,\n         2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5,\n         6, 7, 8, 9]])\n\n\n\nfraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n- 시간 차이 계산하려면?\n\ndiff = fraudTrain.trans_date_trans_time[101]-fraudTrain.trans_date_trans_time[0]\n\n\ndiff\n\nTimedelta('0 days 01:17:00')\n\n\n\ndiff.total_seconds()\n\n4620.0\n\n\n- 적당한 theta값을 정하자.\n\ntheta = 86400*1.2\ntheta\n\n103680.0\n\n\n\ntheta = 86400*1.2\nnp.exp(-diff.total_seconds()/theta)\n\n0.9564180361647693\n\n\n\n\n\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\nN = len(fraudTrain)\nN\n\n1048575\n\n\n\ndf02을 이용해서 해보자.\n\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\n214520*214520\n\n46018830400\n\n\n\n# N = len(df02)\n# edge_index = torch.tensor([[i,j] for i in range(N) for j in range(N)]).T\n\n\ndf50\n\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\n12012*12012\n\n144288144\n\n\n\n\n\ndf50 의 shape이 12000개 이므로 9000개의 T, 3000개의 F를 train mask로 만들자.\n고객정보가 동일하면 edge를 1로, 아니면 0으로 놓고 1에대한 weight를 만들자.\ng(V,E,W)에서의 weight\n\n\ndf50 = df50.reset_index()\n\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.is_fraud.mean().round(5), df50_test.is_fraud.mean().round(5)\n\n(0.49828, 0.50516)\n\n\n\n\n\n\n현재 df50의 fraud 비율은 5:5 인데, 다른 비율을 가진 데이터로도 해보자\nGNN으로 돌려본 것과 다른 방법들과 비교를 해보자\nundersampling한 다른 데이터들과 비교해 볼 수 있을 듯(boost, logis, …)\n9000/3000 데이터를 통해 합성 데이터를 만드는데, 12000개를 그대로 만드는 방법, 고객별로(cc_num) 합성 데이터를 만드는 방법, 똑같은 cc_num로 특이한 데이터가 있다면 normal데이터와 특이 데이터를 생각해서 돌리는 방법 등을 고려하자.\n\n\ndf50_tr.shape, df50_test.shape\n\n((9009, 23), (3003, 23))\n\n\n\nN = len(df50_tr)\n#edge_index = torch.tensor([[i,j] for i in range(N) for j in range(N)]).T\n#edge_index\n\n\ndf50_tr = df50_tr.reset_index()\n\n\nedge_index_list = []\nfor i in range(N):\n    for j in range(N):\n        time_difference = (df50_tr['trans_date_trans_time'][i] - df50_tr['trans_date_trans_time'][j]).total_seconds()\n        edge_index_list.append([i, j, time_difference])\n\n\nedge_index = np.array(edge_index_list)\n\n\nedge_index.shape\n\n(81162081, 3)\n\n\n\nedge_index[:,2] = np.abs(edge_index[:,2])\n\n\ntheta = edge_index[:,2].mean()\ntheta\n\n12230796.273867842\n\n\n\nedge_index[:,2] = (np.exp(-edge_index[:,2]/theta)!=1) * np.exp(-edge_index[:,2]/theta)\n\n\nedge_index\n\narray([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 1.00000000e+00, 1.90157975e-01],\n       [0.00000000e+00, 2.00000000e+00, 9.79646259e-02],\n       ...,\n       [9.00800000e+03, 9.00600000e+03, 6.60662164e-01],\n       [9.00800000e+03, 9.00700000e+03, 1.49150646e-01],\n       [9.00800000e+03, 9.00800000e+03, 0.00000000e+00]])\n\n\n\neee = edge_index[:,:]\n\n\neee[:,1]\n\narray([0.000e+00, 1.000e+00, 2.000e+00, ..., 9.006e+03, 9.007e+03,\n       9.008e+03])\n\n\n\nedge_index_list_updated = edge_index.tolist()\n\n\nedge_index_list_updated[:5]\n\n[[0.0, 0.0, 0.0],\n [0.0, 1.0, 0.19015797528259762],\n [0.0, 2.0, 0.09796462590589798],\n [0.0, 3.0, 0.1424157407389685],\n [0.0, 4.0, 0.11107338192969567]]\n\n\n\ndf50_tr\n\n\n\n\n\n\n\n\nlevel_0\nindex\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n476\n51331\n2019-01-31 00:44:00\n3.543590e+15\nfraud_Medhurst PLC\nshopping_net\n921.24\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nc8928ba53be26fdd997b26f7130c757e\n1327970678\n40.064488\n-78.210499\n1\n\n\n1\n3671\n625691\n2019-09-23 00:09:00\n2.610530e+15\nfraud_Torphy-Goyette\nshopping_pos\n698.28\nTanya\nDickerson\nF\n...\n36.2416\n-86.6117\n22191\nPrison officer\n1994-07-27\n90453290b765904ed1c3426882a6788b\n1348358993\n35.884288\n-87.513318\n1\n\n\n2\n6641\n896244\n2019-12-25 21:30:00\n6.011330e+15\nfraud_Monahan-Morar\npersonal_care\n220.56\nLauren\nButler\nF\n...\n36.0557\n-96.0602\n413574\nTeacher, special educational needs\n1971-09-01\n4072a3effcf51cf7cf88f69d00642cd9\n1356471044\n35.789798\n-95.859736\n0\n\n\n3\n4288\n717690\n2019-11-02 22:22:00\n6.011380e+15\nfraud_Daugherty, Pouros and Beahan\nshopping_pos\n905.43\nMartin\nDuarte\nM\n...\n44.6001\n-84.2931\n864\nGeneral practice doctor\n1942-05-04\nf2fa1b25eef2f43fa5c09e3e1bfe7f77\n1351894926\n44.652759\n-84.500359\n1\n\n\n4\n4770\n815813\n2019-12-08 02:50:00\n4.430880e+15\nfraud_Hudson-Ratke\ngrocery_pos\n307.98\nAlicia\nMorales\nF\n...\n39.3199\n-106.6596\n61\nPublic relations account executive\n1939-11-04\nf06eff8da349e36e623cff026de8e970\n1354935056\n38.389399\n-106.111026\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9004\n11964\n177703\n2019-04-02 21:48:00\n3.572980e+15\nfraud_Ziemann-Waters\nhealth_fitness\n63.89\nWilliam\nLopez\nM\n...\n41.1832\n-96.9882\n614\nAssociate Professor\n1967-06-20\n5b19aad28d65a6b0a912fa7b9d1896de\n1333403300\n42.067169\n-96.876892\n0\n\n\n9005\n5191\n921796\n2019-12-30 23:29:00\n6.762920e+11\nfraud_Wiza, Schaden and Stark\nmisc_pos\n51.41\nLisa\nFitzpatrick\nF\n...\n41.2336\n-75.2389\n104\nFinancial trader\n1927-08-25\nb2a9e44026fc57e54b4e45ade6017668\n1356910178\n40.502189\n-74.814956\n1\n\n\n9006\n5390\n950365\n2020-01-16 03:15:00\n4.807550e+12\nfraud_Murray-Smitham\ngrocery_pos\n357.62\nKimberly\nCastro\nF\n...\n40.2158\n-83.9579\n133\nProfessor Emeritus\n1954-01-29\n4bfa37c329f327074e7220ea6e5d8f8d\n1358306148\n40.620284\n-84.274495\n1\n\n\n9007\n860\n88685\n2019-02-22 02:19:00\n5.738600e+11\nfraud_McDermott-Weimann\ngrocery_pos\n304.75\nCristian\nJones\nM\n...\n42.0765\n-87.7246\n27020\nTrade mark attorney\n1986-07-23\na1c3025ddb615ab2ef890bf82fc3d66a\n1329877195\n42.722479\n-88.362364\n1\n\n\n9008\n7270\n753787\n2019-11-18 10:58:00\n6.042293e+10\nfraud_Terry, Johns and Bins\nmisc_pos\n1.64\nJeffrey\nPowers\nM\n...\n33.6028\n-81.9748\n46944\nSecondary school teacher\n1942-04-02\nee10d61782bde2b5cabc2ad649e977cc\n1353236287\n34.243599\n-82.971344\n0\n\n\n\n\n9009 rows × 24 columns\n\n\n\n\ncc_num로 그룹별로 묶자.\n\n\ndf50_tr[df50_tr['cc_num']==3.543590e+15]\n\n\n\n\n\n\n\n\nlevel_0\nindex\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n476\n51331\n2019-01-31 00:44:00\n3.543590e+15\nfraud_Medhurst PLC\nshopping_net\n921.24\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nc8928ba53be26fdd997b26f7130c757e\n1327970678\n40.064488\n-78.210499\n1\n\n\n344\n462\n50905\n2019-01-30 16:53:00\n3.543590e+15\nfraud_Lesch Ltd\nshopping_pos\n881.11\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n9f7b7675c4decefd03cce56df045ed1c\n1327942400\n39.591484\n-79.575246\n1\n\n\n1377\n6607\n814736\n2019-12-07 22:17:00\n3.543590e+15\nfraud_Botsford and Sons\nhome\n10.41\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\naa9b533e84970309a4ad60a914a8cd77\n1354918668\n41.287791\n-79.980592\n0\n\n\n1447\n485\n51816\n2019-01-31 12:38:00\n3.543590e+15\nfraud_Ruecker-Mayert\nkids_pets\n21.93\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\ncec656f154e0978b0f26702c29ddeeca\n1328013517\n39.946187\n-78.078864\n1\n\n\n1639\n11176\n12947\n2019-01-08 11:08:00\n3.543590e+15\nfraud_Stroman, Hudson and Erdman\ngas_transport\n76.03\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nc10451edc4e21b865d049312acf18ecd\n1326020892\n39.503960\n-78.471680\n0\n\n\n2046\n8124\n627045\n2019-09-23 12:53:00\n3.543590e+15\nfraud_Botsford Ltd\nshopping_pos\n3.20\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n003d591d208f7ee52277b5cc4fa4a37f\n1348404838\n40.066686\n-79.326630\n0\n\n\n2093\n477\n51367\n2019-01-31 01:36:00\n3.543590e+15\nfraud_Watsica, Haag and Considine\nshopping_pos\n1090.67\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nb42bc0820a78de54845c5138b9c39dd5\n1327973774\n40.923284\n-78.882504\n1\n\n\n2415\n491\n52402\n2019-01-31 22:17:00\n3.543590e+15\nfraud_Metz, Russel and Metz\nkids_pets\n22.35\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n51f9352216e99bbe9e8b03b082305971\n1328048275\n39.979547\n-78.851379\n1\n\n\n2625\n463\n51047\n2019-01-30 19:35:00\n3.543590e+15\nfraud_Ruecker-Mayert\nkids_pets\n22.95\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n8e804422b761537e3a49a237afd1ea9a\n1327952100\n40.051981\n-79.021769\n1\n\n\n2769\n478\n51374\n2019-01-31 01:42:00\n3.543590e+15\nfraud_Schmidt and Sons\nshopping_net\n1043.59\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nbbe4e9e431cba66e6531199ffaf79657\n1327974178\n40.192896\n-79.366393\n1\n\n\n3192\n505\n52522\n2019-01-31 23:57:00\n3.543590e+15\nfraud_Kutch, Steuber and Gerhold\nfood_dining\n116.45\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nfcf46ca0264437bbb938c29eca2c92ad\n1328054256\n40.288401\n-78.286914\n1\n\n\n3670\n11714\n1010269\n2020-02-20 06:02:00\n3.543590e+15\nfraud_Huels-Hahn\ngas_transport\n51.80\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nb72c0124f4c5662db13e1bea2f04784b\n1361340164\n39.672719\n-79.642589\n0\n\n\n3945\n6087\n243892\n2019-05-02 13:38:00\n3.543590e+15\nfraud_Cruickshank-Mills\nentertainment\n5.72\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n990da059d387e5fa7481d76ff5c29199\n1335965925\n40.577553\n-79.315460\n0\n\n\n5017\n484\n51431\n2019-01-31 03:28:00\n3.543590e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n741.98\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n41312d7d5fc76be3782b5e9cef04726f\n1327980509\n41.290570\n-79.682069\n1\n\n\n5505\n8148\n181398\n2019-04-04 23:32:00\n3.543590e+15\nfraud_Feil, Hilpert and Koss\nfood_dining\n89.23\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\ne304fd4ebc897fce190925dadcd2b524\n1333582347\n39.736380\n-79.481667\n0\n\n\n5729\n11116\n329202\n2019-06-06 03:26:00\n3.543590e+15\nfraud_Connelly, Reichert and Fritsch\ngas_transport\n69.36\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nbc0832ac8bac6d26548ab6ab553d5d5e\n1338953171\n40.780469\n-79.668417\n0\n\n\n7605\n481\n51392\n2019-01-31 02:16:00\n3.543590e+15\nfraud_Huels-Hahn\ngas_transport\n12.41\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n5f4379c2fc20457f0f99a126cadda1af\n1327976216\n39.884234\n-79.374966\n1\n\n\n7800\n8609\n55920\n2019-02-03 06:51:00\n3.543590e+15\nfraud_Corwin-Gorczany\nmisc_net\n6.70\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n90f33381b6b6644c6d03c8cdb51d05dc\n1328251865\n40.064532\n-78.920283\n0\n\n\n8100\n10488\n509733\n2019-08-09 11:47:00\n3.543590e+15\nfraud_Kutch and Sons\ngrocery_pos\n108.74\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n40a620cc7c5ba396b1fe112f5361e4a9\n1344512838\n40.057443\n-78.569798\n0\n\n\n8313\n504\n52514\n2019-01-31 23:52:00\n3.543590e+15\nfraud_Douglas, Schneider and Turner\nshopping_pos\n1129.56\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nec208107f178422e0953560343d0cf8b\n1328053975\n40.840340\n-78.027854\n1\n\n\n\n\n20 rows × 24 columns\n\n\n\n\ndf50_grouped=df50_tr.groupby(by='cc_num')\n\n\nedge_index_list = []\nfor i in range(N):\n    for j in range(N):\n        if df50_tr['cc_num'][i] != df50_tr['cc_num'][j]:  # cc_num 값이 같다면\n            time_difference = 0\n        else:\n            time_difference = (df50_tr['trans_date_trans_time'][i] - df50_tr['trans_date_trans_time'][j]).total_seconds()\n        edge_index_list.append([i, j, time_difference])\n\n\nedge_index = np.array(edge_index_list)\n\n\nedge_index.shape\n\n(81162081, 3)\n\n\n\nedge_index\n\narray([[0.000e+00, 0.000e+00, 0.000e+00],\n       [0.000e+00, 1.000e+00, 0.000e+00],\n       [0.000e+00, 2.000e+00, 0.000e+00],\n       ...,\n       [9.008e+03, 9.006e+03, 0.000e+00],\n       [9.008e+03, 9.007e+03, 0.000e+00],\n       [9.008e+03, 9.008e+03, 0.000e+00]])\n\n\n\nedge_index[:,2] = np.abs(edge_index[:,2])\n\n\ntheta = edge_index[:,2].mean()\ntheta\n\n10988.585252761077\n\n\n\nedge_index\n\narray([[0.000e+00, 0.000e+00, 0.000e+00],\n       [0.000e+00, 1.000e+00, 0.000e+00],\n       [0.000e+00, 2.000e+00, 0.000e+00],\n       ...,\n       [9.008e+03, 9.006e+03, 0.000e+00],\n       [9.008e+03, 9.007e+03, 0.000e+00],\n       [9.008e+03, 9.008e+03, 0.000e+00]])\n\n\n\nedge_index[:,2] = (np.exp(-edge_index[:,2]/theta)!=1) * np.exp(-edge_index[:,2]/theta)\n\n\nedge_index\n\narray([[0.000e+00, 0.000e+00, 0.000e+00],\n       [0.000e+00, 1.000e+00, 0.000e+00],\n       [0.000e+00, 2.000e+00, 0.000e+00],\n       ...,\n       [9.008e+03, 9.006e+03, 0.000e+00],\n       [9.008e+03, 9.007e+03, 0.000e+00],\n       [9.008e+03, 9.008e+03, 0.000e+00]])\n\n\n\nedge_index_list_updated = edge_index.tolist()\n\n\nnp.array(edge_index_list_updated)[:,2].mean()\n\n8.344409093328692e-05\n\n\n\nmm = np.array(edge_index_list_updated)[:,2].mean()\n\nedge_index_list_updated가 w\n\nselected_edges = [(int(row[0]), int(row[1])) for row in edge_index_list_updated if row[2] &gt; mm]\n\n\nedge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\n\n\nedge_index_selected.shape\n\ntorch.Size([2, 28472])\n\n\n\nedge_index\n\narray([[0.000e+00, 0.000e+00, 0.000e+00],\n       [0.000e+00, 1.000e+00, 0.000e+00],\n       [0.000e+00, 2.000e+00, 0.000e+00],\n       ...,\n       [9.008e+03, 9.006e+03, 0.000e+00],\n       [9.008e+03, 9.007e+03, 0.000e+00],\n       [9.008e+03, 9.008e+03, 0.000e+00]])\n\n\n\n\n\n\nedge_index 돌아가는 게 너무 오래걸려서 이렇게 저장해놓으면 빠르게 실행할 수 있다.\n\n\n#import numpy as np\n\n#data = np.array([1, 2, 3, 4, 5])\nnp.save('edge_index.npy', edge_index)\n\nloaded_data = np.load('edge_index.npy')\n\n\nnpy로 끝나는 건 위에처럼 저장하기 아님 피클로!ㅡ, torch방법\n\n\nx = df50_tr['amt']\n\n\nx\n\n0       921.24\n1       698.28\n2       220.56\n3       905.43\n4       307.98\n         ...  \n9004     63.89\n9005     51.41\n9006    357.62\n9007    304.75\n9008      1.64\nName: amt, Length: 9009, dtype: float64\n\n\n\na = torch.tensor(x, dtype=torch.float)\n\n\na = a.reshape(-1,1)\na\n\ntensor([[921.2400],\n        [698.2800],\n        [220.5600],\n        ...,\n        [357.6200],\n        [304.7500],\n        [  1.6400]])\n\n\n\ny = df50_tr['is_fraud']\n\n\nb = torch.tensor(y,dtype=torch.int64)\n\n\nb\n\ntensor([1, 1, 0,  ..., 1, 1, 0])\n\n\n\nimport torch_geometric\n\n\ndata = torch_geometric.data.Data(x=a, edge_index = edge_index_selected, y=b)\n\n\ndata\n\nData(x=[9009, 1], edge_index=[2, 28472], y=[9009])\n\n\n- pyg lesson6\n\ngconv = torch_geometric.nn.GCNConv(1,4)\ngconv\n\nGCNConv(1, 4)\n\n\n\ngconv(data.x, data.edge_index)\n\ntensor([[ 4.0225e+02,  2.5312e+02, -2.9747e+02, -1.6831e+02],\n        [ 3.7246e+02,  2.3437e+02, -2.7543e+02, -1.5584e+02],\n        [ 1.5695e+02,  9.8760e+01, -1.1606e+02, -6.5670e+01],\n        ...,\n        [ 2.5448e+02,  1.6013e+02, -1.8818e+02, -1.0648e+02],\n        [ 5.4738e+02,  3.4444e+02, -4.0478e+02, -2.2903e+02],\n        [ 1.1670e+00,  7.3434e-01, -8.6299e-01, -4.8830e-01]],\n       grad_fn=&lt;AddBackward0&gt;)\n\n\n\nlist(gconv.parameters())\n\n[Parameter containing:\n tensor([0., 0., 0., 0.], requires_grad=True),\n Parameter containing:\n tensor([[ 0.7116],\n         [ 0.4478],\n         [-0.5262],\n         [-0.2977]], requires_grad=True)]\n\n\n\n_,W = list(gconv.parameters())\nW\n\nParameter containing:\ntensor([[-0.6724],\n        [ 0.7172],\n        [-0.3185],\n        [ 0.5363]], requires_grad=True)\n\n\n- pyg lesson5\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\n\nclass GCN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 16)\n        self.conv2 = GCNConv(16,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\n\nmodel = GCN()\n\n\nmodel\n\nGCN(\n  (conv1): GCNConv(1, 16)\n  (conv2): GCNConv(16, 2)\n)\n\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\n\nGCN(\n  (conv1): GCNConv(1, 16)\n  (conv2): GCNConv(16, 2)\n)\n\n\n\nout\n\ntensor([[-1.8963e+02,  0.0000e+00],\n        [-1.5192e+02,  0.0000e+00],\n        [-5.3630e+01,  0.0000e+00],\n        ...,\n        [-3.0590e+02,  0.0000e+00],\n        [-3.0298e+02,  0.0000e+00],\n        [-1.3924e+00, -2.8567e-01]], grad_fn=&lt;LogSoftmaxBackward0&gt;)\n\n\n\ndata.y\n\ntensor([1, 1, 0,  ..., 1, 1, 0])\n\n\n\nfor epoch in range(200):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out, data.y)\n    loss.backward()\n    optimizer.step()\n\n\nmodel.eval()\npred = model(data).argmax(dim=1)\ncorrect = (pred == data.y).sum() # 애큐러시는 test\nacc = int(correct) / 9009\nprint(f'Accuracy: {acc:.4f}')\n\nAccuracy: 0.9633\n\n\n\nfraud_mask = (data.y == 1)\n\n\nmodel.eval()\npred = model(data).argmax(dim=1)\ncorrect = (pred[fraud_mask] == data.y[fraud_mask]).sum() # 애큐러시는 test\nacc = int(correct) / int(fraud_mask.sum())\nprint(f'recall: {acc:.4f}')\n\nrecall: 0.9619\n\n\n\n위의 recall은 test가 없어서 train으로만 했던 거..!"
  },
  {
    "objectID": "posts/230810 데이터정리.html#시도",
    "href": "posts/230810 데이터정리.html#시도",
    "title": "[FRAUD] 데이터정리",
    "section": "",
    "text": "fraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\nN = len(fraudTrain)\nN\n\n1048575\n\n\n\ndf02을 이용해서 해보자.\n\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\n214520*214520\n\n46018830400\n\n\n\n# N = len(df02)\n# edge_index = torch.tensor([[i,j] for i in range(N) for j in range(N)]).T\n\n\ndf50\n\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\n12012*12012\n\n144288144\n\n\n\n\n\ndf50 의 shape이 12000개 이므로 9000개의 T, 3000개의 F를 train mask로 만들자.\n고객정보가 동일하면 edge를 1로, 아니면 0으로 놓고 1에대한 weight를 만들자.\ng(V,E,W)에서의 weight\n\n\ndf50 = df50.reset_index()\n\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.is_fraud.mean().round(5), df50_test.is_fraud.mean().round(5)\n\n(0.49828, 0.50516)\n\n\n\n\n\n\n현재 df50의 fraud 비율은 5:5 인데, 다른 비율을 가진 데이터로도 해보자\nGNN으로 돌려본 것과 다른 방법들과 비교를 해보자\nundersampling한 다른 데이터들과 비교해 볼 수 있을 듯(boost, logis, …)\n9000/3000 데이터를 통해 합성 데이터를 만드는데, 12000개를 그대로 만드는 방법, 고객별로(cc_num) 합성 데이터를 만드는 방법, 똑같은 cc_num로 특이한 데이터가 있다면 normal데이터와 특이 데이터를 생각해서 돌리는 방법 등을 고려하자.\n\n\ndf50_tr.shape, df50_test.shape\n\n((9009, 23), (3003, 23))\n\n\n\nN = len(df50_tr)\n#edge_index = torch.tensor([[i,j] for i in range(N) for j in range(N)]).T\n#edge_index\n\n\ndf50_tr = df50_tr.reset_index()\n\n\nedge_index_list = []\nfor i in range(N):\n    for j in range(N):\n        time_difference = (df50_tr['trans_date_trans_time'][i] - df50_tr['trans_date_trans_time'][j]).total_seconds()\n        edge_index_list.append([i, j, time_difference])\n\n\nedge_index = np.array(edge_index_list)\n\n\nedge_index.shape\n\n(81162081, 3)\n\n\n\nedge_index[:,2] = np.abs(edge_index[:,2])\n\n\ntheta = edge_index[:,2].mean()\ntheta\n\n12230796.273867842\n\n\n\nedge_index[:,2] = (np.exp(-edge_index[:,2]/theta)!=1) * np.exp(-edge_index[:,2]/theta)\n\n\nedge_index\n\narray([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 1.00000000e+00, 1.90157975e-01],\n       [0.00000000e+00, 2.00000000e+00, 9.79646259e-02],\n       ...,\n       [9.00800000e+03, 9.00600000e+03, 6.60662164e-01],\n       [9.00800000e+03, 9.00700000e+03, 1.49150646e-01],\n       [9.00800000e+03, 9.00800000e+03, 0.00000000e+00]])\n\n\n\neee = edge_index[:,:]\n\n\neee[:,1]\n\narray([0.000e+00, 1.000e+00, 2.000e+00, ..., 9.006e+03, 9.007e+03,\n       9.008e+03])\n\n\n\nedge_index_list_updated = edge_index.tolist()\n\n\nedge_index_list_updated[:5]\n\n[[0.0, 0.0, 0.0],\n [0.0, 1.0, 0.19015797528259762],\n [0.0, 2.0, 0.09796462590589798],\n [0.0, 3.0, 0.1424157407389685],\n [0.0, 4.0, 0.11107338192969567]]\n\n\n\ndf50_tr\n\n\n\n\n\n\n\n\nlevel_0\nindex\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n476\n51331\n2019-01-31 00:44:00\n3.543590e+15\nfraud_Medhurst PLC\nshopping_net\n921.24\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nc8928ba53be26fdd997b26f7130c757e\n1327970678\n40.064488\n-78.210499\n1\n\n\n1\n3671\n625691\n2019-09-23 00:09:00\n2.610530e+15\nfraud_Torphy-Goyette\nshopping_pos\n698.28\nTanya\nDickerson\nF\n...\n36.2416\n-86.6117\n22191\nPrison officer\n1994-07-27\n90453290b765904ed1c3426882a6788b\n1348358993\n35.884288\n-87.513318\n1\n\n\n2\n6641\n896244\n2019-12-25 21:30:00\n6.011330e+15\nfraud_Monahan-Morar\npersonal_care\n220.56\nLauren\nButler\nF\n...\n36.0557\n-96.0602\n413574\nTeacher, special educational needs\n1971-09-01\n4072a3effcf51cf7cf88f69d00642cd9\n1356471044\n35.789798\n-95.859736\n0\n\n\n3\n4288\n717690\n2019-11-02 22:22:00\n6.011380e+15\nfraud_Daugherty, Pouros and Beahan\nshopping_pos\n905.43\nMartin\nDuarte\nM\n...\n44.6001\n-84.2931\n864\nGeneral practice doctor\n1942-05-04\nf2fa1b25eef2f43fa5c09e3e1bfe7f77\n1351894926\n44.652759\n-84.500359\n1\n\n\n4\n4770\n815813\n2019-12-08 02:50:00\n4.430880e+15\nfraud_Hudson-Ratke\ngrocery_pos\n307.98\nAlicia\nMorales\nF\n...\n39.3199\n-106.6596\n61\nPublic relations account executive\n1939-11-04\nf06eff8da349e36e623cff026de8e970\n1354935056\n38.389399\n-106.111026\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9004\n11964\n177703\n2019-04-02 21:48:00\n3.572980e+15\nfraud_Ziemann-Waters\nhealth_fitness\n63.89\nWilliam\nLopez\nM\n...\n41.1832\n-96.9882\n614\nAssociate Professor\n1967-06-20\n5b19aad28d65a6b0a912fa7b9d1896de\n1333403300\n42.067169\n-96.876892\n0\n\n\n9005\n5191\n921796\n2019-12-30 23:29:00\n6.762920e+11\nfraud_Wiza, Schaden and Stark\nmisc_pos\n51.41\nLisa\nFitzpatrick\nF\n...\n41.2336\n-75.2389\n104\nFinancial trader\n1927-08-25\nb2a9e44026fc57e54b4e45ade6017668\n1356910178\n40.502189\n-74.814956\n1\n\n\n9006\n5390\n950365\n2020-01-16 03:15:00\n4.807550e+12\nfraud_Murray-Smitham\ngrocery_pos\n357.62\nKimberly\nCastro\nF\n...\n40.2158\n-83.9579\n133\nProfessor Emeritus\n1954-01-29\n4bfa37c329f327074e7220ea6e5d8f8d\n1358306148\n40.620284\n-84.274495\n1\n\n\n9007\n860\n88685\n2019-02-22 02:19:00\n5.738600e+11\nfraud_McDermott-Weimann\ngrocery_pos\n304.75\nCristian\nJones\nM\n...\n42.0765\n-87.7246\n27020\nTrade mark attorney\n1986-07-23\na1c3025ddb615ab2ef890bf82fc3d66a\n1329877195\n42.722479\n-88.362364\n1\n\n\n9008\n7270\n753787\n2019-11-18 10:58:00\n6.042293e+10\nfraud_Terry, Johns and Bins\nmisc_pos\n1.64\nJeffrey\nPowers\nM\n...\n33.6028\n-81.9748\n46944\nSecondary school teacher\n1942-04-02\nee10d61782bde2b5cabc2ad649e977cc\n1353236287\n34.243599\n-82.971344\n0\n\n\n\n\n9009 rows × 24 columns\n\n\n\n\ncc_num로 그룹별로 묶자.\n\n\ndf50_tr[df50_tr['cc_num']==3.543590e+15]\n\n\n\n\n\n\n\n\nlevel_0\nindex\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n476\n51331\n2019-01-31 00:44:00\n3.543590e+15\nfraud_Medhurst PLC\nshopping_net\n921.24\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nc8928ba53be26fdd997b26f7130c757e\n1327970678\n40.064488\n-78.210499\n1\n\n\n344\n462\n50905\n2019-01-30 16:53:00\n3.543590e+15\nfraud_Lesch Ltd\nshopping_pos\n881.11\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n9f7b7675c4decefd03cce56df045ed1c\n1327942400\n39.591484\n-79.575246\n1\n\n\n1377\n6607\n814736\n2019-12-07 22:17:00\n3.543590e+15\nfraud_Botsford and Sons\nhome\n10.41\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\naa9b533e84970309a4ad60a914a8cd77\n1354918668\n41.287791\n-79.980592\n0\n\n\n1447\n485\n51816\n2019-01-31 12:38:00\n3.543590e+15\nfraud_Ruecker-Mayert\nkids_pets\n21.93\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\ncec656f154e0978b0f26702c29ddeeca\n1328013517\n39.946187\n-78.078864\n1\n\n\n1639\n11176\n12947\n2019-01-08 11:08:00\n3.543590e+15\nfraud_Stroman, Hudson and Erdman\ngas_transport\n76.03\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nc10451edc4e21b865d049312acf18ecd\n1326020892\n39.503960\n-78.471680\n0\n\n\n2046\n8124\n627045\n2019-09-23 12:53:00\n3.543590e+15\nfraud_Botsford Ltd\nshopping_pos\n3.20\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n003d591d208f7ee52277b5cc4fa4a37f\n1348404838\n40.066686\n-79.326630\n0\n\n\n2093\n477\n51367\n2019-01-31 01:36:00\n3.543590e+15\nfraud_Watsica, Haag and Considine\nshopping_pos\n1090.67\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nb42bc0820a78de54845c5138b9c39dd5\n1327973774\n40.923284\n-78.882504\n1\n\n\n2415\n491\n52402\n2019-01-31 22:17:00\n3.543590e+15\nfraud_Metz, Russel and Metz\nkids_pets\n22.35\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n51f9352216e99bbe9e8b03b082305971\n1328048275\n39.979547\n-78.851379\n1\n\n\n2625\n463\n51047\n2019-01-30 19:35:00\n3.543590e+15\nfraud_Ruecker-Mayert\nkids_pets\n22.95\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n8e804422b761537e3a49a237afd1ea9a\n1327952100\n40.051981\n-79.021769\n1\n\n\n2769\n478\n51374\n2019-01-31 01:42:00\n3.543590e+15\nfraud_Schmidt and Sons\nshopping_net\n1043.59\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nbbe4e9e431cba66e6531199ffaf79657\n1327974178\n40.192896\n-79.366393\n1\n\n\n3192\n505\n52522\n2019-01-31 23:57:00\n3.543590e+15\nfraud_Kutch, Steuber and Gerhold\nfood_dining\n116.45\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nfcf46ca0264437bbb938c29eca2c92ad\n1328054256\n40.288401\n-78.286914\n1\n\n\n3670\n11714\n1010269\n2020-02-20 06:02:00\n3.543590e+15\nfraud_Huels-Hahn\ngas_transport\n51.80\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nb72c0124f4c5662db13e1bea2f04784b\n1361340164\n39.672719\n-79.642589\n0\n\n\n3945\n6087\n243892\n2019-05-02 13:38:00\n3.543590e+15\nfraud_Cruickshank-Mills\nentertainment\n5.72\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n990da059d387e5fa7481d76ff5c29199\n1335965925\n40.577553\n-79.315460\n0\n\n\n5017\n484\n51431\n2019-01-31 03:28:00\n3.543590e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n741.98\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n41312d7d5fc76be3782b5e9cef04726f\n1327980509\n41.290570\n-79.682069\n1\n\n\n5505\n8148\n181398\n2019-04-04 23:32:00\n3.543590e+15\nfraud_Feil, Hilpert and Koss\nfood_dining\n89.23\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\ne304fd4ebc897fce190925dadcd2b524\n1333582347\n39.736380\n-79.481667\n0\n\n\n5729\n11116\n329202\n2019-06-06 03:26:00\n3.543590e+15\nfraud_Connelly, Reichert and Fritsch\ngas_transport\n69.36\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nbc0832ac8bac6d26548ab6ab553d5d5e\n1338953171\n40.780469\n-79.668417\n0\n\n\n7605\n481\n51392\n2019-01-31 02:16:00\n3.543590e+15\nfraud_Huels-Hahn\ngas_transport\n12.41\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n5f4379c2fc20457f0f99a126cadda1af\n1327976216\n39.884234\n-79.374966\n1\n\n\n7800\n8609\n55920\n2019-02-03 06:51:00\n3.543590e+15\nfraud_Corwin-Gorczany\nmisc_net\n6.70\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n90f33381b6b6644c6d03c8cdb51d05dc\n1328251865\n40.064532\n-78.920283\n0\n\n\n8100\n10488\n509733\n2019-08-09 11:47:00\n3.543590e+15\nfraud_Kutch and Sons\ngrocery_pos\n108.74\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n40a620cc7c5ba396b1fe112f5361e4a9\n1344512838\n40.057443\n-78.569798\n0\n\n\n8313\n504\n52514\n2019-01-31 23:52:00\n3.543590e+15\nfraud_Douglas, Schneider and Turner\nshopping_pos\n1129.56\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nec208107f178422e0953560343d0cf8b\n1328053975\n40.840340\n-78.027854\n1\n\n\n\n\n20 rows × 24 columns\n\n\n\n\ndf50_grouped=df50_tr.groupby(by='cc_num')\n\n\nedge_index_list = []\nfor i in range(N):\n    for j in range(N):\n        if df50_tr['cc_num'][i] != df50_tr['cc_num'][j]:  # cc_num 값이 같다면\n            time_difference = 0\n        else:\n            time_difference = (df50_tr['trans_date_trans_time'][i] - df50_tr['trans_date_trans_time'][j]).total_seconds()\n        edge_index_list.append([i, j, time_difference])\n\n\nedge_index = np.array(edge_index_list)\n\n\nedge_index.shape\n\n(81162081, 3)\n\n\n\nedge_index\n\narray([[0.000e+00, 0.000e+00, 0.000e+00],\n       [0.000e+00, 1.000e+00, 0.000e+00],\n       [0.000e+00, 2.000e+00, 0.000e+00],\n       ...,\n       [9.008e+03, 9.006e+03, 0.000e+00],\n       [9.008e+03, 9.007e+03, 0.000e+00],\n       [9.008e+03, 9.008e+03, 0.000e+00]])\n\n\n\nedge_index[:,2] = np.abs(edge_index[:,2])\n\n\ntheta = edge_index[:,2].mean()\ntheta\n\n10988.585252761077\n\n\n\nedge_index\n\narray([[0.000e+00, 0.000e+00, 0.000e+00],\n       [0.000e+00, 1.000e+00, 0.000e+00],\n       [0.000e+00, 2.000e+00, 0.000e+00],\n       ...,\n       [9.008e+03, 9.006e+03, 0.000e+00],\n       [9.008e+03, 9.007e+03, 0.000e+00],\n       [9.008e+03, 9.008e+03, 0.000e+00]])\n\n\n\nedge_index[:,2] = (np.exp(-edge_index[:,2]/theta)!=1) * np.exp(-edge_index[:,2]/theta)\n\n\nedge_index\n\narray([[0.000e+00, 0.000e+00, 0.000e+00],\n       [0.000e+00, 1.000e+00, 0.000e+00],\n       [0.000e+00, 2.000e+00, 0.000e+00],\n       ...,\n       [9.008e+03, 9.006e+03, 0.000e+00],\n       [9.008e+03, 9.007e+03, 0.000e+00],\n       [9.008e+03, 9.008e+03, 0.000e+00]])\n\n\n\nedge_index_list_updated = edge_index.tolist()\n\n\nnp.array(edge_index_list_updated)[:,2].mean()\n\n8.344409093328692e-05\n\n\n\nmm = np.array(edge_index_list_updated)[:,2].mean()\n\nedge_index_list_updated가 w\n\nselected_edges = [(int(row[0]), int(row[1])) for row in edge_index_list_updated if row[2] &gt; mm]\n\n\nedge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\n\n\nedge_index_selected.shape\n\ntorch.Size([2, 28472])\n\n\n\nedge_index\n\narray([[0.000e+00, 0.000e+00, 0.000e+00],\n       [0.000e+00, 1.000e+00, 0.000e+00],\n       [0.000e+00, 2.000e+00, 0.000e+00],\n       ...,\n       [9.008e+03, 9.006e+03, 0.000e+00],\n       [9.008e+03, 9.007e+03, 0.000e+00],\n       [9.008e+03, 9.008e+03, 0.000e+00]])\n\n\n\n\n\n\nedge_index 돌아가는 게 너무 오래걸려서 이렇게 저장해놓으면 빠르게 실행할 수 있다.\n\n\n#import numpy as np\n\n#data = np.array([1, 2, 3, 4, 5])\nnp.save('edge_index.npy', edge_index)\n\nloaded_data = np.load('edge_index.npy')\n\n\nnpy로 끝나는 건 위에처럼 저장하기 아님 피클로!ㅡ, torch방법\n\n\nx = df50_tr['amt']\n\n\nx\n\n0       921.24\n1       698.28\n2       220.56\n3       905.43\n4       307.98\n         ...  \n9004     63.89\n9005     51.41\n9006    357.62\n9007    304.75\n9008      1.64\nName: amt, Length: 9009, dtype: float64\n\n\n\na = torch.tensor(x, dtype=torch.float)\n\n\na = a.reshape(-1,1)\na\n\ntensor([[921.2400],\n        [698.2800],\n        [220.5600],\n        ...,\n        [357.6200],\n        [304.7500],\n        [  1.6400]])\n\n\n\ny = df50_tr['is_fraud']\n\n\nb = torch.tensor(y,dtype=torch.int64)\n\n\nb\n\ntensor([1, 1, 0,  ..., 1, 1, 0])\n\n\n\nimport torch_geometric\n\n\ndata = torch_geometric.data.Data(x=a, edge_index = edge_index_selected, y=b)\n\n\ndata\n\nData(x=[9009, 1], edge_index=[2, 28472], y=[9009])\n\n\n- pyg lesson6\n\ngconv = torch_geometric.nn.GCNConv(1,4)\ngconv\n\nGCNConv(1, 4)\n\n\n\ngconv(data.x, data.edge_index)\n\ntensor([[ 4.0225e+02,  2.5312e+02, -2.9747e+02, -1.6831e+02],\n        [ 3.7246e+02,  2.3437e+02, -2.7543e+02, -1.5584e+02],\n        [ 1.5695e+02,  9.8760e+01, -1.1606e+02, -6.5670e+01],\n        ...,\n        [ 2.5448e+02,  1.6013e+02, -1.8818e+02, -1.0648e+02],\n        [ 5.4738e+02,  3.4444e+02, -4.0478e+02, -2.2903e+02],\n        [ 1.1670e+00,  7.3434e-01, -8.6299e-01, -4.8830e-01]],\n       grad_fn=&lt;AddBackward0&gt;)\n\n\n\nlist(gconv.parameters())\n\n[Parameter containing:\n tensor([0., 0., 0., 0.], requires_grad=True),\n Parameter containing:\n tensor([[ 0.7116],\n         [ 0.4478],\n         [-0.5262],\n         [-0.2977]], requires_grad=True)]\n\n\n\n_,W = list(gconv.parameters())\nW\n\nParameter containing:\ntensor([[-0.6724],\n        [ 0.7172],\n        [-0.3185],\n        [ 0.5363]], requires_grad=True)\n\n\n- pyg lesson5\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\n\nclass GCN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 16)\n        self.conv2 = GCNConv(16,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\n\nmodel = GCN()\n\n\nmodel\n\nGCN(\n  (conv1): GCNConv(1, 16)\n  (conv2): GCNConv(16, 2)\n)\n\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\n\nGCN(\n  (conv1): GCNConv(1, 16)\n  (conv2): GCNConv(16, 2)\n)\n\n\n\nout\n\ntensor([[-1.8963e+02,  0.0000e+00],\n        [-1.5192e+02,  0.0000e+00],\n        [-5.3630e+01,  0.0000e+00],\n        ...,\n        [-3.0590e+02,  0.0000e+00],\n        [-3.0298e+02,  0.0000e+00],\n        [-1.3924e+00, -2.8567e-01]], grad_fn=&lt;LogSoftmaxBackward0&gt;)\n\n\n\ndata.y\n\ntensor([1, 1, 0,  ..., 1, 1, 0])\n\n\n\nfor epoch in range(200):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out, data.y)\n    loss.backward()\n    optimizer.step()\n\n\nmodel.eval()\npred = model(data).argmax(dim=1)\ncorrect = (pred == data.y).sum() # 애큐러시는 test\nacc = int(correct) / 9009\nprint(f'Accuracy: {acc:.4f}')\n\nAccuracy: 0.9633\n\n\n\nfraud_mask = (data.y == 1)\n\n\nmodel.eval()\npred = model(data).argmax(dim=1)\ncorrect = (pred[fraud_mask] == data.y[fraud_mask]).sum() # 애큐러시는 test\nacc = int(correct) / int(fraud_mask.sum())\nprint(f'recall: {acc:.4f}')\n\nrecall: 0.9619\n\n\n\n위의 recall은 test가 없어서 train으로만 했던 거..!"
  },
  {
    "objectID": "posts/230913 데이터(11, df50 정리)-Copy1.html",
    "href": "posts/230913 데이터(11, df50 정리)-Copy1.html",
    "title": "[FRAUD] 데이터정리 시도(df50다른것들 시드고정)",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\nimport xgboost as xgb\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# gnn\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric\nfrom torch_geometric.nn import GCNConv\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_geometric/typing.py:18: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/libpyg.so: undefined symbol: _ZN2at4_ops12split_Tensor4callERKNS_6TensorEN3c106SymIntEl\n  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_geometric/typing.py:31: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_scatter/_scatter_cuda.so: undefined symbol: _ZNK3c107SymBool10guard_boolEPKcl\n  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_geometric/typing.py:42: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_sparse/_diag_cuda.so: undefined symbol: _ZN3c106detail19maybe_wrap_dim_slowIlEET_S2_S2_b\n  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n\n\n\ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef compute_time_difference(group):\n    n = len(group)\n    result = []\n    for i in range(n):\n        for j in range(n):\n            time_difference = abs(group.iloc[i].trans_date_trans_time.value - group.iloc[j].trans_date_trans_time.value)\n            result.append([group.iloc[i].name, group.iloc[j].name, time_difference])\n    return result\n\n\nclass GCN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 16)\n        self.conv2 = GCNConv(16,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\n\nfraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\n\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50 = df50.reset_index()\ndf50.shape\n\n(12012, 23)\n\n\n\n\n\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\ndf50_tr.shape, df50_test.shape\n\n((9009, 23), (3003, 23))\n\n\n\nN = len(df50)\ntrain_mask = [i in df50_tr.index for i in range(N)]\ntest_mask = [i in df50_test.index for i in range(N)]\ntrain_mask = np.array(train_mask)\ntest_mask = np.array(test_mask)\ntrain_mask.sum(), test_mask.sum()\n\n(9009, 3003)\n\n\n\ntrain_mask.shape, test_mask.shape\n\n((12012,), (12012,))\n\n\n\n\n\n\n\n# groups = df50.groupby('cc_num')\n# edge_index_list_plus = [compute_time_difference(group) for _, group in groups]\n# edge_index_list_plus_flat = [item for sublist in edge_index_list_plus for item in sublist]\n# edge_index_list_plus_nparr = np.array(edge_index_list_plus_flat)\n# np.save('edge_index_list_plus50.npy', edge_index_list_plus_nparr)\n\n\nedge_index = np.load('edge_index_list_plus50.npy')\nedge_index.shape\n\n(200706, 3)\n\n\n\ntheta = edge_index[:,2].mean()\nedge_index = np.load('edge_index_list_plus50.npy').astype(np.float64)\nedge_index[:,2] = (np.exp(-edge_index[:,2]/theta) != 1)*(np.exp(-edge_index[:,2]/theta))\nedge_index = edge_index.tolist()\nmean_ = np.array(edge_index)[:,2].mean()\nmean_\n\n0.5098736436405648\n\n\n\nedge_index[:5]\n\n[[1023.0, 1023.0, 0.0],\n [1023.0, 1024.0, 0.9994677478343093],\n [1023.0, 1028.0, 0.9902065900321946],\n [1023.0, 1031.0, 0.97983815585674],\n [1023.0, 1032.0, 0.97983815585674]]\n\n\n\nselected_edges = [(int(row[0]), int(row[1])) for row in edge_index if row[2] &gt; mean_]\nedge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\nedge_index_selected.shape\n\ntorch.Size([2, 93730])\n\n\n\n\n\n\n\nx = torch.tensor(df50['amt'], dtype=torch.float).reshape(-1,1)\ny = torch.tensor(df50['is_fraud'],dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index = edge_index_selected, y=y, train_mask = train_mask, test_mask = test_mask)\ndata\n\nData(x=[12012, 1], edge_index=[2, 93730], y=[12012], train_mask=[12012], test_mask=[12012])"
  },
  {
    "objectID": "posts/230913 데이터(11, df50 정리)-Copy1.html#데이터정리",
    "href": "posts/230913 데이터(11, df50 정리)-Copy1.html#데이터정리",
    "title": "[FRAUD] 데이터정리 시도(df50다른것들 시드고정)",
    "section": "",
    "text": "_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50 = df50.reset_index()\ndf50.shape\n\n(12012, 23)\n\n\n\n\n\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\ndf50_tr.shape, df50_test.shape\n\n((9009, 23), (3003, 23))\n\n\n\nN = len(df50)\ntrain_mask = [i in df50_tr.index for i in range(N)]\ntest_mask = [i in df50_test.index for i in range(N)]\ntrain_mask = np.array(train_mask)\ntest_mask = np.array(test_mask)\ntrain_mask.sum(), test_mask.sum()\n\n(9009, 3003)\n\n\n\ntrain_mask.shape, test_mask.shape\n\n((12012,), (12012,))\n\n\n\n\n\n\n\n# groups = df50.groupby('cc_num')\n# edge_index_list_plus = [compute_time_difference(group) for _, group in groups]\n# edge_index_list_plus_flat = [item for sublist in edge_index_list_plus for item in sublist]\n# edge_index_list_plus_nparr = np.array(edge_index_list_plus_flat)\n# np.save('edge_index_list_plus50.npy', edge_index_list_plus_nparr)\n\n\nedge_index = np.load('edge_index_list_plus50.npy')\nedge_index.shape\n\n(200706, 3)\n\n\n\ntheta = edge_index[:,2].mean()\nedge_index = np.load('edge_index_list_plus50.npy').astype(np.float64)\nedge_index[:,2] = (np.exp(-edge_index[:,2]/theta) != 1)*(np.exp(-edge_index[:,2]/theta))\nedge_index = edge_index.tolist()\nmean_ = np.array(edge_index)[:,2].mean()\nmean_\n\n0.5098736436405648\n\n\n\nedge_index[:5]\n\n[[1023.0, 1023.0, 0.0],\n [1023.0, 1024.0, 0.9994677478343093],\n [1023.0, 1028.0, 0.9902065900321946],\n [1023.0, 1031.0, 0.97983815585674],\n [1023.0, 1032.0, 0.97983815585674]]\n\n\n\nselected_edges = [(int(row[0]), int(row[1])) for row in edge_index if row[2] &gt; mean_]\nedge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\nedge_index_selected.shape\n\ntorch.Size([2, 93730])\n\n\n\n\n\n\n\nx = torch.tensor(df50['amt'], dtype=torch.float).reshape(-1,1)\ny = torch.tensor(df50['is_fraud'],dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index = edge_index_selected, y=y, train_mask = train_mask, test_mask = test_mask)\ndata\n\nData(x=[12012, 1], edge_index=[2, 93730], y=[12012], train_mask=[12012], test_mask=[12012])"
  },
  {
    "objectID": "posts/230913 데이터(11, df50 정리)-Copy1.html#분석-1gnn",
    "href": "posts/230913 데이터(11, df50 정리)-Copy1.html#분석-1gnn",
    "title": "[FRAUD] 데이터정리 시도(df50다른것들 시드고정)",
    "section": "분석 1(GNN)",
    "text": "분석 1(GNN)\n\ntorch.manual_seed(202250926)\n\n&lt;torch._C.Generator at 0x7ff55abd42d0&gt;\n\n\n\nX = (data.x[data.train_mask]).numpy()\nXX = (data.x[data.test_mask]).numpy()\ny = (data.y[data.train_mask]).numpy()\nyy = (data.y[data.test_mask]).numpy()\n\n\nmodel = GCN()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(400):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\nmodel.eval()\n\nGCN(\n  (conv1): GCNConv(1, 16)\n  (conv2): GCNConv(16, 2)\n)\n\n\n\npred = model(data).argmax(dim=1)\nyyhat = pred[data.test_mask]\n\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n\n_results1= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석1'])\n_results1\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석1\n0.897436\n0.861759\n0.949242\n0.903388"
  },
  {
    "objectID": "posts/230913 데이터(11, df50 정리)-Copy1.html#분석2로지스틱-회귀",
    "href": "posts/230913 데이터(11, df50 정리)-Copy1.html#분석2로지스틱-회귀",
    "title": "[FRAUD] 데이터정리 시도(df50다른것들 시드고정)",
    "section": "분석2(로지스틱 회귀)",
    "text": "분석2(로지스틱 회귀)\n\nX = np.array(df50_tr.loc[:,['amt']])\nXX = np.array(df50_test.loc[:,['amt']])\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n\nlrnr = sklearn.linear_model.LogisticRegression()\n\n\nlrnr.fit(X,y)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\n#thresh = y.mean()\n#yyhat = (lrnr.predict_proba(XX)&gt; thresh)[:,-1]\nyyhat = lrnr.predict(XX) \n\n\nyyhat\n\narray([0, 1, 0, ..., 0, 0, 1])\n\n\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n\n_results2= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석2'])\n_results2\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석2\n0.849484\n0.933279\n0.756098\n0.835397"
  },
  {
    "objectID": "posts/230913 데이터(11, df50 정리)-Copy1.html#분석3서포트-벡터-머신",
    "href": "posts/230913 데이터(11, df50 정리)-Copy1.html#분석3서포트-벡터-머신",
    "title": "[FRAUD] 데이터정리 시도(df50다른것들 시드고정)",
    "section": "분석3(서포트 벡터 머신)",
    "text": "분석3(서포트 벡터 머신)\n\nX = np.array(df50_tr.loc[:, ['amt']])\nXX = np.array(df50_test.loc[:, ['amt']])\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n\nlrnr = SVC(kernel='linear')  \nlrnr.fit(X,y)\nyyhat = lrnr.predict(XX)\n\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n\n_results3= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석3'])\n_results3\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석3\n0.85015\n0.93551\n0.755438\n0.835886"
  },
  {
    "objectID": "posts/230913 데이터(11, df50 정리)-Copy1.html#분석4랜덤-포레스트",
    "href": "posts/230913 데이터(11, df50 정리)-Copy1.html#분석4랜덤-포레스트",
    "title": "[FRAUD] 데이터정리 시도(df50다른것들 시드고정)",
    "section": "분석4(랜덤 포레스트)",
    "text": "분석4(랜덤 포레스트)\n\nX = np.array(df50_tr.loc[:, ['amt']])\nXX = np.array(df50_test.loc[:, ['amt']])\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n\nlrnr = RandomForestClassifier()  \nlrnr.fit(X, y)\nyyhat = lrnr.predict(XX)\n\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n\n_results4= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석4'])\n_results4\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석4\n0.846487\n0.850133\n0.845089\n0.847603"
  },
  {
    "objectID": "posts/230913 데이터(11, df50 정리)-Copy1.html#분석5부스팅",
    "href": "posts/230913 데이터(11, df50 정리)-Copy1.html#분석5부스팅",
    "title": "[FRAUD] 데이터정리 시도(df50다른것들 시드고정)",
    "section": "분석5(부스팅)",
    "text": "분석5(부스팅)\n\nX = np.array(df50_tr.loc[:, ['amt']])\nXX = np.array(df50_test.loc[:, ['amt']])\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n\nlrnr = xgb.XGBClassifier()  \nlrnr.fit(X, y)\nyyhat = lrnr.predict(XX)\n\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n\n_results5= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석5'])\n_results5\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석5\n0.88012\n0.886957\n0.874094\n0.880478"
  },
  {
    "objectID": "posts/230913 데이터(11, df50 정리)-Copy1.html#분석6naive-bayes",
    "href": "posts/230913 데이터(11, df50 정리)-Copy1.html#분석6naive-bayes",
    "title": "[FRAUD] 데이터정리 시도(df50다른것들 시드고정)",
    "section": "분석6(Naive Bayes)",
    "text": "분석6(Naive Bayes)\n\nX = np.array(df50_tr.loc[:, ['amt']])\nXX = np.array(df50_test.loc[:, ['amt']])\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n\nlrnr = GaussianNB() \nlrnr.fit(X, y)\nyyhat = lrnr.predict(XX)\n\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n\n_results6= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석6'])\n_results6\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석6\n0.857143\n0.957143\n0.750824\n0.841522"
  },
  {
    "objectID": "posts/231011 데이터(18, df50 정리3)-Copy1.html",
    "href": "posts/231011 데이터(18, df50 정리3)-Copy1.html",
    "title": "[FRAUD] df02_f1 0.259",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\nimport xgboost as xgb\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# gnn\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric\nfrom torch_geometric.nn import GCNConv\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_geometric/typing.py:18: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/libpyg.so: undefined symbol: _ZN2at4_ops12split_Tensor4callERKNS_6TensorEN3c106SymIntEl\n  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_geometric/typing.py:31: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_scatter/_scatter_cuda.so: undefined symbol: _ZNK3c107SymBool10guard_boolEPKcl\n  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_geometric/typing.py:42: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_sparse/_diag_cuda.so: undefined symbol: _ZN3c106detail19maybe_wrap_dim_slowIlEET_S2_S2_b\n  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n\n\n\ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef compute_time_difference(group):\n    n = len(group)\n    result = []\n    for i in range(n):\n        for j in range(n):\n            time_difference = abs(group.iloc[i].trans_date_trans_time.value - group.iloc[j].trans_date_trans_time.value)\n            result.append([group.iloc[i].name, group.iloc[j].name, time_difference])\n    return result\n\ndef mask(df):\n    df_tr,df_test = sklearn.model_selection.train_test_split(df, random_state=42)\n    N = len(df)\n    train_mask = [i in df_tr.index for i in range(N)]\n    test_mask = [i in df_test.index for i in range(N)]\n    train_mask = np.array(train_mask)\n    test_mask = np.array(test_mask)\n    return train_mask, test_mask\n\ndef edge_index_selected(edge_index):\n    theta = edge_index[:,2].mean()\n    edge_index[:,2] = (np.exp(-edge_index[:,2]/theta) != 1)*(np.exp(-edge_index[:,2]/theta))\n    edge_index = edge_index.tolist()\n    mean_ = np.array(edge_index)[:,2].mean()\n    selected_edges = [(int(row[0]), int(row[1])) for row in edge_index if row[2] &gt; mean_]\n    edge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\n    return edge_index_selected\n\n\n\n\nfraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\n\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf02 = df02.reset_index()\n\n\n# df50 = down_sample_textbook(df02)\n# df50 = df50.reset_index()\n# df50.shape\n\n\n\n\n\nmask(df02)\n\n(array([ True,  True,  True, ...,  True,  True,  True]),\n array([False, False, False, ..., False, False, False]))\n\n\n\ntrain_mask, test_mask = mask(df02)\n\n\n\n\n\n\n# def compute_time_difference(group):\n#     n = len(group)\n#     result = []\n#     for i in range(n):\n#         for j in range(n):\n#             time_difference = abs(group.iloc[i].trans_date_trans_time.value - group.iloc[j].trans_date_trans_time.value)\n#             result.append([group.iloc[i].name, group.iloc[j].name, time_difference])\n#     return result\n\n\n\n# groups = df02.groupby('cc_num')\n# edge_index_list_plus = [compute_time_difference(group) for _, group in groups]\n# edge_index_list_plus_flat = [item for sublist in edge_index_list_plus for item in sublist]\n# edge_index_list_plus_nparr = np.array(edge_index_list_plus_flat)\n# np.save('edge_index_list_plus02_.npy', edge_index_list_plus_nparr)\n\n\n# groups = df50.groupby('cc_num')\n# edge_index_list_plus = [compute_time_difference(group) for _, group in groups]\n# edge_index_list_plus_flat = [item for sublist in edge_index_list_plus for item in sublist]\n# edge_index_list_plus_nparr = np.array(edge_index_list_plus_flat)\n# np.save('edge_index_list_plus50.npy', edge_index_list_plus_nparr)\n\n\nedge_index = np.load('edge_index_list_plus02_.npy').astype(np.float64)\nedge_index.shape\n\n(65831594, 3)\n\n\n\nedge_index\n\narray([[2.881000e+03, 2.881000e+03, 0.000000e+00],\n       [2.881000e+03, 3.061000e+03, 1.909224e+16],\n       [2.881000e+03, 4.867000e+03, 2.427060e+15],\n       ...,\n       [2.127710e+05, 2.127650e+05, 2.688000e+13],\n       [2.127710e+05, 2.127690e+05, 9.000000e+11],\n       [2.127710e+05, 2.127710e+05, 0.000000e+00]])\n\n\n\nedge_index_selected = edge_index_selected(edge_index)\n\n\nedge_index_selected\n\ntensor([[  2881,   2881,   2881,  ..., 212771, 212771, 212771],\n        [  4867,   9618,  12525,  ..., 212759, 212765, 212769]])\n\n\n\n\n\n\n\nx = torch.tensor(df02['amt'], dtype=torch.float).reshape(-1,1)\ny = torch.tensor(df02['is_fraud'],dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index = edge_index_selected, y=y, train_mask = train_mask, test_mask = test_mask)\ndata\n\nData(x=[214520, 1], edge_index=[2, 29970380], y=[214520], train_mask=[214520], test_mask=[214520])\n\n\n\n\n\n\ntorch.manual_seed(202250926)\nclass GCN2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 32)\n        self.conv2 = GCNConv(32,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\nX = (data.x[data.train_mask]).numpy()\nXX = (data.x[data.test_mask]).numpy()\ny = (data.y[data.train_mask]).numpy()\nyy = (data.y[data.test_mask]).numpy()\n\nmodel = GCN2()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(400):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\nmodel.eval()\n\npred = model(data).argmax(dim=1)\nyyhat = pred[data.test_mask]\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results1= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석1'])\n_results1\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석1\n0.972217\n0.590909\n0.165605\n0.258706"
  },
  {
    "objectID": "posts/231011 데이터(18, df50 정리3)-Copy1.html#데이터정리",
    "href": "posts/231011 데이터(18, df50 정리3)-Copy1.html#데이터정리",
    "title": "[FRAUD] df02_f1 0.259",
    "section": "",
    "text": "_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf02 = df02.reset_index()\n\n\n# df50 = down_sample_textbook(df02)\n# df50 = df50.reset_index()\n# df50.shape\n\n\n\n\n\nmask(df02)\n\n(array([ True,  True,  True, ...,  True,  True,  True]),\n array([False, False, False, ..., False, False, False]))\n\n\n\ntrain_mask, test_mask = mask(df02)\n\n\n\n\n\n\n# def compute_time_difference(group):\n#     n = len(group)\n#     result = []\n#     for i in range(n):\n#         for j in range(n):\n#             time_difference = abs(group.iloc[i].trans_date_trans_time.value - group.iloc[j].trans_date_trans_time.value)\n#             result.append([group.iloc[i].name, group.iloc[j].name, time_difference])\n#     return result\n\n\n\n# groups = df02.groupby('cc_num')\n# edge_index_list_plus = [compute_time_difference(group) for _, group in groups]\n# edge_index_list_plus_flat = [item for sublist in edge_index_list_plus for item in sublist]\n# edge_index_list_plus_nparr = np.array(edge_index_list_plus_flat)\n# np.save('edge_index_list_plus02_.npy', edge_index_list_plus_nparr)\n\n\n# groups = df50.groupby('cc_num')\n# edge_index_list_plus = [compute_time_difference(group) for _, group in groups]\n# edge_index_list_plus_flat = [item for sublist in edge_index_list_plus for item in sublist]\n# edge_index_list_plus_nparr = np.array(edge_index_list_plus_flat)\n# np.save('edge_index_list_plus50.npy', edge_index_list_plus_nparr)\n\n\nedge_index = np.load('edge_index_list_plus02_.npy').astype(np.float64)\nedge_index.shape\n\n(65831594, 3)\n\n\n\nedge_index\n\narray([[2.881000e+03, 2.881000e+03, 0.000000e+00],\n       [2.881000e+03, 3.061000e+03, 1.909224e+16],\n       [2.881000e+03, 4.867000e+03, 2.427060e+15],\n       ...,\n       [2.127710e+05, 2.127650e+05, 2.688000e+13],\n       [2.127710e+05, 2.127690e+05, 9.000000e+11],\n       [2.127710e+05, 2.127710e+05, 0.000000e+00]])\n\n\n\nedge_index_selected = edge_index_selected(edge_index)\n\n\nedge_index_selected\n\ntensor([[  2881,   2881,   2881,  ..., 212771, 212771, 212771],\n        [  4867,   9618,  12525,  ..., 212759, 212765, 212769]])\n\n\n\n\n\n\n\nx = torch.tensor(df02['amt'], dtype=torch.float).reshape(-1,1)\ny = torch.tensor(df02['is_fraud'],dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index = edge_index_selected, y=y, train_mask = train_mask, test_mask = test_mask)\ndata\n\nData(x=[214520, 1], edge_index=[2, 29970380], y=[214520], train_mask=[214520], test_mask=[214520])\n\n\n\n\n\n\ntorch.manual_seed(202250926)\nclass GCN2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 32)\n        self.conv2 = GCNConv(32,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\nX = (data.x[data.train_mask]).numpy()\nXX = (data.x[data.test_mask]).numpy()\ny = (data.y[data.train_mask]).numpy()\nyy = (data.y[data.test_mask]).numpy()\n\nmodel = GCN2()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(400):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\nmodel.eval()\n\npred = model(data).argmax(dim=1)\nyyhat = pred[data.test_mask]\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results1= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석1'])\n_results1\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석1\n0.972217\n0.590909\n0.165605\n0.258706"
  },
  {
    "objectID": "posts/231102 책_코드 비교_그림.html",
    "href": "posts/231102 책_코드 비교_그림.html",
    "title": "[FRAUD] 책_코드",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\nimport xgboost as xgb\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# gnn\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric\nfrom torch_geometric.nn import GCNConv\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_geometric/typing.py:18: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/libpyg.so: undefined symbol: _ZN2at4_ops12split_Tensor4callERKNS_6TensorEN3c106SymIntEl\n  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_geometric/typing.py:31: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_scatter/_scatter_cuda.so: undefined symbol: _ZNK3c107SymBool10guard_boolEPKcl\n  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_geometric/typing.py:42: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_sparse/_diag_cuda.so: undefined symbol: _ZN3c106detail19maybe_wrap_dim_slowIlEET_S2_S2_b\n  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef compute_time_difference(group):\n    n = len(group)\n    result = []\n    for i in range(n):\n        for j in range(n):\n            time_difference = abs(group.iloc[i].trans_date_trans_time.value - group.iloc[j].trans_date_trans_time.value)\n            result.append([group.iloc[i].name, group.iloc[j].name, time_difference])\n    return result\n\ndef mask(df):\n    df_tr,df_test = sklearn.model_selection.train_test_split(df, random_state=42)\n    N = len(df)\n    train_mask = [i in df_tr.index for i in range(N)]\n    test_mask = [i in df_test.index for i in range(N)]\n    train_mask = np.array(train_mask)\n    test_mask = np.array(test_mask)\n    return train_mask, test_mask\n\ndef edge_index_selected(edge_index):\n    theta = edge_index[:,2].mean()\n    edge_index[:,2] = (np.exp(-edge_index[:,2]/theta) != 1)*(np.exp(-edge_index[:,2]/theta))\n    edge_index = edge_index.tolist()\n    mean_ = np.array(edge_index)[:,2].mean()\n    selected_edges = [(int(row[0]), int(row[1])) for row in edge_index if row[2] &gt; mean_]\n    edge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\n    return edge_index_selected\nfraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns"
  },
  {
    "objectID": "posts/231102 책_코드 비교_그림.html#데이터정리",
    "href": "posts/231102 책_코드 비교_그림.html#데이터정리",
    "title": "[FRAUD] 책_코드",
    "section": "데이터정리",
    "text": "데이터정리\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50 = df50.reset_index()\ndf50.shape\n\n(12012, 23)\n\n\n\n\ntr/test\n\nmask(df50)\n\n(array([False,  True,  True, ...,  True, False,  True]),\n array([ True, False, False, ..., False,  True, False]))\n\n\n\ntrain_mask, test_mask = mask(df50)\n\n\n\n\nedge_index 설정\n\n# groups = df50.groupby('cc_num')\n# edge_index_list_plus = [compute_time_difference(group) for _, group in groups]\n# edge_index_list_plus_flat = [item for sublist in edge_index_list_plus for item in sublist]\n# edge_index_list_plus_nparr = np.array(edge_index_list_plus_flat)\n# np.save('edge_index_list_plus50.npy', edge_index_list_plus_nparr)\n\n\nedge_index = np.load('edge_index_list_plus50.npy').astype(np.float64)\nedge_index.shape\n\n(200706, 3)\n\n\n\nedge_index_selected = edge_index_selected(edge_index)\n\n\nedge_index_selected\n\ntensor([[ 1023,  1023,  1023,  ..., 11944, 11944, 11944],\n        [ 1024,  1028,  1031,  ...,  4257,  9241,  9782]])"
  },
  {
    "objectID": "posts/231102 책_코드 비교_그림.html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "href": "posts/231102 책_코드 비교_그림.html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "title": "[FRAUD] 책_코드",
    "section": "사기 탐지를 위한 지도 및 비지도 임베딩",
    "text": "사기 탐지를 위한 지도 및 비지도 임베딩\n\n지도학습\n\nG_down = build_graph_bipartite(df50)\n\n\n# from sklearn.utils import resample\n\n# df_majority = df[df.is_fraud==0]\n# df_minority = df[df.is_fraud==1]\n\n# df_maj_dowsampled = resample(df_majority,\n#                              n_samples=len(df_minority),\n#                              random_state=42)\n\n# df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n\n# print(df_downsampled.is_fraud.value_counts())\n# G_down = build_graph_bipartite(df_downsampled)\n\n1    6006\n0    6006\nName: is_fraud, dtype: int64\n\n\n\n# G_down.edges\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_edges, test_edges, train_labels, test_labels = train_test_split(list(range(len(G_down.edges))), \n                                                                      list(nx.get_edge_attributes(G_down, \"label\").values()), \n                                                                      test_size=0.20, \n                                                                      random_state=42)\n\n\nedgs = list(G_down.edges)\ntrain_graph = G_down.edge_subgraph([edgs[x] for x in train_edges]).copy()\ntrain_graph.add_nodes_from(list(set(G_down.nodes) - set(train_graph.nodes)))\n\n\ntrain_graph.number_of_edges(), train_graph.number_of_nodes()\n\n(9351, 1624)\n\n\n\n데이터 8:2 비율로 학습 검증\n\n\nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\nnode2vec_train = Node2Vec(train_graph, weight_key='weight')\nmodel_train = node2vec_train.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04&lt;00:00,  2.41it/s]\n\n\n\nNode2Vec 알고리즘 사용해 특징 공간 구축\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\n\n# 설정할 시드 값\nseed = 202250926\n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\n\nfor cl in classes:\n    embeddings_train = cl(keyed_vectors=model_train.wv)\n\n    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n\n    rf = RandomForestClassifier(n_estimators=1000, random_state=seed)\n    rf.fit(train_embeddings, train_labels)\n\n    y_pred = rf.predict(test_embeddings)\n    print(cl)\n    print('Precision:', metrics.precision_score(test_labels, y_pred))\n    print('Recall:', metrics.recall_score(test_labels, y_pred))\n    print('F1-Score:', metrics.f1_score(test_labels, y_pred))\n    print('Accuracy:', metrics.accuracy_score(test_labels, y_pred))\n\n&lt;class 'node2vec.edges.HadamardEmbedder'&gt;\nPrecision: 0.7210884353741497\nRecall: 0.1781512605042017\nF1-Score: 0.2857142857142857\nAccuracy: 0.5466210436270317\n&lt;class 'node2vec.edges.AverageEmbedder'&gt;\nPrecision: 0.6987673343605547\nRecall: 0.7621848739495798\nF1-Score: 0.7290996784565916\nAccuracy: 0.7117194183062446\n&lt;class 'node2vec.edges.WeightedL1Embedder'&gt;\nPrecision: 0.5813953488372093\nRecall: 0.02100840336134454\nF1-Score: 0.040551500405515\nAccuracy: 0.4940119760479042\n&lt;class 'node2vec.edges.WeightedL2Embedder'&gt;\nPrecision: 0.6444444444444445\nRecall: 0.024369747899159664\nF1-Score: 0.04696356275303644\nAccuracy: 0.4965782720273738\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# 그래프를 그릴 때 사용할 노드 및 엣지 색상 설정\nnode_color = 'lightblue'\nedge_color = 'gray'\n\n# 그래프 그리기\nplt.figure(figsize=(10, 10))\npos = nx.spring_layout(train_graph)  # 그래프의 레이아웃 설정\nnx.draw(train_graph, pos, with_labels=True, node_color=node_color, edge_color=edge_color)\nplt.title('Train Graph')\nplt.show()\n\n\n\n\n\n\n비지도학습\n\nk-means 알고리즘 사용\n지도학습과의 차이점은 특징 공간이 학습-검증 분할을 안함.\n\n\nnod2vec_unsup = Node2Vec(G_down, weight_key='weight')\nunsup_vals = nod2vec_unsup.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04&lt;00:00,  2.28it/s]\n\n\n\n다운샘플링 절차에 전체 그래프 알고리즘 계산\n\n\nfrom sklearn.cluster import KMeans\n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\ntrue_labels = [x for x in nx.get_edge_attributes(G_down, \"label\").values()]\n\nfor cl in classes:\n    embedding_edge = cl(keyed_vectors=unsup_vals.wv) \n\n    embedding = [embedding_edge[str(x[0]), str(x[1])] for x in G_down.edges()]\n    kmeans = KMeans(2, random_state=42).fit(embedding)\n    \n    \n    nmi = metrics.adjusted_mutual_info_score(true_labels, kmeans.labels_)\n    ho = metrics.homogeneity_score(true_labels, kmeans.labels_)\n    co = metrics.completeness_score(true_labels, kmeans.labels_)\n    vmeasure = metrics.v_measure_score(true_labels, kmeans.labels_)\n    \n    print(cl)\n    print('NMI:', nmi)\n    print('Homogeneity:', ho)\n    print('Completeness:', co)\n    print('V-Measure:', vmeasure)\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601a0a60&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601a0a60&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601a0a60&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601a0a60&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f417cde11f0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f417cde11f0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601a0a60&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\n\n\n&lt;class 'node2vec.edges.HadamardEmbedder'&gt;\nNMI: 0.0488304103554333\nHomogeneity: 0.04260773965601987\nCompleteness: 0.05736666210172528\nV-Measure: 0.04889779305082587\n&lt;class 'node2vec.edges.AverageEmbedder'&gt;\nNMI: 0.09709110346937713\nHomogeneity: 0.09171818451571066\nCompleteness: 0.10326600134945695\nV-Measure: 0.09715013680667017\n&lt;class 'node2vec.edges.WeightedL1Embedder'&gt;\nNMI: 0.1713964349047398\nHomogeneity: 0.1714171156088649\nCompleteness: 0.17147807224687064\nV-Measure: 0.17144758850972272\n&lt;class 'node2vec.edges.WeightedL2Embedder'&gt;\nNMI: 0.13969156796365498\nHomogeneity: 0.13857242672795833\nCompleteness: 0.1409378289079951\nV-Measure: 0.13974511901264577"
  },
  {
    "objectID": "posts/230822 데이터(6, df02).html",
    "href": "posts/230822 데이터(6, df02).html",
    "title": "[FRAUD] 데이터정리 시도(8.22 df02시도)",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\nimport torch\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics \n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부     \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\ndef build_graph_tripartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n                                                       df[\"cc_num\"].values.tolist() +\n                                                       df[\"merchant\"].values.tolist()))}\n    df[\"in_node\"]= df[\"cc_num\"].apply(lambda x: mapping[x])\n    df[\"out_node\"]=df[\"merchant\"].apply(lambda x:mapping[x])\n    \n        \n    G=nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\\\n                        [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")     \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")   \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")  \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n\n    return G\n    \n    \ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef embedding(Graph):\n    # Graph -&gt; X (feature)\n    _edgs = list(Graph.edges)\n    subGraph = Graph.edge_subgraph([_edgs[x] for x in range(len(Graph.edges))]).copy()\n    subGraph.add_nodes_from(list(set(Graph.nodes) - set(subGraph.nodes)))    \n    embedded = AverageEmbedder(Node2Vec(subGraph, weight_key='weight').fit(window=10).wv)\n    X = [embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in range(len(Graph.edges))]\n    # Graph -&gt; y (label)\n    y = np.array(list(nx.get_edge_attributes(Graph, \"label\").values()))\n    return X,y \n\ndef anal(df):\n    Graph = build_graph_bipartite(df)\n    X,XX,y,yy = embedding(Graph)\n    lrnr = RandomForestClassifier(n_estimators=100, random_state=42) \n    lrnr.fit(X,y)\n    yyhat = lrnr.predict(XX)\n    df = pd.DataFrame({\n        'acc':[sklearn.metrics.accuracy_score(yy,yyhat)], \n        'pre':[sklearn.metrics.precision_score(yy,yyhat)], \n        'rec':[sklearn.metrics.recall_score(yy,yyhat)],\n        'f1':[sklearn.metrics.f1_score(yy,yyhat)]}\n    )    \n    return df\n\ndef our_sampling1(df):\n    cus_list = set(df.query('is_fraud==1').cc_num.tolist())\n    return df.query(\"cc_num in @ cus_list\")\n\n\nfraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\n\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\n214520*214520\n\n46018830400\n\n\n\ndf02.is_fraud.mean().round(5)\n\n0.028\n\n\n\n사기거래 빈도..\n\n\ndf02 = df02.reset_index()\n\n\nN = len(df02)\n\n\n\n\n\ndf02_tr,df02_test = sklearn.model_selection.train_test_split(df02, random_state=42)\n\n\ndf02_tr.is_fraud.mean().round(5), df02_test.is_fraud.mean().round(5)\n\n(0.02757, 0.02927)\n\n\n\ndf02_tr.shape, df02_test.shape\n\n((160890, 23), (53630, 23))\n\n\n\ntrain_mask = np.concatenate((np.full(160890, True), np.full(53630, False)))\ntest_mask = np.concatenate((np.full(160890, False), np.full(53630, True)))\nprint(\"Train Mask:\", train_mask)\nprint(\"Test Mask:\", test_mask)\n\nTrain Mask: [ True  True  True ... False False False]\nTest Mask: [False False False ...  True  True  True]\n\n\n\ntrain_mask.sum(), test_mask.sum()\n\n(160890, 53630)\n\n\n\ndf02_com = pd.concat([df02_tr, df02_test])\n\n\ndf02_com = df02_com.reset_index()\n\n\nnp.save('df02_com.npy', df02_com)\n\n\ndf02_com\n\n\n\n\n\n\n\n\nlevel_0\nindex\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n176322\n944206\n2020-01-12 14:26:00\n1.800680e+14\nfraud_Durgan, Gislason and Spencer\nhome\n83.42\nMary\nJuarez\nF\n...\n42.9385\n-88.3950\n2328\nApplications developer\n1942-01-06\ndac0ad2e6b9956237cdca85beea4b422\n1358000819\n43.301471\n-88.731241\n0\n\n\n1\n57361\n305252\n2019-05-27 23:22:00\n4.158950e+15\nfraud_Douglas-White\nentertainment\n119.90\nJustin\nBell\nM\n...\n40.4308\n-79.9205\n687276\nScientist, marine\n1973-10-19\n6660431462def289ceb3e176e88f58e5\n1338160935\n40.673836\n-80.710911\n0\n\n\n2\n76922\n326443\n2019-06-04 19:27:00\n3.040770e+13\nfraud_Bernier and Sons\nkids_pets\n47.11\nDanielle\nEvans\nF\n...\n42.1939\n-76.7361\n520\nPsychotherapist\n1991-10-13\n3e0fdbbb80e5e068e5873be2a539cc24\n1338838050\n42.298622\n-77.473862\n0\n\n\n3\n73661\n515686\n2019-08-11 09:04:00\n4.319580e+18\nfraud_Kutch LLC\ngas_transport\n56.51\nKathleen\nNash\nF\n...\n37.1788\n-82.6950\n502\nChief Financial Officer\n1960-02-01\n66c331ada80949f23b6eb54a2a805b30\n1344675872\n37.867947\n-83.096063\n0\n\n\n4\n149325\n217309\n2019-04-20 21:16:00\n6.041621e+10\nfraud_Beer-Jast\nkids_pets\n1.42\nMary\nDiaz\nF\n...\n43.0048\n-108.8964\n1645\nInformation systems manager\n1986-02-17\n73d345383dacf28ddb303df878af6034\n1334956594\n43.454507\n-109.492721\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n214515\n209807\n138275\n2019-03-16 22:16:00\n6.761020e+11\nfraud_Crooks and Sons\npersonal_care\n23.07\nNatasha\nMclaughlin\nF\n...\n38.4549\n-122.2564\n94014\nAirline pilot\n1985-08-21\nf5ed056d58e7cafe35201991383d5af7\n1331936181\n38.440172\n-121.930178\n1\n\n\n214516\n116046\n108839\n2019-03-03 16:55:00\n3.453890e+14\nfraud_Dooley Inc\nshopping_pos\n2.57\nJustin\nFowler\nM\n...\n33.9215\n-89.6782\n3451\nFinancial trader\n1984-05-19\n617d850e784d23f70bdbf99aec16877d\n1330793701\n34.248988\n-88.691253\n0\n\n\n214517\n84374\n244548\n2019-05-02 21:02:00\n4.170690e+15\nfraud_Hettinger, McCullough and Fay\nhome\n15.40\nSamuel\nFrey\nM\n...\n35.6665\n-97.4798\n116001\nMedia buyer\n1993-05-10\n6e7be41baa4854c8c122124573ab826a\n1335992520\n34.789935\n-96.704044\n0\n\n\n214518\n766\n968783\n2020-01-26 17:33:00\n4.477160e+18\nfraud_Jast Ltd\nshopping_net\n66.19\nAngela\nRoss\nF\n...\n40.3928\n-111.7941\n42384\nFutures trader\n1992-12-29\n823cb59773e114b5de50c51ce520f181\n1359221619\n40.382572\n-111.342788\n0\n\n\n214519\n18374\n929851\n2020-01-04 13:22:00\n4.742880e+18\nfraud_Kihn-Schuster\nfood_dining\n30.57\nCassandra\nSanders\nF\n...\n20.0271\n-155.3697\n1490\nScientist, research (maths)\n1991-04-13\na0921bbdc96d65bfcfdb78b369233303\n1357305750\n21.010877\n-155.079405\n0\n\n\n\n\n214520 rows × 24 columns"
  },
  {
    "objectID": "posts/230822 데이터(6, df02).html#시도",
    "href": "posts/230822 데이터(6, df02).html#시도",
    "title": "[FRAUD] 데이터정리 시도(8.22 df02시도)",
    "section": "",
    "text": "_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\n214520*214520\n\n46018830400\n\n\n\ndf02.is_fraud.mean().round(5)\n\n0.028\n\n\n\n사기거래 빈도..\n\n\ndf02 = df02.reset_index()\n\n\nN = len(df02)\n\n\n\n\n\ndf02_tr,df02_test = sklearn.model_selection.train_test_split(df02, random_state=42)\n\n\ndf02_tr.is_fraud.mean().round(5), df02_test.is_fraud.mean().round(5)\n\n(0.02757, 0.02927)\n\n\n\ndf02_tr.shape, df02_test.shape\n\n((160890, 23), (53630, 23))\n\n\n\ntrain_mask = np.concatenate((np.full(160890, True), np.full(53630, False)))\ntest_mask = np.concatenate((np.full(160890, False), np.full(53630, True)))\nprint(\"Train Mask:\", train_mask)\nprint(\"Test Mask:\", test_mask)\n\nTrain Mask: [ True  True  True ... False False False]\nTest Mask: [False False False ...  True  True  True]\n\n\n\ntrain_mask.sum(), test_mask.sum()\n\n(160890, 53630)\n\n\n\ndf02_com = pd.concat([df02_tr, df02_test])\n\n\ndf02_com = df02_com.reset_index()\n\n\nnp.save('df02_com.npy', df02_com)\n\n\ndf02_com\n\n\n\n\n\n\n\n\nlevel_0\nindex\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n176322\n944206\n2020-01-12 14:26:00\n1.800680e+14\nfraud_Durgan, Gislason and Spencer\nhome\n83.42\nMary\nJuarez\nF\n...\n42.9385\n-88.3950\n2328\nApplications developer\n1942-01-06\ndac0ad2e6b9956237cdca85beea4b422\n1358000819\n43.301471\n-88.731241\n0\n\n\n1\n57361\n305252\n2019-05-27 23:22:00\n4.158950e+15\nfraud_Douglas-White\nentertainment\n119.90\nJustin\nBell\nM\n...\n40.4308\n-79.9205\n687276\nScientist, marine\n1973-10-19\n6660431462def289ceb3e176e88f58e5\n1338160935\n40.673836\n-80.710911\n0\n\n\n2\n76922\n326443\n2019-06-04 19:27:00\n3.040770e+13\nfraud_Bernier and Sons\nkids_pets\n47.11\nDanielle\nEvans\nF\n...\n42.1939\n-76.7361\n520\nPsychotherapist\n1991-10-13\n3e0fdbbb80e5e068e5873be2a539cc24\n1338838050\n42.298622\n-77.473862\n0\n\n\n3\n73661\n515686\n2019-08-11 09:04:00\n4.319580e+18\nfraud_Kutch LLC\ngas_transport\n56.51\nKathleen\nNash\nF\n...\n37.1788\n-82.6950\n502\nChief Financial Officer\n1960-02-01\n66c331ada80949f23b6eb54a2a805b30\n1344675872\n37.867947\n-83.096063\n0\n\n\n4\n149325\n217309\n2019-04-20 21:16:00\n6.041621e+10\nfraud_Beer-Jast\nkids_pets\n1.42\nMary\nDiaz\nF\n...\n43.0048\n-108.8964\n1645\nInformation systems manager\n1986-02-17\n73d345383dacf28ddb303df878af6034\n1334956594\n43.454507\n-109.492721\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n214515\n209807\n138275\n2019-03-16 22:16:00\n6.761020e+11\nfraud_Crooks and Sons\npersonal_care\n23.07\nNatasha\nMclaughlin\nF\n...\n38.4549\n-122.2564\n94014\nAirline pilot\n1985-08-21\nf5ed056d58e7cafe35201991383d5af7\n1331936181\n38.440172\n-121.930178\n1\n\n\n214516\n116046\n108839\n2019-03-03 16:55:00\n3.453890e+14\nfraud_Dooley Inc\nshopping_pos\n2.57\nJustin\nFowler\nM\n...\n33.9215\n-89.6782\n3451\nFinancial trader\n1984-05-19\n617d850e784d23f70bdbf99aec16877d\n1330793701\n34.248988\n-88.691253\n0\n\n\n214517\n84374\n244548\n2019-05-02 21:02:00\n4.170690e+15\nfraud_Hettinger, McCullough and Fay\nhome\n15.40\nSamuel\nFrey\nM\n...\n35.6665\n-97.4798\n116001\nMedia buyer\n1993-05-10\n6e7be41baa4854c8c122124573ab826a\n1335992520\n34.789935\n-96.704044\n0\n\n\n214518\n766\n968783\n2020-01-26 17:33:00\n4.477160e+18\nfraud_Jast Ltd\nshopping_net\n66.19\nAngela\nRoss\nF\n...\n40.3928\n-111.7941\n42384\nFutures trader\n1992-12-29\n823cb59773e114b5de50c51ce520f181\n1359221619\n40.382572\n-111.342788\n0\n\n\n214519\n18374\n929851\n2020-01-04 13:22:00\n4.742880e+18\nfraud_Kihn-Schuster\nfood_dining\n30.57\nCassandra\nSanders\nF\n...\n20.0271\n-155.3697\n1490\nScientist, research (maths)\n1991-04-13\na0921bbdc96d65bfcfdb78b369233303\n1357305750\n21.010877\n-155.079405\n0\n\n\n\n\n214520 rows × 24 columns"
  },
  {
    "objectID": "posts/231020 데이터(19, df5.html",
    "href": "posts/231020 데이터(19, df5.html",
    "title": "[FRAUD] 데이터정리 시도(GCN_엣지손x)",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\nimport xgboost as xgb\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# gnn\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric\nfrom torch_geometric.nn import GCNConv\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_geometric/typing.py:18: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/libpyg.so: undefined symbol: _ZN2at4_ops12split_Tensor4callERKNS_6TensorEN3c106SymIntEl\n  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_geometric/typing.py:31: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_scatter/_scatter_cuda.so: undefined symbol: _ZNK3c107SymBool10guard_boolEPKcl\n  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_geometric/typing.py:42: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_sparse/_diag_cuda.so: undefined symbol: _ZN3c106detail19maybe_wrap_dim_slowIlEET_S2_S2_b\n  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n\n\n\ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef compute_time_difference(group):\n    n = len(group)\n    result = []\n    for i in range(n):\n        for j in range(n):\n            time_difference = abs(group.iloc[i].trans_date_trans_time.value - group.iloc[j].trans_date_trans_time.value)\n            result.append([group.iloc[i].name, group.iloc[j].name, time_difference])\n    return result\n\ndef mask(df):\n    df_tr,df_test = sklearn.model_selection.train_test_split(df, random_state=42)\n    N = len(df)\n    train_mask = [i in df_tr.index for i in range(N)]\n    test_mask = [i in df_test.index for i in range(N)]\n    train_mask = np.array(train_mask)\n    test_mask = np.array(test_mask)\n    return train_mask, test_mask\n\ndef edge_index_selected(edge_index):\n    theta = edge_index[:,2].mean()\n    edge_index[:,2] = (np.exp(-edge_index[:,2]/theta) != 1)*(np.exp(-edge_index[:,2]/theta))\n    edge_index = edge_index.tolist()\n    mean_ = np.array(edge_index)[:,2].mean()\n    selected_edges = [(int(row[0]), int(row[1])) for row in edge_index if row[2] &gt; mean_]\n    edge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\n    return edge_index_selected\n\n\n\n\nfraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\n\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50 = df50.reset_index()\ndf50.shape\n\n(12012, 23)\n\n\n\n\n\n\nmask(df50)\n\n(array([False,  True,  True, ...,  True, False,  True]),\n array([ True, False, False, ..., False,  True, False]))\n\n\n\ntrain_mask, test_mask = mask(df50)\n\n\n\n\n\n\n# groups = df50.groupby('cc_num')\n# edge_index_list_plus = [compute_time_difference(group) for _, group in groups]\n# edge_index_list_plus_flat = [item for sublist in edge_index_list_plus for item in sublist]\n# edge_index_list_plus_nparr = np.array(edge_index_list_plus_flat)\n# np.save('edge_index_list_plus50.npy', edge_index_list_plus_nparr)\n\n\nedge_index = np.load('edge_index_list_plus50.npy').astype(np.float64)\nedge_index.shape\n\n(200706, 3)\n\n\n\nedge_index\n\narray([[1.023000e+03, 1.023000e+03, 0.000000e+00],\n       [1.023000e+03, 1.024000e+03, 4.200000e+12],\n       [1.023000e+03, 1.028000e+03, 7.764000e+13],\n       ...,\n       [1.194400e+04, 9.782000e+03, 9.528000e+13],\n       [1.194400e+04, 1.176700e+04, 3.055758e+16],\n       [1.194400e+04, 1.194400e+04, 0.000000e+00]])\n\n\n\ngroups = df50.groupby('cc_num')\ngroups\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f8740677af0&gt;\n\n\n\nimport itertools\n\nindex_pairs = []\n\nfor _, group_df in groups:\n    indices = group_df.index.tolist()\n    index_combinations = list(itertools.combinations(indices, 2))\n    index_pairs.extend(index_combinations)\n\n\nindex_pairs = torch.tensor(index_pairs).t()\n\n\nindex_pairs.shape\n\ntorch.Size([2, 94347])\n\n\n\nindex_pairs\n\ntensor([[ 1023,  1023,  1023,  ...,  9782,  9782, 11767],\n        [ 1024,  1028,  1031,  ..., 11767, 11944, 11944]])\n\n\n\n# edge_index_selected = edge_index_selected(edge_index)\n\n\n\n\n\n\nx = torch.tensor(df50['amt'], dtype=torch.float).reshape(-1,1)\ny = torch.tensor(df50['is_fraud'],dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index = index_pairs, y=y, train_mask = train_mask, test_mask = test_mask)\ndata\n\n\nData(x=[12012, 1], edge_index=[2, 94347], y=[12012], train_mask=[12012], test_mask=[12012])\n\n\n\n\n\n\n\ntorch.manual_seed(202250926)\nclass GCN2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 32)\n        self.conv2 = GCNConv(32,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\nX = (data.x[data.train_mask]).numpy()\nXX = (data.x[data.test_mask]).numpy()\ny = (data.y[data.train_mask]).numpy()\nyy = (data.y[data.test_mask]).numpy()\n\n\nmodel = GCN2()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(400):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\nmodel.eval()\n\nGCN2(\n  (conv1): GCNConv(1, 32)\n  (conv2): GCNConv(32, 2)\n)\n\n\n\npred = model(data).argmax(dim=1)\n\n\nyyhat = pred[data.test_mask]\n\n\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results1= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석1'])\n_results1\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석1\n0.494838\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n\n\nb,W = list(model.conv1.parameters())\n\n\nb,W\n\n(Parameter containing:\n tensor([-2.6334e-02, -2.7689e-01, -1.6926e-02, -5.2382e-02, -1.2667e-01,\n         -4.2815e-02, -2.6620e-02,  1.1360e-01, -2.8240e-02, -2.1838e-01,\n          9.7086e-02, -1.2640e-02, -1.5231e-02, -1.3934e-03, -7.6017e-03,\n         -3.6946e-02, -5.4102e-10, -1.9268e-01,  1.7090e-07, -8.3471e-10,\n         -2.0092e-01, -7.3827e-02, -4.7944e-02, -1.9878e-01, -2.0639e-01,\n         -2.7078e-01, -5.2340e-03, -3.9832e-02,  3.9550e-02,  5.1139e-01,\n         -3.2245e-02,  2.2552e-02], requires_grad=True),\n Parameter containing:\n tensor([[ 0.1231],\n         [ 0.1348],\n         [-0.0504],\n         [ 0.0830],\n         [-0.0851],\n         [-0.0475],\n         [-0.0506],\n         [-0.0098],\n         [ 0.2435],\n         [ 0.1998],\n         [-0.0074],\n         [-0.0560],\n         [ 0.1415],\n         [-0.0649],\n         [-0.0586],\n         [-0.0573],\n         [-0.0653],\n         [ 0.0411],\n         [-0.0703],\n         [-0.0537],\n         [ 0.1198],\n         [-0.0690],\n         [-0.0654],\n         [ 0.2001],\n         [-0.0048],\n         [-0.0085],\n         [-0.0613],\n         [-0.0797],\n         [ 0.1049],\n         [-0.0017],\n         [-0.0547],\n         [ 0.1147]], requires_grad=True))\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# PyTorch Geometric 그래프를 NetworkX 그래프로 변환\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\n\n# 노드 1023과 연결된 모든 노드와 엣지를 포함하는 부분 그래프 추출\nsubgraph1 = nx.ego_graph(G, 1023, radius=1)\nsubgraph2 = nx.ego_graph(G, 9782, radius=1)\n\n# 왼쪽 그래프 레이아웃 설정\npos1 = nx.spring_layout(subgraph1, pos=None, seed=42)  # 그래프 레이아웃 설정\n\n# 오른쪽 그래프 레이아웃 설정\npos2 = nx.spring_layout(subgraph2, pos=None, seed=43)\n\n# 노드 그리기\nnx.draw_networkx_nodes(subgraph1, pos1, node_size=200, node_color='b', label='Node 1023')\nnx.draw_networkx_nodes(subgraph2, pos2, node_size=200, node_color='g', label='Node 9782')\n\n# 엣지 그리기\nnx.draw_networkx_edges(subgraph1, pos1)\nnx.draw_networkx_edges(subgraph2, pos2)\n\n# 노드 인덱스 표시\nlabels1 = {node: str(node) for node in subgraph1.nodes()}\nlabels2 = {node: str(node) for node in subgraph2.nodes()}\nnx.draw_networkx_labels(subgraph1, pos1, labels1, font_size=10)\nnx.draw_networkx_labels(subgraph2, pos2, labels2, font_size=10)\n\n# 그래프 출력\nplt.axis('off')  # 축 숨기기\n\n# 레이블 표시\nplt.legend(loc='best')\n\nplt.show()\n\n[](231020 데이터(19, df5_files/figure-html/cell-28-output-1.png){}\n\n\n\n# 노드 1023와 9782 간의 연결 확인\nare_connected = G.has_edge(1023, 9782)\n\nif are_connected:\n    print(\"Node 1023 and Node 9782 are connected.\")\nelse:\n    print(\"Node 1023 and Node 9782 are not connected.\")\n\nNode 1023 and Node 9782 are not connected.\n\n\n\nG.has_edge(1023,1024)\n\nTrue"
  },
  {
    "objectID": "posts/231020 데이터(19, df5.html#데이터정리",
    "href": "posts/231020 데이터(19, df5.html#데이터정리",
    "title": "[FRAUD] 데이터정리 시도(GCN_엣지손x)",
    "section": "",
    "text": "_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50 = df50.reset_index()\ndf50.shape\n\n(12012, 23)\n\n\n\n\n\n\nmask(df50)\n\n(array([False,  True,  True, ...,  True, False,  True]),\n array([ True, False, False, ..., False,  True, False]))\n\n\n\ntrain_mask, test_mask = mask(df50)\n\n\n\n\n\n\n# groups = df50.groupby('cc_num')\n# edge_index_list_plus = [compute_time_difference(group) for _, group in groups]\n# edge_index_list_plus_flat = [item for sublist in edge_index_list_plus for item in sublist]\n# edge_index_list_plus_nparr = np.array(edge_index_list_plus_flat)\n# np.save('edge_index_list_plus50.npy', edge_index_list_plus_nparr)\n\n\nedge_index = np.load('edge_index_list_plus50.npy').astype(np.float64)\nedge_index.shape\n\n(200706, 3)\n\n\n\nedge_index\n\narray([[1.023000e+03, 1.023000e+03, 0.000000e+00],\n       [1.023000e+03, 1.024000e+03, 4.200000e+12],\n       [1.023000e+03, 1.028000e+03, 7.764000e+13],\n       ...,\n       [1.194400e+04, 9.782000e+03, 9.528000e+13],\n       [1.194400e+04, 1.176700e+04, 3.055758e+16],\n       [1.194400e+04, 1.194400e+04, 0.000000e+00]])\n\n\n\ngroups = df50.groupby('cc_num')\ngroups\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f8740677af0&gt;\n\n\n\nimport itertools\n\nindex_pairs = []\n\nfor _, group_df in groups:\n    indices = group_df.index.tolist()\n    index_combinations = list(itertools.combinations(indices, 2))\n    index_pairs.extend(index_combinations)\n\n\nindex_pairs = torch.tensor(index_pairs).t()\n\n\nindex_pairs.shape\n\ntorch.Size([2, 94347])\n\n\n\nindex_pairs\n\ntensor([[ 1023,  1023,  1023,  ...,  9782,  9782, 11767],\n        [ 1024,  1028,  1031,  ..., 11767, 11944, 11944]])\n\n\n\n# edge_index_selected = edge_index_selected(edge_index)\n\n\n\n\n\n\nx = torch.tensor(df50['amt'], dtype=torch.float).reshape(-1,1)\ny = torch.tensor(df50['is_fraud'],dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index = index_pairs, y=y, train_mask = train_mask, test_mask = test_mask)\ndata\n\n\nData(x=[12012, 1], edge_index=[2, 94347], y=[12012], train_mask=[12012], test_mask=[12012])"
  },
  {
    "objectID": "posts/231020 데이터(19, df5.html#분석-1gcn-amt",
    "href": "posts/231020 데이터(19, df5.html#분석-1gcn-amt",
    "title": "[FRAUD] 데이터정리 시도(GCN_엣지손x)",
    "section": "",
    "text": "torch.manual_seed(202250926)\nclass GCN2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 32)\n        self.conv2 = GCNConv(32,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\nX = (data.x[data.train_mask]).numpy()\nXX = (data.x[data.test_mask]).numpy()\ny = (data.y[data.train_mask]).numpy()\nyy = (data.y[data.test_mask]).numpy()\n\n\nmodel = GCN2()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(400):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\nmodel.eval()\n\nGCN2(\n  (conv1): GCNConv(1, 32)\n  (conv2): GCNConv(32, 2)\n)\n\n\n\npred = model(data).argmax(dim=1)\n\n\nyyhat = pred[data.test_mask]\n\n\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results1= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석1'])\n_results1\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석1\n0.494838\n0.0\n0.0\n0.0\n\n\n\n\n\n\n\n\n\nb,W = list(model.conv1.parameters())\n\n\nb,W\n\n(Parameter containing:\n tensor([-2.6334e-02, -2.7689e-01, -1.6926e-02, -5.2382e-02, -1.2667e-01,\n         -4.2815e-02, -2.6620e-02,  1.1360e-01, -2.8240e-02, -2.1838e-01,\n          9.7086e-02, -1.2640e-02, -1.5231e-02, -1.3934e-03, -7.6017e-03,\n         -3.6946e-02, -5.4102e-10, -1.9268e-01,  1.7090e-07, -8.3471e-10,\n         -2.0092e-01, -7.3827e-02, -4.7944e-02, -1.9878e-01, -2.0639e-01,\n         -2.7078e-01, -5.2340e-03, -3.9832e-02,  3.9550e-02,  5.1139e-01,\n         -3.2245e-02,  2.2552e-02], requires_grad=True),\n Parameter containing:\n tensor([[ 0.1231],\n         [ 0.1348],\n         [-0.0504],\n         [ 0.0830],\n         [-0.0851],\n         [-0.0475],\n         [-0.0506],\n         [-0.0098],\n         [ 0.2435],\n         [ 0.1998],\n         [-0.0074],\n         [-0.0560],\n         [ 0.1415],\n         [-0.0649],\n         [-0.0586],\n         [-0.0573],\n         [-0.0653],\n         [ 0.0411],\n         [-0.0703],\n         [-0.0537],\n         [ 0.1198],\n         [-0.0690],\n         [-0.0654],\n         [ 0.2001],\n         [-0.0048],\n         [-0.0085],\n         [-0.0613],\n         [-0.0797],\n         [ 0.1049],\n         [-0.0017],\n         [-0.0547],\n         [ 0.1147]], requires_grad=True))\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# PyTorch Geometric 그래프를 NetworkX 그래프로 변환\nG = torch_geometric.utils.to_networkx(data, to_undirected=True)\n\n# 노드 1023과 연결된 모든 노드와 엣지를 포함하는 부분 그래프 추출\nsubgraph1 = nx.ego_graph(G, 1023, radius=1)\nsubgraph2 = nx.ego_graph(G, 9782, radius=1)\n\n# 왼쪽 그래프 레이아웃 설정\npos1 = nx.spring_layout(subgraph1, pos=None, seed=42)  # 그래프 레이아웃 설정\n\n# 오른쪽 그래프 레이아웃 설정\npos2 = nx.spring_layout(subgraph2, pos=None, seed=43)\n\n# 노드 그리기\nnx.draw_networkx_nodes(subgraph1, pos1, node_size=200, node_color='b', label='Node 1023')\nnx.draw_networkx_nodes(subgraph2, pos2, node_size=200, node_color='g', label='Node 9782')\n\n# 엣지 그리기\nnx.draw_networkx_edges(subgraph1, pos1)\nnx.draw_networkx_edges(subgraph2, pos2)\n\n# 노드 인덱스 표시\nlabels1 = {node: str(node) for node in subgraph1.nodes()}\nlabels2 = {node: str(node) for node in subgraph2.nodes()}\nnx.draw_networkx_labels(subgraph1, pos1, labels1, font_size=10)\nnx.draw_networkx_labels(subgraph2, pos2, labels2, font_size=10)\n\n# 그래프 출력\nplt.axis('off')  # 축 숨기기\n\n# 레이블 표시\nplt.legend(loc='best')\n\nplt.show()\n\n[](231020 데이터(19, df5_files/figure-html/cell-28-output-1.png){}\n\n\n\n# 노드 1023와 9782 간의 연결 확인\nare_connected = G.has_edge(1023, 9782)\n\nif are_connected:\n    print(\"Node 1023 and Node 9782 are connected.\")\nelse:\n    print(\"Node 1023 and Node 9782 are not connected.\")\n\nNode 1023 and Node 9782 are not connected.\n\n\n\nG.has_edge(1023,1024)\n\nTrue"
  },
  {
    "objectID": "posts/230816 fraud(3, df50_com, tr,test합치기).html",
    "href": "posts/230816 fraud(3, df50_com, tr,test합치기).html",
    "title": "[FRAUD] 데이터 정리 시도(8.18-망함 df50을 tr/test로 분리했다가 다시 합쳐봄..)",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\nimport torch\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics \n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부     \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\ndef build_graph_tripartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n                                                       df[\"cc_num\"].values.tolist() +\n                                                       df[\"merchant\"].values.tolist()))}\n    df[\"in_node\"]= df[\"cc_num\"].apply(lambda x: mapping[x])\n    df[\"out_node\"]=df[\"merchant\"].apply(lambda x:mapping[x])\n    \n        \n    G=nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\\\n                        [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")     \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")   \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")  \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n\n    return G\n    \n    \ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef embedding(Graph):\n    # Graph -&gt; X (feature)\n    _edgs = list(Graph.edges)\n    subGraph = Graph.edge_subgraph([_edgs[x] for x in range(len(Graph.edges))]).copy()\n    subGraph.add_nodes_from(list(set(Graph.nodes) - set(subGraph.nodes)))    \n    embedded = AverageEmbedder(Node2Vec(subGraph, weight_key='weight').fit(window=10).wv)\n    X = [embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in range(len(Graph.edges))]\n    # Graph -&gt; y (label)\n    y = np.array(list(nx.get_edge_attributes(Graph, \"label\").values()))\n    return X,y \n\ndef anal(df):\n    Graph = build_graph_bipartite(df)\n    X,XX,y,yy = embedding(Graph)\n    lrnr = RandomForestClassifier(n_estimators=100, random_state=42) \n    lrnr.fit(X,y)\n    yyhat = lrnr.predict(XX)\n    df = pd.DataFrame({\n        'acc':[sklearn.metrics.accuracy_score(yy,yyhat)], \n        'pre':[sklearn.metrics.precision_score(yy,yyhat)], \n        'rec':[sklearn.metrics.recall_score(yy,yyhat)],\n        'f1':[sklearn.metrics.f1_score(yy,yyhat)]}\n    )    \n    return df\n\ndef our_sampling1(df):\n    cus_list = set(df.query('is_fraud==1').cc_num.tolist())\n    return df.query(\"cc_num in @ cus_list\")\n\n\nfraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\n\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\n12012*12012\n\n144288144\n\n\n\n\n\ndf50 의 shape이 12000개 이므로 9000개의 T, 3000개의 F를 train mask로 만들자.\n고객정보가 동일하면 edge를 1로, 아니면 0으로 놓고 1에대한 weight를 만들자.\ng(V,E,W)에서의 weight\n\n\ndf50 = df50.reset_index()\n\n\nN = len(df50)\n\n\n\n\n\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.is_fraud.mean().round(5), df50_test.is_fraud.mean().round(5)\n\n(0.49828, 0.50516)\n\n\n\ndf50_tr.shape, df50_test.shape\n\n((9009, 23), (3003, 23))\n\n\n\ntrain_mask = np.concatenate((np.full(9009, True), np.full(3003, False)))\ntest_mask = np.concatenate((np.full(9009, False), np.full(3003, True)))\nprint(\"Train Mask:\", train_mask)\nprint(\"Test Mask:\", test_mask)\n\nTrain Mask: [ True  True  True ... False False False]\nTest Mask: [False False False ...  True  True  True]\n\n\n\ntrain_mask.shape, test_mask.shape\n\n((12012,), (12012,))\n\n\n\ntrain_mask.sum(), test_mask.sum()\n\n(9009, 3003)\n\n\n\ndf50_com = pd.concat([df50_tr, df50_test])\n\n\ndf50_com = df50_com.reset_index()\n\n\ndf50_com\n\n\n\n\n\n\n\n\nlevel_0\nindex\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n476\n51331\n2019-01-31 00:44:00\n3.543590e+15\nfraud_Medhurst PLC\nshopping_net\n921.24\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nc8928ba53be26fdd997b26f7130c757e\n1327970678\n40.064488\n-78.210499\n1\n\n\n1\n3671\n625691\n2019-09-23 00:09:00\n2.610530e+15\nfraud_Torphy-Goyette\nshopping_pos\n698.28\nTanya\nDickerson\nF\n...\n36.2416\n-86.6117\n22191\nPrison officer\n1994-07-27\n90453290b765904ed1c3426882a6788b\n1348358993\n35.884288\n-87.513318\n1\n\n\n2\n6641\n896244\n2019-12-25 21:30:00\n6.011330e+15\nfraud_Monahan-Morar\npersonal_care\n220.56\nLauren\nButler\nF\n...\n36.0557\n-96.0602\n413574\nTeacher, special educational needs\n1971-09-01\n4072a3effcf51cf7cf88f69d00642cd9\n1356471044\n35.789798\n-95.859736\n0\n\n\n3\n4288\n717690\n2019-11-02 22:22:00\n6.011380e+15\nfraud_Daugherty, Pouros and Beahan\nshopping_pos\n905.43\nMartin\nDuarte\nM\n...\n44.6001\n-84.2931\n864\nGeneral practice doctor\n1942-05-04\nf2fa1b25eef2f43fa5c09e3e1bfe7f77\n1351894926\n44.652759\n-84.500359\n1\n\n\n4\n4770\n815813\n2019-12-08 02:50:00\n4.430880e+15\nfraud_Hudson-Ratke\ngrocery_pos\n307.98\nAlicia\nMorales\nF\n...\n39.3199\n-106.6596\n61\nPublic relations account executive\n1939-11-04\nf06eff8da349e36e623cff026de8e970\n1354935056\n38.389399\n-106.111026\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n12007\n56\n12246\n2019-01-08 01:50:00\n4.859530e+15\nfraud_Howe Ltd\nmisc_pos\n664.73\nRebecca\nFarley\nF\n...\n42.0716\n-75.0152\n1228\nPsychotherapist, child\n1990-02-25\n61817f427fdb2a54d7c51595026021d2\n1325987429\n41.124211\n-75.954718\n1\n\n\n12008\n3194\n538327\n2019-08-18 23:37:00\n5.020130e+11\nfraud_Miller-Harris\nmisc_net\n844.60\nSherry\nMartinez\nF\n...\n42.6315\n-75.1866\n165\nNaval architect\n1945-09-20\n635ba4a5f582514e053e96bf3a4376ac\n1345333050\n42.207966\n-74.695138\n1\n\n\n12009\n2855\n459431\n2019-07-22 03:18:00\n4.917190e+15\nfraud_Corwin-Collins\ngas_transport\n17.97\nJoel\nRivera\nM\n...\n35.8759\n-96.9623\n1165\nPsychotherapist, child\n1944-11-11\nf9526787905f648773a69e1f97faa017\n1342927100\n34.880538\n-96.384044\n1\n\n\n12010\n10690\n720826\n2019-11-03 21:51:00\n4.997730e+15\nfraud_Hagenes, Hermann and Stroman\ntravel\n7.58\nStephanie\nTaylor\nF\n...\n44.9913\n-92.9487\n753116\nFisheries officer\n1971-08-06\n3cd0cc36fa115887dba94c1d5b3fb2df\n1351979471\n44.177391\n-92.998310\n0\n\n\n12011\n2986\n489045\n2019-08-02 01:42:00\n2.450830e+15\nfraud_Herman, Treutel and Dickens\nmisc_net\n824.99\nTimothy\nKirby\nM\n...\n45.6040\n-94.1591\n16163\nHydrographic surveyor\n1987-02-22\n5832beb3af071da9ddd41d9ff8f7a5a1\n1343871750\n44.785690\n-93.624590\n1\n\n\n\n\n12012 rows × 24 columns\n\n\n\n\n\n\n\n\n# edge_index_list = []\n# for i in range(N):\n#     for j in range(N):\n#         time_difference = (df50['trans_date_trans_time'][i] - df50['trans_date_trans_time'][j]).total_seconds()\n#         edge_index_list.append([i, j, time_difference])\n\n\n# edge_index_list[:5]\n\n[[0, 0, 0.0],\n [0, 1, -2460.0],\n [0, 2, -7140.0],\n [0, 3, -9120.0],\n [0, 4, -10140.0]]\n\n\n\n# np.save('edge_index_list_50.npy', edge_index_list)\n\n# loaded_data = np.load('edge_index_list_50.npy')\n\n\n# edge_index = np.array(edge_index_list)\n# edge_index[:,2] = np.abs(edge_index[:,2])\n# theta = edge_index[:,2].mean()\n# theta\n\n12238996.895508753\n\n\n\n# edge_index[:,2] = (np.exp(-edge_index[:,2]/theta)!=1) * np.exp(-edge_index[:,2]/theta)\n# edge_index\n\narray([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 1.00000000e+00, 9.99799023e-01],\n       [0.00000000e+00, 2.00000000e+00, 9.99416789e-01],\n       ...,\n       [1.20110000e+04, 1.20090000e+04, 4.19756312e-01],\n       [1.20110000e+04, 1.20100000e+04, 2.26811434e-01],\n       [1.20110000e+04, 1.20110000e+04, 0.00000000e+00]])\n\n\n\n# edge_index[:,2]\n\narray([0.        , 0.99979902, 0.99941679, ..., 0.41975631, 0.22681143,\n       0.        ])\n\n\nQ. 그런데 밑에서 random으로 train하고 test로 나누게 되면.. wieght랑 edge를 어떻게 적용시키지?\n\n\n\n\nedge_index_list2_com = []\nfor i in range(N):\n    for j in range(N):\n        if df50_com['cc_num'][i] != df50_com['cc_num'][j]:  \n            edge = 0\n        else:\n            edge = 1\n        edge_index_list2_com.append([i, j, edge])\n\n\nnp.save('edge_index_list2_50_com.npy', edge_index_list2_com)\n\nloaded_data = np.load('edge_index_list2_50_com.npy')\n\n\nedge_index_list2_com[:5]\n\n[[0, 0, 1], [0, 1, 0], [0, 2, 0], [0, 3, 0], [0, 4, 0]]\n\n\n\nedge_one_com = [(i, j) for i, j, edge in edge_index_list2_com if edge == 1]\nedge_one_com[:5]\n\n[(0, 0), (0, 344), (0, 1377), (0, 1447), (0, 1639)]\n\n\n\nlen(edge_one_com)\n\n200706\n\n\n\nedge_one_index_com = torch.tensor(edge_one_com, dtype=torch.long).t()\n\n\nedge_one_index_com.shape\n\ntorch.Size([2, 200706])\n\n\n\n\n\n\n\nx = df50_com['amt']\n\n\na = torch.tensor(x, dtype=torch.float)\n\n\na = a.reshape(-1,1)\na\n\ntensor([[921.2400],\n        [698.2800],\n        [220.5600],\n        ...,\n        [ 17.9700],\n        [  7.5800],\n        [824.9900]])\n\n\n\ny = df50_com['is_fraud']\n\n\nb = torch.tensor(y,dtype=torch.int64)\n\n\nb\n\ntensor([1, 1, 0,  ..., 1, 0, 1])\n\n\n\nimport torch_geometric\n\n\ndata = torch_geometric.data.Data(x=a, edge_index = edge_one_index_com, y=b, train_mask = train_mask, test_mask = test_mask)\n\n\ndata\n\nData(x=[12012, 1], edge_index=[2, 200706], y=[12012], train_mask=[12012], test_mask=[12012])\n\n\n\n\n\n\n\n\n\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\n\nclass GCN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 16)\n        self.conv2 = GCNConv(16,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\n\nmodel = GCN()\n\n\nmodel\n\nGCN(\n  (conv1): GCNConv(1, 16)\n  (conv2): GCNConv(16, 2)\n)\n\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\n\nGCN(\n  (conv1): GCNConv(1, 16)\n  (conv2): GCNConv(16, 2)\n)\n\n\n\n\nfor epoch in range(200):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\n\n\nmodel.eval()\npred = model(data).argmax(dim=1)\ncorrect = (pred[data.test_mask] == data.y[data.test_mask]).sum()\nacc = int(correct) / int(data.test_mask.sum())\nprint(f'Accuracy: {acc:.4f}')\n\nAccuracy: 0.5012\n\n\n\n왜 자꾸 accuracy가 안좋게 나오노\n\n\ndata.y[data.test_mask].sum()\n\ntensor(1517)\n\n\n\nout[data.train_mask]\n\ntensor([[-1.0039, -0.4564],\n        [-1.6018, -0.2251],\n        [-0.3896, -1.1311],\n        ...,\n        [-1.7755, -0.1856],\n        [-0.9515, -0.4880],\n        [-2.3393, -0.1014]], grad_fn=&lt;IndexBackward0&gt;)\n\n\n\ndata.y[data.test_mask]\n\ntensor([0, 1, 0,  ..., 1, 0, 1])\n\n\n\n음……………. edge_list를 다시 해보자.\n\n\n!!!! 첫번째 시도에서는 edge_list를 무작정 1인걸 고른것이 아니였는데 왜 여기서는 이러고 있엇니…………….."
  },
  {
    "objectID": "posts/230816 fraud(3, df50_com, tr,test합치기).html#시도",
    "href": "posts/230816 fraud(3, df50_com, tr,test합치기).html#시도",
    "title": "[FRAUD] 데이터 정리 시도(8.18-망함 df50을 tr/test로 분리했다가 다시 합쳐봄..)",
    "section": "",
    "text": "_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\n12012*12012\n\n144288144\n\n\n\n\n\ndf50 의 shape이 12000개 이므로 9000개의 T, 3000개의 F를 train mask로 만들자.\n고객정보가 동일하면 edge를 1로, 아니면 0으로 놓고 1에대한 weight를 만들자.\ng(V,E,W)에서의 weight\n\n\ndf50 = df50.reset_index()\n\n\nN = len(df50)\n\n\n\n\n\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.is_fraud.mean().round(5), df50_test.is_fraud.mean().round(5)\n\n(0.49828, 0.50516)\n\n\n\ndf50_tr.shape, df50_test.shape\n\n((9009, 23), (3003, 23))\n\n\n\ntrain_mask = np.concatenate((np.full(9009, True), np.full(3003, False)))\ntest_mask = np.concatenate((np.full(9009, False), np.full(3003, True)))\nprint(\"Train Mask:\", train_mask)\nprint(\"Test Mask:\", test_mask)\n\nTrain Mask: [ True  True  True ... False False False]\nTest Mask: [False False False ...  True  True  True]\n\n\n\ntrain_mask.shape, test_mask.shape\n\n((12012,), (12012,))\n\n\n\ntrain_mask.sum(), test_mask.sum()\n\n(9009, 3003)\n\n\n\ndf50_com = pd.concat([df50_tr, df50_test])\n\n\ndf50_com = df50_com.reset_index()\n\n\ndf50_com\n\n\n\n\n\n\n\n\nlevel_0\nindex\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n476\n51331\n2019-01-31 00:44:00\n3.543590e+15\nfraud_Medhurst PLC\nshopping_net\n921.24\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nc8928ba53be26fdd997b26f7130c757e\n1327970678\n40.064488\n-78.210499\n1\n\n\n1\n3671\n625691\n2019-09-23 00:09:00\n2.610530e+15\nfraud_Torphy-Goyette\nshopping_pos\n698.28\nTanya\nDickerson\nF\n...\n36.2416\n-86.6117\n22191\nPrison officer\n1994-07-27\n90453290b765904ed1c3426882a6788b\n1348358993\n35.884288\n-87.513318\n1\n\n\n2\n6641\n896244\n2019-12-25 21:30:00\n6.011330e+15\nfraud_Monahan-Morar\npersonal_care\n220.56\nLauren\nButler\nF\n...\n36.0557\n-96.0602\n413574\nTeacher, special educational needs\n1971-09-01\n4072a3effcf51cf7cf88f69d00642cd9\n1356471044\n35.789798\n-95.859736\n0\n\n\n3\n4288\n717690\n2019-11-02 22:22:00\n6.011380e+15\nfraud_Daugherty, Pouros and Beahan\nshopping_pos\n905.43\nMartin\nDuarte\nM\n...\n44.6001\n-84.2931\n864\nGeneral practice doctor\n1942-05-04\nf2fa1b25eef2f43fa5c09e3e1bfe7f77\n1351894926\n44.652759\n-84.500359\n1\n\n\n4\n4770\n815813\n2019-12-08 02:50:00\n4.430880e+15\nfraud_Hudson-Ratke\ngrocery_pos\n307.98\nAlicia\nMorales\nF\n...\n39.3199\n-106.6596\n61\nPublic relations account executive\n1939-11-04\nf06eff8da349e36e623cff026de8e970\n1354935056\n38.389399\n-106.111026\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n12007\n56\n12246\n2019-01-08 01:50:00\n4.859530e+15\nfraud_Howe Ltd\nmisc_pos\n664.73\nRebecca\nFarley\nF\n...\n42.0716\n-75.0152\n1228\nPsychotherapist, child\n1990-02-25\n61817f427fdb2a54d7c51595026021d2\n1325987429\n41.124211\n-75.954718\n1\n\n\n12008\n3194\n538327\n2019-08-18 23:37:00\n5.020130e+11\nfraud_Miller-Harris\nmisc_net\n844.60\nSherry\nMartinez\nF\n...\n42.6315\n-75.1866\n165\nNaval architect\n1945-09-20\n635ba4a5f582514e053e96bf3a4376ac\n1345333050\n42.207966\n-74.695138\n1\n\n\n12009\n2855\n459431\n2019-07-22 03:18:00\n4.917190e+15\nfraud_Corwin-Collins\ngas_transport\n17.97\nJoel\nRivera\nM\n...\n35.8759\n-96.9623\n1165\nPsychotherapist, child\n1944-11-11\nf9526787905f648773a69e1f97faa017\n1342927100\n34.880538\n-96.384044\n1\n\n\n12010\n10690\n720826\n2019-11-03 21:51:00\n4.997730e+15\nfraud_Hagenes, Hermann and Stroman\ntravel\n7.58\nStephanie\nTaylor\nF\n...\n44.9913\n-92.9487\n753116\nFisheries officer\n1971-08-06\n3cd0cc36fa115887dba94c1d5b3fb2df\n1351979471\n44.177391\n-92.998310\n0\n\n\n12011\n2986\n489045\n2019-08-02 01:42:00\n2.450830e+15\nfraud_Herman, Treutel and Dickens\nmisc_net\n824.99\nTimothy\nKirby\nM\n...\n45.6040\n-94.1591\n16163\nHydrographic surveyor\n1987-02-22\n5832beb3af071da9ddd41d9ff8f7a5a1\n1343871750\n44.785690\n-93.624590\n1\n\n\n\n\n12012 rows × 24 columns\n\n\n\n\n\n\n\n\n# edge_index_list = []\n# for i in range(N):\n#     for j in range(N):\n#         time_difference = (df50['trans_date_trans_time'][i] - df50['trans_date_trans_time'][j]).total_seconds()\n#         edge_index_list.append([i, j, time_difference])\n\n\n# edge_index_list[:5]\n\n[[0, 0, 0.0],\n [0, 1, -2460.0],\n [0, 2, -7140.0],\n [0, 3, -9120.0],\n [0, 4, -10140.0]]\n\n\n\n# np.save('edge_index_list_50.npy', edge_index_list)\n\n# loaded_data = np.load('edge_index_list_50.npy')\n\n\n# edge_index = np.array(edge_index_list)\n# edge_index[:,2] = np.abs(edge_index[:,2])\n# theta = edge_index[:,2].mean()\n# theta\n\n12238996.895508753\n\n\n\n# edge_index[:,2] = (np.exp(-edge_index[:,2]/theta)!=1) * np.exp(-edge_index[:,2]/theta)\n# edge_index\n\narray([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 1.00000000e+00, 9.99799023e-01],\n       [0.00000000e+00, 2.00000000e+00, 9.99416789e-01],\n       ...,\n       [1.20110000e+04, 1.20090000e+04, 4.19756312e-01],\n       [1.20110000e+04, 1.20100000e+04, 2.26811434e-01],\n       [1.20110000e+04, 1.20110000e+04, 0.00000000e+00]])\n\n\n\n# edge_index[:,2]\n\narray([0.        , 0.99979902, 0.99941679, ..., 0.41975631, 0.22681143,\n       0.        ])\n\n\nQ. 그런데 밑에서 random으로 train하고 test로 나누게 되면.. wieght랑 edge를 어떻게 적용시키지?\n\n\n\n\nedge_index_list2_com = []\nfor i in range(N):\n    for j in range(N):\n        if df50_com['cc_num'][i] != df50_com['cc_num'][j]:  \n            edge = 0\n        else:\n            edge = 1\n        edge_index_list2_com.append([i, j, edge])\n\n\nnp.save('edge_index_list2_50_com.npy', edge_index_list2_com)\n\nloaded_data = np.load('edge_index_list2_50_com.npy')\n\n\nedge_index_list2_com[:5]\n\n[[0, 0, 1], [0, 1, 0], [0, 2, 0], [0, 3, 0], [0, 4, 0]]\n\n\n\nedge_one_com = [(i, j) for i, j, edge in edge_index_list2_com if edge == 1]\nedge_one_com[:5]\n\n[(0, 0), (0, 344), (0, 1377), (0, 1447), (0, 1639)]\n\n\n\nlen(edge_one_com)\n\n200706\n\n\n\nedge_one_index_com = torch.tensor(edge_one_com, dtype=torch.long).t()\n\n\nedge_one_index_com.shape\n\ntorch.Size([2, 200706])\n\n\n\n\n\n\n\nx = df50_com['amt']\n\n\na = torch.tensor(x, dtype=torch.float)\n\n\na = a.reshape(-1,1)\na\n\ntensor([[921.2400],\n        [698.2800],\n        [220.5600],\n        ...,\n        [ 17.9700],\n        [  7.5800],\n        [824.9900]])\n\n\n\ny = df50_com['is_fraud']\n\n\nb = torch.tensor(y,dtype=torch.int64)\n\n\nb\n\ntensor([1, 1, 0,  ..., 1, 0, 1])\n\n\n\nimport torch_geometric\n\n\ndata = torch_geometric.data.Data(x=a, edge_index = edge_one_index_com, y=b, train_mask = train_mask, test_mask = test_mask)\n\n\ndata\n\nData(x=[12012, 1], edge_index=[2, 200706], y=[12012], train_mask=[12012], test_mask=[12012])\n\n\n\n\n\n\n\n\n\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\n\nclass GCN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 16)\n        self.conv2 = GCNConv(16,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\n\nmodel = GCN()\n\n\nmodel\n\nGCN(\n  (conv1): GCNConv(1, 16)\n  (conv2): GCNConv(16, 2)\n)\n\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\n\nGCN(\n  (conv1): GCNConv(1, 16)\n  (conv2): GCNConv(16, 2)\n)\n\n\n\n\nfor epoch in range(200):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\n\n\nmodel.eval()\npred = model(data).argmax(dim=1)\ncorrect = (pred[data.test_mask] == data.y[data.test_mask]).sum()\nacc = int(correct) / int(data.test_mask.sum())\nprint(f'Accuracy: {acc:.4f}')\n\nAccuracy: 0.5012\n\n\n\n왜 자꾸 accuracy가 안좋게 나오노\n\n\ndata.y[data.test_mask].sum()\n\ntensor(1517)\n\n\n\nout[data.train_mask]\n\ntensor([[-1.0039, -0.4564],\n        [-1.6018, -0.2251],\n        [-0.3896, -1.1311],\n        ...,\n        [-1.7755, -0.1856],\n        [-0.9515, -0.4880],\n        [-2.3393, -0.1014]], grad_fn=&lt;IndexBackward0&gt;)\n\n\n\ndata.y[data.test_mask]\n\ntensor([0, 1, 0,  ..., 1, 0, 1])\n\n\n\n음……………. edge_list를 다시 해보자.\n\n\n!!!! 첫번째 시도에서는 edge_list를 무작정 1인걸 고른것이 아니였는데 왜 여기서는 이러고 있엇니…………….."
  },
  {
    "objectID": "posts/231011 데이터(17, 15버넊에서df50 auto_best).html",
    "href": "posts/231011 데이터(17, 15버넊에서df50 auto_best).html",
    "title": "[FRAUD] 데이터정리 시도(df50 X범주_auto_best)",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\nimport xgboost as xgb\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# gnn\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric\nfrom torch_geometric.nn import GCNConv\n\n\n# autogluon\nfrom autogluon.tabular import TabularDataset, TabularPredictor\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_geometric/typing.py:18: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/libpyg.so: undefined symbol: _ZN2at4_ops12split_Tensor4callERKNS_6TensorEN3c106SymIntEl\n  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_geometric/typing.py:31: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_scatter/_scatter_cuda.so: undefined symbol: _ZNK3c107SymBool10guard_boolEPKcl\n  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_geometric/typing.py:42: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_sparse/_diag_cuda.so: undefined symbol: _ZN3c106detail19maybe_wrap_dim_slowIlEET_S2_S2_b\n  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n\n\n\ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef compute_time_difference(group):\n    n = len(group)\n    result = []\n    for i in range(n):\n        for j in range(n):\n            time_difference = abs(group.iloc[i].trans_date_trans_time.value - group.iloc[j].trans_date_trans_time.value)\n            result.append([group.iloc[i].name, group.iloc[j].name, time_difference])\n    return result\n\n\nclass GCN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 16)\n        self.conv2 = GCNConv(16,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\n\nfraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\n\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\ndf50 = df50.reset_index()\n\n\nN = len(df50)"
  },
  {
    "objectID": "posts/231011 데이터(17, 15버넊에서df50 auto_best).html#데이터정리",
    "href": "posts/231011 데이터(17, 15버넊에서df50 auto_best).html#데이터정리",
    "title": "[FRAUD] 데이터정리 시도(df50 X범주_auto_best)",
    "section": "",
    "text": "_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\ndf50 = df50.reset_index()\n\n\nN = len(df50)"
  },
  {
    "objectID": "posts/231011 데이터(17, 15버넊에서df50 auto_best).html#a.-데이터",
    "href": "posts/231011 데이터(17, 15버넊에서df50 auto_best).html#a.-데이터",
    "title": "[FRAUD] 데이터정리 시도(df50 X범주_auto_best)",
    "section": "A. 데이터",
    "text": "A. 데이터\n\ntr = TabularDataset(df50_tr)\ntst = TabularDataset(df50_test)"
  },
  {
    "objectID": "posts/231011 데이터(17, 15버넊에서df50 auto_best).html#b.-predictor-생성",
    "href": "posts/231011 데이터(17, 15버넊에서df50 auto_best).html#b.-predictor-생성",
    "title": "[FRAUD] 데이터정리 시도(df50 X범주_auto_best)",
    "section": "B. predictor 생성",
    "text": "B. predictor 생성\n\npredictr = TabularPredictor(\"is_fraud\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231011_125519/\""
  },
  {
    "objectID": "posts/231011 데이터(17, 15버넊에서df50 auto_best).html#c.적합fit",
    "href": "posts/231011 데이터(17, 15버넊에서df50 auto_best).html#c.적합fit",
    "title": "[FRAUD] 데이터정리 시도(df50 X범주_auto_best)",
    "section": "C.적합(fit)",
    "text": "C.적합(fit)\n\npredictr.fit(tr, presets='best_quality')\n\nPresets specified: ['best_quality']\nStack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=1\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20231011_125519/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.8.18\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   746.18 GB / 982.82 GB (75.9%)\nTrain Data Rows:    9009\nTrain Data Columns: 1\nLabel Column: is_fraud\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [1, 0]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    12696.17 MB\n    Train Data (Original)  Memory Usage: 0.07 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', []) : 1 | ['amt']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', []) : 1 | ['amt']\n    0.0s = Fit runtime\n    1 features in original data used to generate 1 features in processed data.\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.02s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif_BAG_L1 ...\n    0.8782   = Validation score   (accuracy)\n    0.0s     = Training   runtime\n    0.01s    = Validation runtime\nFitting model: KNeighborsDist_BAG_L1 ...\n    0.8641   = Validation score   (accuracy)\n    0.0s     = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMXT_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.885    = Validation score   (accuracy)\n    0.5s     = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.894    = Validation score   (accuracy)\n    0.64s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: RandomForestGini_BAG_L1 ...\n    0.856    = Validation score   (accuracy)\n    0.35s    = Training   runtime\n    0.19s    = Validation runtime\nFitting model: RandomForestEntr_BAG_L1 ...\n    0.856    = Validation score   (accuracy)\n    0.6s     = Training   runtime\n    0.19s    = Validation runtime\nFitting model: CatBoost_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8947   = Validation score   (accuracy)\n    1.56s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesGini_BAG_L1 ...\n    0.8622   = Validation score   (accuracy)\n    0.33s    = Training   runtime\n    0.21s    = Validation runtime\nFitting model: ExtraTreesEntr_BAG_L1 ...\n    0.8626   = Validation score   (accuracy)\n    0.31s    = Training   runtime\n    0.25s    = Validation runtime\nFitting model: NeuralNetFastAI_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.867    = Validation score   (accuracy)\n    9.1s     = Training   runtime\n    0.17s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8944   = Validation score   (accuracy)\n    0.57s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: NeuralNetTorch_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8888   = Validation score   (accuracy)\n    18.41s   = Training   runtime\n    0.09s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8941   = Validation score   (accuracy)\n    0.85s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8948   = Validation score   (accuracy)\n    2.26s    = Training   runtime\n    0.01s    = Validation runtime\nAutoGluon training complete, total runtime = 45.37s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231011_125519/\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7fcc0a6335b0&gt;\n\n\n\npredictr.leaderboard()\n\n                      model  score_val  pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0       WeightedEnsemble_L2   0.894772       0.020864   4.666441                0.009758           2.256641            2       True         14\n1           CatBoost_BAG_L1   0.894661       0.004361   1.564344                0.004361           1.564344            1       True          7\n2            XGBoost_BAG_L1   0.894439       0.022174   0.569362                0.022174           0.569362            1       True         11\n3      LightGBMLarge_BAG_L1   0.894106       0.006745   0.845457                0.006745           0.845457            1       True         13\n4           LightGBM_BAG_L1   0.893995       0.016963   0.637937                0.016963           0.637937            1       True          4\n5     NeuralNetTorch_BAG_L1   0.888778       0.089444  18.408010                0.089444          18.408010            1       True         12\n6         LightGBMXT_BAG_L1   0.885004       0.033841   0.504977                0.033841           0.504977            1       True          3\n7     KNeighborsUnif_BAG_L1   0.878233       0.009831   0.002898                0.009831           0.002898            1       True          1\n8    NeuralNetFastAI_BAG_L1   0.867022       0.171568   9.103081                0.171568           9.103081            1       True         10\n9     KNeighborsDist_BAG_L1   0.864136       0.008431   0.003336                0.008431           0.003336            1       True          2\n10    ExtraTreesEntr_BAG_L1   0.862582       0.253638   0.310474                0.253638           0.310474            1       True          9\n11    ExtraTreesGini_BAG_L1   0.862249       0.210405   0.327562                0.210405           0.327562            1       True          8\n12  RandomForestEntr_BAG_L1   0.856033       0.189045   0.599637                0.189045           0.599637            1       True          6\n13  RandomForestGini_BAG_L1   0.856033       0.191019   0.349150                0.191019           0.349150            1       True          5\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nWeightedEnsemble_L2\n0.894772\n0.020864\n4.666441\n0.009758\n2.256641\n2\nTrue\n14\n\n\n1\nCatBoost_BAG_L1\n0.894661\n0.004361\n1.564344\n0.004361\n1.564344\n1\nTrue\n7\n\n\n2\nXGBoost_BAG_L1\n0.894439\n0.022174\n0.569362\n0.022174\n0.569362\n1\nTrue\n11\n\n\n3\nLightGBMLarge_BAG_L1\n0.894106\n0.006745\n0.845457\n0.006745\n0.845457\n1\nTrue\n13\n\n\n4\nLightGBM_BAG_L1\n0.893995\n0.016963\n0.637937\n0.016963\n0.637937\n1\nTrue\n4\n\n\n5\nNeuralNetTorch_BAG_L1\n0.888778\n0.089444\n18.408010\n0.089444\n18.408010\n1\nTrue\n12\n\n\n6\nLightGBMXT_BAG_L1\n0.885004\n0.033841\n0.504977\n0.033841\n0.504977\n1\nTrue\n3\n\n\n7\nKNeighborsUnif_BAG_L1\n0.878233\n0.009831\n0.002898\n0.009831\n0.002898\n1\nTrue\n1\n\n\n8\nNeuralNetFastAI_BAG_L1\n0.867022\n0.171568\n9.103081\n0.171568\n9.103081\n1\nTrue\n10\n\n\n9\nKNeighborsDist_BAG_L1\n0.864136\n0.008431\n0.003336\n0.008431\n0.003336\n1\nTrue\n2\n\n\n10\nExtraTreesEntr_BAG_L1\n0.862582\n0.253638\n0.310474\n0.253638\n0.310474\n1\nTrue\n9\n\n\n11\nExtraTreesGini_BAG_L1\n0.862249\n0.210405\n0.327562\n0.210405\n0.327562\n1\nTrue\n8\n\n\n12\nRandomForestEntr_BAG_L1\n0.856033\n0.189045\n0.599637\n0.189045\n0.599637\n1\nTrue\n6\n\n\n13\nRandomForestGini_BAG_L1\n0.856033\n0.191019\n0.349150\n0.191019\n0.349150\n1\nTrue\n5"
  },
  {
    "objectID": "posts/231011 데이터(17, 15버넊에서df50 auto_best).html#a.-데이터-1",
    "href": "posts/231011 데이터(17, 15버넊에서df50 auto_best).html#a.-데이터-1",
    "title": "[FRAUD] 데이터정리 시도(df50 X범주_auto_best)",
    "section": "A. 데이터",
    "text": "A. 데이터\n\ntr = TabularDataset(df50_tr)\ntst = TabularDataset(df50_test)"
  },
  {
    "objectID": "posts/231011 데이터(17, 15버넊에서df50 auto_best).html#b.-predictor-생성-1",
    "href": "posts/231011 데이터(17, 15버넊에서df50 auto_best).html#b.-predictor-생성-1",
    "title": "[FRAUD] 데이터정리 시도(df50 X범주_auto_best)",
    "section": "B. predictor 생성",
    "text": "B. predictor 생성\n\npredictr = TabularPredictor(\"is_fraud\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231011_125738/\""
  },
  {
    "objectID": "posts/231011 데이터(17, 15버넊에서df50 auto_best).html#c.적합fit-1",
    "href": "posts/231011 데이터(17, 15버넊에서df50 auto_best).html#c.적합fit-1",
    "title": "[FRAUD] 데이터정리 시도(df50 X범주_auto_best)",
    "section": "C.적합(fit)",
    "text": "C.적합(fit)\n\npredictr.fit(tr, presets='best_quality')\n\nPresets specified: ['best_quality']\nStack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=1\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20231011_125738/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.8.18\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   738.25 GB / 982.82 GB (75.1%)\nTrain Data Rows:    9009\nTrain Data Columns: 2\nLabel Column: is_fraud\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [1, 0]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    10891.21 MB\n    Train Data (Original)  Memory Usage: 0.14 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', []) : 2 | ['amt', 'distance_km']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', []) : 2 | ['amt', 'distance_km']\n    0.0s = Fit runtime\n    2 features in original data used to generate 2 features in processed data.\n    Train Data (Processed) Memory Usage: 0.14 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.03s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif_BAG_L1 ...\n    0.8779   = Validation score   (accuracy)\n    0.0s     = Training   runtime\n    0.02s    = Validation runtime\nFitting model: KNeighborsDist_BAG_L1 ...\n    0.8716   = Validation score   (accuracy)\n    0.0s     = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMXT_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8898   = Validation score   (accuracy)\n    0.77s    = Training   runtime\n    0.15s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.894    = Validation score   (accuracy)\n    0.66s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: RandomForestGini_BAG_L1 ...\n    0.8788   = Validation score   (accuracy)\n    0.41s    = Training   runtime\n    0.21s    = Validation runtime\nFitting model: RandomForestEntr_BAG_L1 ...\n    0.8793   = Validation score   (accuracy)\n    0.58s    = Training   runtime\n    0.23s    = Validation runtime\nFitting model: CatBoost_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8948   = Validation score   (accuracy)\n    2.02s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: ExtraTreesGini_BAG_L1 ...\n    0.8771   = Validation score   (accuracy)\n    0.31s    = Training   runtime\n    0.23s    = Validation runtime\nFitting model: ExtraTreesEntr_BAG_L1 ...\n    0.8766   = Validation score   (accuracy)\n    0.38s    = Training   runtime\n    0.23s    = Validation runtime\nFitting model: NeuralNetFastAI_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8671   = Validation score   (accuracy)\n    10.03s   = Training   runtime\n    0.11s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8955   = Validation score   (accuracy)\n    0.72s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: NeuralNetTorch_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8869   = Validation score   (accuracy)\n    14.71s   = Training   runtime\n    0.07s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8866   = Validation score   (accuracy)\n    0.94s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.8955   = Validation score   (accuracy)\n    2.32s    = Training   runtime\n    0.01s    = Validation runtime\nAutoGluon training complete, total runtime = 42.82s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231011_125738/\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7fcb15583dc0&gt;\n\n\n\npredictr.leaderboard()\n\n                      model  score_val  pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0            XGBoost_BAG_L1   0.895549       0.026780   0.716487                0.026780           0.716487            1       True         11\n1       WeightedEnsemble_L2   0.895549       0.036584   3.040059                0.009804           2.323572            2       True         14\n2           CatBoost_BAG_L1   0.894772       0.007168   2.017300                0.007168           2.017300            1       True          7\n3           LightGBM_BAG_L1   0.893995       0.009826   0.656309                0.009826           0.656309            1       True          4\n4         LightGBMXT_BAG_L1   0.889777       0.153301   0.774388                0.153301           0.774388            1       True          3\n5     NeuralNetTorch_BAG_L1   0.886891       0.067880  14.708995                0.067880          14.708995            1       True         12\n6      LightGBMLarge_BAG_L1   0.886558       0.011474   0.938887                0.011474           0.938887            1       True         13\n7   RandomForestEntr_BAG_L1   0.879343       0.227656   0.576854                0.227656           0.576854            1       True          6\n8   RandomForestGini_BAG_L1   0.878788       0.210062   0.409630                0.210062           0.409630            1       True          5\n9     KNeighborsUnif_BAG_L1   0.877900       0.019640   0.004492                0.019640           0.004492            1       True          1\n10    ExtraTreesGini_BAG_L1   0.877123       0.230533   0.311612                0.230533           0.311612            1       True          8\n11    ExtraTreesEntr_BAG_L1   0.876568       0.228856   0.381866                0.228856           0.381866            1       True          9\n12    KNeighborsDist_BAG_L1   0.871573       0.010582   0.003571                0.010582           0.003571            1       True          2\n13   NeuralNetFastAI_BAG_L1   0.867133       0.106724  10.029284                0.106724          10.029284            1       True         10\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nXGBoost_BAG_L1\n0.895549\n0.026780\n0.716487\n0.026780\n0.716487\n1\nTrue\n11\n\n\n1\nWeightedEnsemble_L2\n0.895549\n0.036584\n3.040059\n0.009804\n2.323572\n2\nTrue\n14\n\n\n2\nCatBoost_BAG_L1\n0.894772\n0.007168\n2.017300\n0.007168\n2.017300\n1\nTrue\n7\n\n\n3\nLightGBM_BAG_L1\n0.893995\n0.009826\n0.656309\n0.009826\n0.656309\n1\nTrue\n4\n\n\n4\nLightGBMXT_BAG_L1\n0.889777\n0.153301\n0.774388\n0.153301\n0.774388\n1\nTrue\n3\n\n\n5\nNeuralNetTorch_BAG_L1\n0.886891\n0.067880\n14.708995\n0.067880\n14.708995\n1\nTrue\n12\n\n\n6\nLightGBMLarge_BAG_L1\n0.886558\n0.011474\n0.938887\n0.011474\n0.938887\n1\nTrue\n13\n\n\n7\nRandomForestEntr_BAG_L1\n0.879343\n0.227656\n0.576854\n0.227656\n0.576854\n1\nTrue\n6\n\n\n8\nRandomForestGini_BAG_L1\n0.878788\n0.210062\n0.409630\n0.210062\n0.409630\n1\nTrue\n5\n\n\n9\nKNeighborsUnif_BAG_L1\n0.877900\n0.019640\n0.004492\n0.019640\n0.004492\n1\nTrue\n1\n\n\n10\nExtraTreesGini_BAG_L1\n0.877123\n0.230533\n0.311612\n0.230533\n0.311612\n1\nTrue\n8\n\n\n11\nExtraTreesEntr_BAG_L1\n0.876568\n0.228856\n0.381866\n0.228856\n0.381866\n1\nTrue\n9\n\n\n12\nKNeighborsDist_BAG_L1\n0.871573\n0.010582\n0.003571\n0.010582\n0.003571\n1\nTrue\n2\n\n\n13\nNeuralNetFastAI_BAG_L1\n0.867133\n0.106724\n10.029284\n0.106724\n10.029284\n1\nTrue\n10"
  },
  {
    "objectID": "posts/231011 데이터(17, 15버넊에서df50 auto_best).html#a.-데이터-2",
    "href": "posts/231011 데이터(17, 15버넊에서df50 auto_best).html#a.-데이터-2",
    "title": "[FRAUD] 데이터정리 시도(df50 X범주_auto_best)",
    "section": "A. 데이터",
    "text": "A. 데이터\n\ntr = TabularDataset(df50_tr)\ntst = TabularDataset(df50_test)"
  },
  {
    "objectID": "posts/231011 데이터(17, 15버넊에서df50 auto_best).html#b.-predictor-생성-2",
    "href": "posts/231011 데이터(17, 15버넊에서df50 auto_best).html#b.-predictor-생성-2",
    "title": "[FRAUD] 데이터정리 시도(df50 X범주_auto_best)",
    "section": "B. predictor 생성",
    "text": "B. predictor 생성\n\npredictr = TabularPredictor(\"is_fraud\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231011_125833/\""
  },
  {
    "objectID": "posts/231011 데이터(17, 15버넊에서df50 auto_best).html#c.적합fit-2",
    "href": "posts/231011 데이터(17, 15버넊에서df50 auto_best).html#c.적합fit-2",
    "title": "[FRAUD] 데이터정리 시도(df50 X범주_auto_best)",
    "section": "C.적합(fit)",
    "text": "C.적합(fit)\n\npredictr.fit(tr, presets='best_quality')\n\nPresets specified: ['best_quality']\nStack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=1\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20231011_125833/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.8.18\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   738.82 GB / 982.82 GB (75.2%)\nTrain Data Rows:    9009\nTrain Data Columns: 3\nLabel Column: is_fraud\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [1, 0]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    12201.29 MB\n    Train Data (Original)  Memory Usage: 0.22 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', []) : 2 | ['amt', 'distance_km']\n        ('int', [])   : 1 | ['trans_date_trans_time']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', []) : 2 | ['amt', 'distance_km']\n        ('int', [])   : 1 | ['trans_date_trans_time']\n    0.0s = Fit runtime\n    3 features in original data used to generate 3 features in processed data.\n    Train Data (Processed) Memory Usage: 0.22 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.04s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif_BAG_L1 ...\n    0.7325   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: KNeighborsDist_BAG_L1 ...\n    0.737    = Validation score   (accuracy)\n    0.0s     = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMXT_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8918   = Validation score   (accuracy)\n    1.14s    = Training   runtime\n    0.17s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.9003   = Validation score   (accuracy)\n    0.79s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: RandomForestGini_BAG_L1 ...\n    0.887    = Validation score   (accuracy)\n    0.39s    = Training   runtime\n    0.2s     = Validation runtime\nFitting model: RandomForestEntr_BAG_L1 ...\n    0.8876   = Validation score   (accuracy)\n    0.53s    = Training   runtime\n    0.21s    = Validation runtime\nFitting model: CatBoost_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8993   = Validation score   (accuracy)\n    2.63s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: ExtraTreesGini_BAG_L1 ...\n    0.8818   = Validation score   (accuracy)\n    0.32s    = Training   runtime\n    0.23s    = Validation runtime\nFitting model: ExtraTreesEntr_BAG_L1 ...\n    0.8813   = Validation score   (accuracy)\n    0.32s    = Training   runtime\n    0.24s    = Validation runtime\nFitting model: NeuralNetFastAI_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8664   = Validation score   (accuracy)\n    8.79s    = Training   runtime\n    0.13s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8993   = Validation score   (accuracy)\n    0.79s    = Training   runtime\n    0.04s    = Validation runtime\nFitting model: NeuralNetTorch_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8862   = Validation score   (accuracy)\n    15.89s   = Training   runtime\n    0.07s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ...\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n    0.8928   = Validation score   (accuracy)\n    1.36s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.922    = Validation score   (accuracy)\n    2.2s     = Training   runtime\n    0.01s    = Validation runtime\nAutoGluon training complete, total runtime = 43.5s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231011_125833/\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7fcb14ccb700&gt;\n\n\n\npredictr.leaderboard()\n\n                      model  score_val  pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0       WeightedEnsemble_L2   0.921967       1.031116  26.060251                0.009939           2.203469            2       True         14\n1           LightGBM_BAG_L1   0.900322       0.031546   0.788781                0.031546           0.788781            1       True          4\n2           CatBoost_BAG_L1   0.899323       0.005555   2.631501                0.005555           2.631501            1       True          7\n3            XGBoost_BAG_L1   0.899323       0.037676   0.793630                0.037676           0.793630            1       True         11\n4      LightGBMLarge_BAG_L1   0.892774       0.034059   1.360121                0.034059           1.360121            1       True         13\n5         LightGBMXT_BAG_L1   0.891775       0.173170   1.136529                0.173170           1.136529            1       True          3\n6   RandomForestEntr_BAG_L1   0.887557       0.205703   0.534585                0.205703           0.534585            1       True          6\n7   RandomForestGini_BAG_L1   0.887002       0.204052   0.386727                0.204052           0.386727            1       True          5\n8     NeuralNetTorch_BAG_L1   0.886225       0.067507  15.891491                0.067507          15.891491            1       True         12\n9     ExtraTreesGini_BAG_L1   0.881785       0.233411   0.318739                0.233411           0.318739            1       True          8\n10    ExtraTreesEntr_BAG_L1   0.881341       0.244429   0.322460                0.244429           0.322460            1       True          9\n11   NeuralNetFastAI_BAG_L1   0.866356       0.129635   8.785419                0.129635           8.785419            1       True         10\n12    KNeighborsDist_BAG_L1   0.737041       0.008475   0.004331                0.008475           0.004331            1       True          2\n13    KNeighborsUnif_BAG_L1   0.732490       0.009005   0.006626                0.009005           0.006626            1       True          1\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nWeightedEnsemble_L2\n0.921967\n1.031116\n26.060251\n0.009939\n2.203469\n2\nTrue\n14\n\n\n1\nLightGBM_BAG_L1\n0.900322\n0.031546\n0.788781\n0.031546\n0.788781\n1\nTrue\n4\n\n\n2\nCatBoost_BAG_L1\n0.899323\n0.005555\n2.631501\n0.005555\n2.631501\n1\nTrue\n7\n\n\n3\nXGBoost_BAG_L1\n0.899323\n0.037676\n0.793630\n0.037676\n0.793630\n1\nTrue\n11\n\n\n4\nLightGBMLarge_BAG_L1\n0.892774\n0.034059\n1.360121\n0.034059\n1.360121\n1\nTrue\n13\n\n\n5\nLightGBMXT_BAG_L1\n0.891775\n0.173170\n1.136529\n0.173170\n1.136529\n1\nTrue\n3\n\n\n6\nRandomForestEntr_BAG_L1\n0.887557\n0.205703\n0.534585\n0.205703\n0.534585\n1\nTrue\n6\n\n\n7\nRandomForestGini_BAG_L1\n0.887002\n0.204052\n0.386727\n0.204052\n0.386727\n1\nTrue\n5\n\n\n8\nNeuralNetTorch_BAG_L1\n0.886225\n0.067507\n15.891491\n0.067507\n15.891491\n1\nTrue\n12\n\n\n9\nExtraTreesGini_BAG_L1\n0.881785\n0.233411\n0.318739\n0.233411\n0.318739\n1\nTrue\n8\n\n\n10\nExtraTreesEntr_BAG_L1\n0.881341\n0.244429\n0.322460\n0.244429\n0.322460\n1\nTrue\n9\n\n\n11\nNeuralNetFastAI_BAG_L1\n0.866356\n0.129635\n8.785419\n0.129635\n8.785419\n1\nTrue\n10\n\n\n12\nKNeighborsDist_BAG_L1\n0.737041\n0.008475\n0.004331\n0.008475\n0.004331\n1\nTrue\n2\n\n\n13\nKNeighborsUnif_BAG_L1\n0.732490\n0.009005\n0.006626\n0.009005\n0.006626\n1\nTrue\n1"
  },
  {
    "objectID": "posts/2023-11-09-graft.out-Copy1.html",
    "href": "posts/2023-11-09-graft.out-Copy1.html",
    "title": "[Essays] graft",
    "section": "",
    "text": "[Essays] graft\n신록예찬\n2023-11-09\n\n\nImports\n소스코드 다운로드: https://github.com/guebin/graft\n\n계속 업데이트할 예정\n\n\n#!git clone https://github.com/guebin/graft\n\n\n#!conda install -c conda-forge graph-tool -y\n\n\nimport numpy as np\nimport torch\nimport torch_geometric\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nimport graft\n\n\nlinks = torch.tensor([[0, 1, 2, 3, 4, 3],\n                      [1, 0, 3, 2, 3, 4]], dtype=torch.long)\ng = torch_geometric.data.Data(\n    edge_index = links\n)\n\n\ngraft.graph.plot_undirected_unweighted(g)\n\n\n\n\n# Ex – node_names\n\ngraft.graph.plot_undirected_unweighted(\n    g, \n    node_names = ['a','b','c','d','e'], \n)\n\n\n\n\n# Ex – node_color (continuous)\n\nlinks = torch.tensor([[0, 1, 2, 3, 4, 3],\n                      [1, 0, 3, 2, 3, 4]], dtype=torch.long)\ng = torch_geometric.data.Data(\n    edge_index = links, \n    y = np.random.randn(5)\n)\n\n\ngraft.graph.plot_undirected_unweighted(\n    g,\n    node_color=g.y\n)\n\n\n\n\n# Ex – node_color (discrete)\n\nlinks = torch.tensor([[0, 1, 2, 3, 4, 3],\n                      [1, 0, 3, 2, 3, 4]], dtype=torch.long)\ng = torch_geometric.data.Data(\n    edge_index = links, \n    y = torch.tensor([0,1,0,0,1])\n)\n\n\ngraft.graph.plot_undirected_unweighted(\n    g,\n    node_color=g.y\n)\n\n\n\n\n# Ex – node_color (discrete) / node_size\n\nlinks = torch.tensor([[0, 1, 2, 3, 4, 3],\n                      [1, 0, 3, 2, 3, 4]], dtype=torch.long)\ng = torch_geometric.data.Data(\n    edge_index = links, \n    y = torch.tensor([0,1,0,0,1]),\n    x = torch.tensor([10,100,15,20,150])\n)\n\n\ngraft.graph.plot_undirected_unweighted(\n    g,\n    node_color=g.y,\n    node_size=g.x\n)\n\n\n\n\n# Ex – draw options\n\nlinks = torch.tensor([[0, 1, 2, 3, 4, 3],\n                      [1, 0, 3, 2, 3, 4]], dtype=torch.long)\ng = torch_geometric.data.Data(\n    edge_index = links\n)\n\n\ngraft.graph.plot_undirected_unweighted(\n    g,\n)\n\n\n\n\n\ndr_opts = {\n    'vertex_size':30\n}\ngraft.graph.plot_undirected_unweighted(\n    g,\n    draw_options= dr_opts\n)\n\n\n\n\n\ndr_opts = {\n    'edge_marker_size': 200, \n    'output_size': (300,300)\n}\ngraft.graph.plot_undirected_unweighted(\n    g,\n    draw_options= dr_opts\n)\n\n\n\n\n\n\nUndirected / Weighted\n# Ex\n\nlinks = torch.tensor([[0, 1, 2, 3, 4, 3],\n                      [1, 0, 3, 2, 3, 4]], dtype=torch.long)\nweights = torch.tensor([5, 5, 1.5, 1.5, 0.19, 0.19], dtype=torch.float)\n\ng = torch_geometric.data.Data(\n    edge_index=links,\n    edge_attr=weights,\n)\n\n\ngraft.graph.plot_undirected_weighted(\n    g,\n)\n\n\n\n\n# Ex\n\nlinks = torch.tensor([[0, 1, 2, 3, 4, 3],\n                      [1, 0, 3, 2, 3, 4]], dtype=torch.long)\nweights = torch.tensor([5, 5, 1.5, 1.5, 0.19, 0.19], dtype=torch.float)\n\ng = torch_geometric.data.Data(\n    edge_index=links,\n    edge_attr=weights,\n)\n\n\ngraft.graph.plot_undirected_weighted(\n    g,\n    edge_weight_text=False\n)\n\n\n\n\n\ngraft.graph.plot_undirected_weighted(\n    g,\n    edge_weight_text=False,\n    edge_weight_width=False\n)\n\n\n\n\n\ngraft.graph.plot_undirected_weighted(\n    g,\n    edge_weight_text=True,\n    edge_weight_width=True,\n    edge_weight_text_format='.1f',\n)\n\n\n\n\n\ngraft.graph.plot_undirected_weighted(\n    g,\n    edge_weight_text=True,\n    edge_weight_width=True,\n    edge_weight_text_format='.1f',\n    edge_weight_width_scale=5.0,\n)\n\n\n\n\n# Ex\n\nlinks = torch.tensor([[0, 1, 2, 3, 4, 3],\n                      [1, 0, 3, 2, 3, 4]], dtype=torch.long)\nweights = torch.tensor([5, 5, 1.5, 1.5, 0.19, 0.19], dtype=torch.float)\n\ng = torch_geometric.data.Data(\n    edge_index=links,\n    edge_attr=weights,\n    y = torch.tensor([1,1,0,0,0])\n)\n\n\ngraft.graph.plot_undirected_weighted(\n    g,\n    node_color=g.y\n)\n\n\n\n\n# Ex\n\nlinks = torch.tensor([[0, 1, 2, 3, 4, 3],\n                      [1, 0, 3, 2, 3, 4]], dtype=torch.long)\nweights = torch.tensor([5, 5, 1.5, 1.5, 0.19, 0.19], dtype=torch.float)\ng = torch_geometric.data.Data(\n    edge_index = links, \n    edge_attr = weights,\n    y = torch.tensor([0,1,0,0,1]),\n    x = torch.tensor([1,3,1,2,4])\n)\n\n\ngraft.graph.plot_undirected_weighted(\n    g,\n    node_color=g.y,\n    node_size=g.x,\n    edge_weight_text=False,\n)"
  },
  {
    "objectID": "posts/231002 데이터(14, df50 정리).html",
    "href": "posts/231002 데이터(14, df50 정리).html",
    "title": "[FRAUD] 데이터정리 시도(GCN)",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\nimport xgboost as xgb\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# gnn\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric\nfrom torch_geometric.nn import GCNConv\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_geometric/typing.py:18: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/libpyg.so: undefined symbol: _ZN2at4_ops12split_Tensor4callERKNS_6TensorEN3c106SymIntEl\n  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_geometric/typing.py:31: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_scatter/_scatter_cuda.so: undefined symbol: _ZNK3c107SymBool10guard_boolEPKcl\n  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_geometric/typing.py:42: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_sparse/_diag_cuda.so: undefined symbol: _ZN3c106detail19maybe_wrap_dim_slowIlEET_S2_S2_b\n  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n\n\n\ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef compute_time_difference(group):\n    n = len(group)\n    result = []\n    for i in range(n):\n        for j in range(n):\n            time_difference = abs(group.iloc[i].trans_date_trans_time.value - group.iloc[j].trans_date_trans_time.value)\n            result.append([group.iloc[i].name, group.iloc[j].name, time_difference])\n    return result\n\ndef mask(df):\n    df_tr,df_test = sklearn.model_selection.train_test_split(df, random_state=42)\n    N = len(df)\n    train_mask = [i in df_tr.index for i in range(N)]\n    test_mask = [i in df_test.index for i in range(N)]\n    train_mask = np.array(train_mask)\n    test_mask = np.array(test_mask)\n    return train_mask, test_mask\n\ndef edge_index_selected(edge_index):\n    theta = edge_index[:,2].mean()\n    edge_index[:,2] = (np.exp(-edge_index[:,2]/theta) != 1)*(np.exp(-edge_index[:,2]/theta))\n    edge_index = edge_index.tolist()\n    mean_ = np.array(edge_index)[:,2].mean()\n    selected_edges = [(int(row[0]), int(row[1])) for row in edge_index if row[2] &gt; mean_]\n    edge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\n    return edge_index_selected\n\n\n\n\nfraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\n\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50 = df50.reset_index()\ndf50.shape\n\n(12012, 23)\n\n\n\n\n\n\nmask(df50)\n\n(array([False,  True,  True, ...,  True, False,  True]),\n array([ True, False, False, ..., False,  True, False]))\n\n\n\ntrain_mask, test_mask = mask(df50)\n\n\n\n\n\n\n# groups = df50.groupby('cc_num')\n# edge_index_list_plus = [compute_time_difference(group) for _, group in groups]\n# edge_index_list_plus_flat = [item for sublist in edge_index_list_plus for item in sublist]\n# edge_index_list_plus_nparr = np.array(edge_index_list_plus_flat)\n# np.save('edge_index_list_plus50.npy', edge_index_list_plus_nparr)\n\n\nedge_index = np.load('edge_index_list_plus50.npy').astype(np.float64)\nedge_index.shape\n\n(200706, 3)\n\n\n\nedge_index_selected = edge_index_selected(edge_index)\n\n\n\n\n\n\nx = torch.tensor(df50['amt'], dtype=torch.float).reshape(-1,1)\ny = torch.tensor(df50['is_fraud'],dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index = edge_index_selected, y=y, train_mask = train_mask, test_mask = test_mask)\ndata\n\nData(x=[12012, 1], edge_index=[2, 93730], y=[12012], train_mask=[12012], test_mask=[12012])"
  },
  {
    "objectID": "posts/231002 데이터(14, df50 정리).html#데이터정리",
    "href": "posts/231002 데이터(14, df50 정리).html#데이터정리",
    "title": "[FRAUD] 데이터정리 시도(GCN)",
    "section": "",
    "text": "_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50 = df50.reset_index()\ndf50.shape\n\n(12012, 23)\n\n\n\n\n\n\nmask(df50)\n\n(array([False,  True,  True, ...,  True, False,  True]),\n array([ True, False, False, ..., False,  True, False]))\n\n\n\ntrain_mask, test_mask = mask(df50)\n\n\n\n\n\n\n# groups = df50.groupby('cc_num')\n# edge_index_list_plus = [compute_time_difference(group) for _, group in groups]\n# edge_index_list_plus_flat = [item for sublist in edge_index_list_plus for item in sublist]\n# edge_index_list_plus_nparr = np.array(edge_index_list_plus_flat)\n# np.save('edge_index_list_plus50.npy', edge_index_list_plus_nparr)\n\n\nedge_index = np.load('edge_index_list_plus50.npy').astype(np.float64)\nedge_index.shape\n\n(200706, 3)\n\n\n\nedge_index_selected = edge_index_selected(edge_index)\n\n\n\n\n\n\nx = torch.tensor(df50['amt'], dtype=torch.float).reshape(-1,1)\ny = torch.tensor(df50['is_fraud'],dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index = edge_index_selected, y=y, train_mask = train_mask, test_mask = test_mask)\ndata\n\nData(x=[12012, 1], edge_index=[2, 93730], y=[12012], train_mask=[12012], test_mask=[12012])"
  },
  {
    "objectID": "posts/231002 데이터(14, df50 정리).html#분석-1gcn",
    "href": "posts/231002 데이터(14, df50 정리).html#분석-1gcn",
    "title": "[FRAUD] 데이터정리 시도(GCN)",
    "section": "분석 1(GCN)",
    "text": "분석 1(GCN)\n\ntorch.manual_seed(202250926)\n\nclass GCN1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 16)\n        self.conv2 = GCNConv(16,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\nX = (data.x[data.train_mask]).numpy()\nXX = (data.x[data.test_mask]).numpy()\ny = (data.y[data.train_mask]).numpy()\nyy = (data.y[data.test_mask]).numpy()\n\nmodel = GCN1()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(400):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\nmodel.eval()\n\npred = model(data).argmax(dim=1)\nyyhat = pred[data.test_mask]\n\npred\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results1= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석1'])\n_results1\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석1\n0.897436\n0.861759\n0.949242\n0.903388"
  },
  {
    "objectID": "posts/231002 데이터(14, df50 정리).html#분석-2gcn",
    "href": "posts/231002 데이터(14, df50 정리).html#분석-2gcn",
    "title": "[FRAUD] 데이터정리 시도(GCN)",
    "section": "분석 2(GCN)",
    "text": "분석 2(GCN)\n\ntorch.manual_seed(202250926)\nclass GCN2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 32)\n        self.conv2 = GCNConv(32,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\nX = (data.x[data.train_mask]).numpy()\nXX = (data.x[data.test_mask]).numpy()\ny = (data.y[data.train_mask]).numpy()\nyy = (data.y[data.test_mask]).numpy()\n\nmodel = GCN2()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(400):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\nmodel.eval()\n\npred = model(data).argmax(dim=1)\nyyhat = pred[data.test_mask]\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results2= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석2'])\n_results2\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석2\n0.902098\n0.862478\n0.95913\n0.90824"
  },
  {
    "objectID": "posts/231002 데이터(14, df50 정리).html#분석-3gcn",
    "href": "posts/231002 데이터(14, df50 정리).html#분석-3gcn",
    "title": "[FRAUD] 데이터정리 시도(GCN)",
    "section": "분석 3(GCN)",
    "text": "분석 3(GCN)\n\ntorch.manual_seed(202250926)\nclass GCN3(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 64)\n        self.conv2 = GCNConv(64,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\nX = (data.x[data.train_mask]).numpy()\nXX = (data.x[data.test_mask]).numpy()\ny = (data.y[data.train_mask]).numpy()\nyy = (data.y[data.test_mask]).numpy()\n\nmodel = GCN3()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(400):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\nmodel.eval()\n\npred = model(data).argmax(dim=1)\nyyhat = pred[data.test_mask]\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results3= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석3'])\n_results3\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석3\n0.901099\n0.863962\n0.954515\n0.906984"
  },
  {
    "objectID": "posts/231002 데이터(14, df50 정리).html#분석-4gcn",
    "href": "posts/231002 데이터(14, df50 정리).html#분석-4gcn",
    "title": "[FRAUD] 데이터정리 시도(GCN)",
    "section": "분석 4(GCN)",
    "text": "분석 4(GCN)\n\ntorch.manual_seed(202250926)\nX = (data.x[data.train_mask]).numpy()\nXX = (data.x[data.test_mask]).numpy()\ny = (data.y[data.train_mask]).numpy()\nyy = (data.y[data.test_mask]).numpy()\n\nmodel = GCN1()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.03, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(400):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\nmodel.eval()\n\npred = model(data).argmax(dim=1)\nyyhat = pred[data.test_mask]\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results4= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석4'])\n_results4\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석4\n0.894772\n0.864602\n0.938695\n0.900126"
  },
  {
    "objectID": "posts/231002 데이터(14, df50 정리).html#분석-5gcn",
    "href": "posts/231002 데이터(14, df50 정리).html#분석-5gcn",
    "title": "[FRAUD] 데이터정리 시도(GCN)",
    "section": "분석 5(GCN)",
    "text": "분석 5(GCN)\n\ntorch.manual_seed(202250926)\nclass GCN1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 16)\n        self.conv2 = GCNConv(16,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\nX = (data.x[data.train_mask]).numpy()\nXX = (data.x[data.test_mask]).numpy()\ny = (data.y[data.train_mask]).numpy()\nyy = (data.y[data.test_mask]).numpy()\n\nmodel = GCN1()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.05, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(400):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\nmodel.eval()\n\npred = model(data).argmax(dim=1)\nyyhat = pred[data.test_mask]\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results5= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석5'])\n_results5\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석5\n0.889444\n0.864167\n0.926829\n0.894402"
  },
  {
    "objectID": "posts/231002 데이터(14, df50 정리).html#분석-6gcn",
    "href": "posts/231002 데이터(14, df50 정리).html#분석-6gcn",
    "title": "[FRAUD] 데이터정리 시도(GCN)",
    "section": "분석 6(GCN)",
    "text": "분석 6(GCN)\n\ntorch.manual_seed(202250926)\nX = (data.x[data.train_mask]).numpy()\nXX = (data.x[data.test_mask]).numpy()\ny = (data.y[data.train_mask]).numpy()\nyy = (data.y[data.test_mask]).numpy()\n\nmodel = GCN1()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(400):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\nmodel.eval()\n\npred = model(data).argmax(dim=1)\nyyhat = pred[data.test_mask]\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results6= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석6'])\n_results6\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석6\n0.897436\n0.861759\n0.949242\n0.903388"
  },
  {
    "objectID": "posts/231002 데이터(14, df50 정리).html#분석-7gcn",
    "href": "posts/231002 데이터(14, df50 정리).html#분석-7gcn",
    "title": "[FRAUD] 데이터정리 시도(GCN)",
    "section": "분석 7(GCN)",
    "text": "분석 7(GCN)\n\ntorch.manual_seed(202250926)\nX = (data.x[data.train_mask]).numpy()\nXX = (data.x[data.test_mask]).numpy()\ny = (data.y[data.train_mask]).numpy()\nyy = (data.y[data.test_mask]).numpy()\n\nmodel = GCN1()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(1000):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\nmodel.eval()\n\npred = model(data).argmax(dim=1)\nyyhat = pred[data.test_mask]\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results7= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석7'])\n_results7\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석7\n0.900766\n0.863881\n0.953856\n0.906642"
  },
  {
    "objectID": "posts/231020 책_코드 비교.html",
    "href": "posts/231020 책_코드 비교.html",
    "title": "[FRAUD] 책_코드",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\nimport xgboost as xgb\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# gnn\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric\nfrom torch_geometric.nn import GCNConv\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_geometric/typing.py:18: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/libpyg.so: undefined symbol: _ZN2at4_ops12split_Tensor4callERKNS_6TensorEN3c106SymIntEl\n  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_geometric/typing.py:31: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_scatter/_scatter_cuda.so: undefined symbol: _ZNK3c107SymBool10guard_boolEPKcl\n  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_geometric/typing.py:42: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_sparse/_diag_cuda.so: undefined symbol: _ZN3c106detail19maybe_wrap_dim_slowIlEET_S2_S2_b\n  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n\n\n\ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef compute_time_difference(group):\n    n = len(group)\n    result = []\n    for i in range(n):\n        for j in range(n):\n            time_difference = abs(group.iloc[i].trans_date_trans_time.value - group.iloc[j].trans_date_trans_time.value)\n            result.append([group.iloc[i].name, group.iloc[j].name, time_difference])\n    return result\n\ndef mask(df):\n    df_tr,df_test = sklearn.model_selection.train_test_split(df, random_state=42)\n    N = len(df)\n    train_mask = [i in df_tr.index for i in range(N)]\n    test_mask = [i in df_test.index for i in range(N)]\n    train_mask = np.array(train_mask)\n    test_mask = np.array(test_mask)\n    return train_mask, test_mask\n\ndef edge_index_selected(edge_index):\n    theta = edge_index[:,2].mean()\n    edge_index[:,2] = (np.exp(-edge_index[:,2]/theta) != 1)*(np.exp(-edge_index[:,2]/theta))\n    edge_index = edge_index.tolist()\n    mean_ = np.array(edge_index)[:,2].mean()\n    selected_edges = [(int(row[0]), int(row[1])) for row in edge_index if row[2] &gt; mean_]\n    edge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\n    return edge_index_selected\n\n\n\n\nfraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\n\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50 = df50.reset_index()\ndf50.shape\n\n(12012, 23)\n\n\n\n\n\n\nmask(df50)\n\n(array([False,  True,  True, ...,  True, False,  True]),\n array([ True, False, False, ..., False,  True, False]))\n\n\n\ntrain_mask, test_mask = mask(df50)\n\n\n신용카드 거래 그래프 생성\n그래프에서 속성 및 커뮤니티 추출\n사기 거래 분류에 지도 및 비지도 머신러닝 알고리즘 적용"
  },
  {
    "objectID": "posts/231020 책_코드 비교.html#데이터정리",
    "href": "posts/231020 책_코드 비교.html#데이터정리",
    "title": "[FRAUD] 책_코드",
    "section": "",
    "text": "_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50 = df50.reset_index()\ndf50.shape\n\n(12012, 23)\n\n\n\n\n\n\nmask(df50)\n\n(array([False,  True,  True, ...,  True, False,  True]),\n array([ True, False, False, ..., False,  True, False]))\n\n\n\ntrain_mask, test_mask = mask(df50)\n\n\n신용카드 거래 그래프 생성\n그래프에서 속성 및 커뮤니티 추출\n사기 거래 분류에 지도 및 비지도 머신러닝 알고리즘 적용"
  },
  {
    "objectID": "posts/231020 책_코드 비교.html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "href": "posts/231020 책_코드 비교.html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "title": "[FRAUD] 책_코드",
    "section": "사기 탐지를 위한 지도 및 비지도 임베딩",
    "text": "사기 탐지를 위한 지도 및 비지도 임베딩\n\n지도학습\n\nfrom sklearn.utils import resample\n\ndf_majority = df[df.is_fraud==0]\ndf_minority = df[df.is_fraud==1]\n\ndf_maj_dowsampled = resample(df_majority,\n                             n_samples=len(df_minority),\n                             random_state=42)\n\ndf_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n\nprint(df_downsampled.is_fraud.value_counts())\nG_down = build_graph_bipartite(df_downsampled)\n\n1    6006\n0    6006\nName: is_fraud, dtype: int64\n\n\n\n(df_downsampled.is_fraud == 0).count()\n\n12012\n\n\n\ndf_downsampled.shape()\n\nTypeError: 'tuple' object is not callable\n\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_edges, test_edges, train_labels, test_labels = train_test_split(list(range(len(G_down.edges))), \n                                                                      list(nx.get_edge_attributes(G_down, \"label\").values()), \n                                                                      test_size=0.20, \n                                                                      random_state=42)\n\n\nedgs = list(G_down.edges)\ntrain_graph = G_down.edge_subgraph([edgs[x] for x in train_edges]).copy()\ntrain_graph.add_nodes_from(list(set(G_down.nodes) - set(train_graph.nodes)))\n\n\ntrain_graph.number_of_edges(), train_graph.number_of_nodes()\n\n(9351, 1624)\n\n\n\n데이터 8:2 비율로 학습 검증\n\n\nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\nnode2vec_train = Node2Vec(train_graph, weight_key='weight')\nmodel_train = node2vec_train.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04&lt;00:00,  2.41it/s]\n\n\n\nNode2Vec 알고리즘 사용해 특징 공간 구축\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\n\n# 설정할 시드 값\nseed = 202250926\n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\n\nfor cl in classes:\n    embeddings_train = cl(keyed_vectors=model_train.wv)\n\n    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n\n    rf = RandomForestClassifier(n_estimators=1000, random_state=seed)\n    rf.fit(train_embeddings, train_labels)\n\n    y_pred = rf.predict(test_embeddings)\n    print(cl)\n    print('Precision:', metrics.precision_score(test_labels, y_pred))\n    print('Recall:', metrics.recall_score(test_labels, y_pred))\n    print('F1-Score:', metrics.f1_score(test_labels, y_pred))\n    print('Accuracy:', metrics.accuracy_score(test_labels, y_pred))\n\n&lt;class 'node2vec.edges.HadamardEmbedder'&gt;\nPrecision: 0.7210884353741497\nRecall: 0.1781512605042017\nF1-Score: 0.2857142857142857\nAccuracy: 0.5466210436270317\n&lt;class 'node2vec.edges.AverageEmbedder'&gt;\nPrecision: 0.6987673343605547\nRecall: 0.7621848739495798\nF1-Score: 0.7290996784565916\nAccuracy: 0.7117194183062446\n&lt;class 'node2vec.edges.WeightedL1Embedder'&gt;\nPrecision: 0.5813953488372093\nRecall: 0.02100840336134454\nF1-Score: 0.040551500405515\nAccuracy: 0.4940119760479042\n&lt;class 'node2vec.edges.WeightedL2Embedder'&gt;\nPrecision: 0.6444444444444445\nRecall: 0.024369747899159664\nF1-Score: 0.04696356275303644\nAccuracy: 0.4965782720273738\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# 그래프를 그릴 때 사용할 노드 및 엣지 색상 설정\nnode_color = 'lightblue'\nedge_color = 'gray'\n\n# 그래프 그리기\nplt.figure(figsize=(10, 10))\npos = nx.spring_layout(train_graph)  # 그래프의 레이아웃 설정\nnx.draw(train_graph, pos, with_labels=True, node_color=node_color, edge_color=edge_color)\nplt.title('Train Graph')\nplt.show()\n\n\n\n\n\n\n비지도학습\n\nk-means 알고리즘 사용\n지도학습과의 차이점은 특징 공간이 학습-검증 분할을 안함.\n\n\nnod2vec_unsup = Node2Vec(G_down, weight_key='weight')\nunsup_vals = nod2vec_unsup.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04&lt;00:00,  2.28it/s]\n\n\n\n다운샘플링 절차에 전체 그래프 알고리즘 계산\n\n\nfrom sklearn.cluster import KMeans\n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\ntrue_labels = [x for x in nx.get_edge_attributes(G_down, \"label\").values()]\n\nfor cl in classes:\n    embedding_edge = cl(keyed_vectors=unsup_vals.wv) \n\n    embedding = [embedding_edge[str(x[0]), str(x[1])] for x in G_down.edges()]\n    kmeans = KMeans(2, random_state=42).fit(embedding)\n    \n    \n    nmi = metrics.adjusted_mutual_info_score(true_labels, kmeans.labels_)\n    ho = metrics.homogeneity_score(true_labels, kmeans.labels_)\n    co = metrics.completeness_score(true_labels, kmeans.labels_)\n    vmeasure = metrics.v_measure_score(true_labels, kmeans.labels_)\n    \n    print(cl)\n    print('NMI:', nmi)\n    print('Homogeneity:', ho)\n    print('Completeness:', co)\n    print('V-Measure:', vmeasure)\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601a0a60&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601a0a60&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601a0a60&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601a0a60&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f417cde11f0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f417cde11f0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601a0a60&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\nException ignored on calling ctypes callback function: &lt;function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.&lt;locals&gt;.match_module_callback at 0x7f41601b3af0&gt;\nTraceback (most recent call last):\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\nAttributeError: 'NoneType' object has no attribute 'split'\n\n\n&lt;class 'node2vec.edges.HadamardEmbedder'&gt;\nNMI: 0.0488304103554333\nHomogeneity: 0.04260773965601987\nCompleteness: 0.05736666210172528\nV-Measure: 0.04889779305082587\n&lt;class 'node2vec.edges.AverageEmbedder'&gt;\nNMI: 0.09709110346937713\nHomogeneity: 0.09171818451571066\nCompleteness: 0.10326600134945695\nV-Measure: 0.09715013680667017\n&lt;class 'node2vec.edges.WeightedL1Embedder'&gt;\nNMI: 0.1713964349047398\nHomogeneity: 0.1714171156088649\nCompleteness: 0.17147807224687064\nV-Measure: 0.17144758850972272\n&lt;class 'node2vec.edges.WeightedL2Embedder'&gt;\nNMI: 0.13969156796365498\nHomogeneity: 0.13857242672795833\nCompleteness: 0.1409378289079951\nV-Measure: 0.13974511901264577"
  },
  {
    "objectID": "posts/230814 fraud(2, tr,test_mask).html",
    "href": "posts/230814 fraud(2, tr,test_mask).html",
    "title": "[FRAUD] 데이터정리 시도(8.14-망함 tr/test_mask 만들어봄)",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\nimport torch\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics \n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부     \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\ndef build_graph_tripartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n                                                       df[\"cc_num\"].values.tolist() +\n                                                       df[\"merchant\"].values.tolist()))}\n    df[\"in_node\"]= df[\"cc_num\"].apply(lambda x: mapping[x])\n    df[\"out_node\"]=df[\"merchant\"].apply(lambda x:mapping[x])\n    \n        \n    G=nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\\\n                        [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")     \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")   \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")  \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n\n    return G\n    \n    \ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef embedding(Graph):\n    # Graph -&gt; X (feature)\n    _edgs = list(Graph.edges)\n    subGraph = Graph.edge_subgraph([_edgs[x] for x in range(len(Graph.edges))]).copy()\n    subGraph.add_nodes_from(list(set(Graph.nodes) - set(subGraph.nodes)))    \n    embedded = AverageEmbedder(Node2Vec(subGraph, weight_key='weight').fit(window=10).wv)\n    X = [embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in range(len(Graph.edges))]\n    # Graph -&gt; y (label)\n    y = np.array(list(nx.get_edge_attributes(Graph, \"label\").values()))\n    return X,y \n\ndef anal(df):\n    Graph = build_graph_bipartite(df)\n    X,XX,y,yy = embedding(Graph)\n    lrnr = RandomForestClassifier(n_estimators=100, random_state=42) \n    lrnr.fit(X,y)\n    yyhat = lrnr.predict(XX)\n    df = pd.DataFrame({\n        'acc':[sklearn.metrics.accuracy_score(yy,yyhat)], \n        'pre':[sklearn.metrics.precision_score(yy,yyhat)], \n        'rec':[sklearn.metrics.recall_score(yy,yyhat)],\n        'f1':[sklearn.metrics.f1_score(yy,yyhat)]}\n    )    \n    return df\n\ndef our_sampling1(df):\n    cus_list = set(df.query('is_fraud==1').cc_num.tolist())\n    return df.query(\"cc_num in @ cus_list\")\n\n\nfraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\n\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\n12012*12012\n\n144288144\n\n\n\n\n\ndf50 의 shape이 12000개 이므로 9000개의 T, 3000개의 F를 train mask로 만들자.\n고객정보가 동일하면 edge를 1로, 아니면 0으로 놓고 1에대한 weight를 만들자.\ng(V,E,W)에서의 weight\n\n\ndf50 = df50.reset_index()\n\n\nN = len(df50)\n\n\n\n\n\nedge_index_list = []\nfor i in range(N):\n    for j in range(N):\n        time_difference = (df50['trans_date_trans_time'][i] - df50['trans_date_trans_time'][j]).total_seconds()\n        edge_index_list.append([i, j, time_difference])\n\n\nedge_index_list[:5]\n\n[[0, 0, 0.0],\n [0, 1, -2460.0],\n [0, 2, -7140.0],\n [0, 3, -9120.0],\n [0, 4, -10140.0]]\n\n\n\nnp.save('edge_index_list_50.npy', edge_index_list)\n\nloaded_data = np.load('edge_index_list_50.npy')\n\n\nedge_index = np.array(edge_index_list)\nedge_index[:,2] = np.abs(edge_index[:,2])\ntheta = edge_index[:,2].mean()\ntheta\n\n12238996.895508753\n\n\n\nedge_index[:,2] = (np.exp(-edge_index[:,2]/theta)!=1) * np.exp(-edge_index[:,2]/theta)\nedge_index\n\narray([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 1.00000000e+00, 9.99799023e-01],\n       [0.00000000e+00, 2.00000000e+00, 9.99416789e-01],\n       ...,\n       [1.20110000e+04, 1.20090000e+04, 4.19756312e-01],\n       [1.20110000e+04, 1.20100000e+04, 2.26811434e-01],\n       [1.20110000e+04, 1.20110000e+04, 0.00000000e+00]])\n\n\n\nedge_index[:,2]\n\narray([0.        , 0.99979902, 0.99941679, ..., 0.41975631, 0.22681143,\n       0.        ])\n\n\nQ. 그런데 밑에서 random으로 train하고 test로 나누게 되면.. wieght랑 edge를 어떻게 적용시키지?\n\n\n\n\nedge_index_list2 = []\nfor i in range(N):\n    for j in range(N):\n        if df50['cc_num'][i] != df50['cc_num'][j]:  \n            edge = 0\n        else:\n            edge = 1\n        edge_index_list2.append([i, j, edge])\n\n\nnp.save('edge_index_list2_50.npy', edge_index_list2)\n\nloaded_data = np.load('edge_index_list2_50.npy')\n\n\nedge_index_list2[:5]\n\n[[0, 0, 1], [0, 1, 0], [0, 2, 0], [0, 3, 1], [0, 4, 0]]\n\n\n\nedge_one = [(i, j) for i, j, edge in edge_index_list2 if edge == 1]\nedge_one[:5]\n\n[(0, 0), (0, 3), (0, 5), (0, 6), (0, 13)]\n\n\n\nlen(edge_one)\n\n200706\n\n\n\nedge_one_index = torch.tensor(edge_one, dtype=torch.long).t()\n\n\nedge_one_index.shape\n\ntorch.Size([2, 200706])\n\n\n\nedge_list2를 만든다 해도.. 밑에서 다시 적용하려면 index가 달라지면 못하ㅏ는거아닌가?\n\n\n\n\n\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.is_fraud.mean().round(5), df50_test.is_fraud.mean().round(5)\n\n(0.49828, 0.50516)\n\n\n\ndf50_tr.shape, df50_test.shape\n\n((9009, 23), (3003, 23))\n\n\n\ntrain_mask = np.concatenate((np.full(9009, True), np.full(3003, False)))\ntest_mask = np.concatenate((np.full(9009, False), np.full(3003, True)))\nprint(\"Train Mask:\", train_mask)\nprint(\"Test Mask:\", test_mask)\n\nTrain Mask: [ True  True  True ... False False False]\nTest Mask: [False False False ...  True  True  True]\n\n\n\ntrain_mask.shape, test_mask.shape\n\n((12012,), (12012,))\n\n\n\ntrain_mask.sum(), test_mask.sum()\n\n(9009, 3003)\n\n\n\n여기서 tr/test를 나눠서 하는건 안될거같어.\n\n\n\n\n\n\nx = df50['amt']\n\n\na = torch.tensor(x, dtype=torch.float)\n\n\na = a.reshape(-1,1)\na\n\ntensor([[281.0600],\n        [ 11.5200],\n        [276.3100],\n        ...,\n        [  8.1200],\n        [  3.5200],\n        [ 84.1500]])\n\n\n\ny = df50['is_fraud']\n\n\nb = torch.tensor(y,dtype=torch.int64)\n\n\nb\n\ntensor([1, 1, 1,  ..., 0, 0, 0])\n\n\n\nimport torch_geometric\n\n\ndata = torch_geometric.data.Data(x=a, edge_index = edge_one_index, y=b, train_mask = train_mask, test_mask = test_mask)\n\n\ndata\n\nData(x=[12012, 1], edge_index=[2, 200706], y=[12012], train_mask=[12012], test_mask=[12012])\n\n\n흠 .. 위의 edge_index에서. 각각의 w를 어떻게 연산해주려나\nx랑 y의 순서를 무작위로 바꿔야하눈뎀. &gt; 음 그럼 일단 edge_one_index이거부터 또 다싷..?\n\n\n\n\n\nx = np.concatenate((np.array(df50_tr['amt']), np.array(df50_test['amt'])))\n\n\na = torch.tensor(x2, dtype=torch.float)\n\n\na = a.reshape(-1,1)\na\n\ntensor([[921.2400],\n        [698.2800],\n        [220.5600],\n        ...,\n        [ 17.9700],\n        [  7.5800],\n        [824.9900]])\n\n\n\ny = np.concatenate((np.array(df50_tr['is_fraud']), np.array(df50_test['is_fraud'])))\n\n\nb = torch.tensor(y,dtype=torch.int64)\n\n\ndata = torch_geometric.data.Data(x=a, edge_index = edge_one_index, y=b, train_mask = train_mask, test_mask = test_mask)\n\n\n바꿔버리면 ..\n\n\n\n\n\n\n\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\n\nclass GCN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 16)\n        self.conv2 = GCNConv(16,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\n\nmodel = GCN()\n\n\nmodel\n\nGCN(\n  (conv1): GCNConv(1, 16)\n  (conv2): GCNConv(16, 2)\n)\n\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\n\nGCN(\n  (conv1): GCNConv(1, 16)\n  (conv2): GCNConv(16, 2)\n)\n\n\n\n\nfor epoch in range(200):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\n\n\nmodel.eval()\npred = model(data).argmax(dim=1)\ncorrect = (pred[data.test_mask] == data.y[data.test_mask]).sum()\nacc = int(correct) / int(data.test_mask.sum())\nprint(f'Accuracy: {acc:.4f}')\n\nAccuracy: 0.5758\n\n\n\nout[data.train_mask]\n\ntensor([[-0.7724, -0.6197],\n        [-0.9459, -0.4916],\n        [-0.9459, -0.4916],\n        ...,\n        [-0.7770, -0.6158],\n        [-0.6621, -0.7252],\n        [-0.6588, -0.7287]], grad_fn=&lt;IndexBackward0&gt;)\n\n\n\ndata.y[data.train_mask].sum()\n\ntensor(4489)\n\n\n\ndata.test_mask.sum()\n\n3003\n\n\n\ncorrect/3003\n\ntensor(0.5758)\n\n\n\n\n\n\n\n\nmodel.eval()\npred = model(data).argmax(dim=1)\ncorrect = (pred == data.y).sum() # 애큐러시는 test\nacc = int(correct) / 9009\nprint(f'Accuracy: {acc:.4f}')\n\nAccuracy: 0.9325\n\n\n\n\n\n\n현재 df50의 fraud 비율은 5:5 인데, 다른 비율을 가진 데이터로도 해보자\nGNN으로 돌려본 것과 다른 방법들과 비교를 해보자\nundersampling한 다른 데이터들과 비교해 볼 수 있을 듯(boost, logis, …)\n9000/3000 데이터를 통해 합성 데이터를 만드는데, 12000개를 그대로 만드는 방법, 고객별로(cc_num) 합성 데이터를 만드는 방법, 똑같은 cc_num로 특이한 데이터가 있다면 normal데이터와 특이 데이터를 생각해서 돌리는 방법 등을 고려하자.\n\n\nedge_index = np.array(edge_index_list)\n\n\nedge_index.shape\n\n(81162081, 3)\n\n\n\nedge_index[:,2] = np.abs(edge_index[:,2])\n\n\ntheta = edge_index[:,2].mean()\ntheta\n\n12230796.273867842\n\n\n\nedge_index[:,2] = (np.exp(-edge_index[:,2]/theta)!=1) * np.exp(-edge_index[:,2]/theta)\n\n\nedge_index\n\narray([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 1.00000000e+00, 1.90157975e-01],\n       [0.00000000e+00, 2.00000000e+00, 9.79646259e-02],\n       ...,\n       [9.00800000e+03, 9.00600000e+03, 6.60662164e-01],\n       [9.00800000e+03, 9.00700000e+03, 1.49150646e-01],\n       [9.00800000e+03, 9.00800000e+03, 0.00000000e+00]])\n\n\n\neee = edge_index[:,:]\n\n\neee[:,1]\n\narray([0.000e+00, 1.000e+00, 2.000e+00, ..., 9.006e+03, 9.007e+03,\n       9.008e+03])\n\n\n\nedge_index_list_updated = edge_index.tolist()\n\n\nedge_index_list_updated[:5]\n\n[[0.0, 0.0, 0.0],\n [0.0, 1.0, 0.19015797528259762],\n [0.0, 2.0, 0.09796462590589798],\n [0.0, 3.0, 0.1424157407389685],\n [0.0, 4.0, 0.11107338192969567]]\n\n\n\ncc_num로 그룹별로 묶자.\n\n\nedge_index = np.array(edge_index_list)\n\n\nedge_index.shape\n\n(81162081, 3)\n\n\n\nedge_index\n\narray([[0.000e+00, 0.000e+00, 0.000e+00],\n       [0.000e+00, 1.000e+00, 0.000e+00],\n       [0.000e+00, 2.000e+00, 0.000e+00],\n       ...,\n       [9.008e+03, 9.006e+03, 0.000e+00],\n       [9.008e+03, 9.007e+03, 0.000e+00],\n       [9.008e+03, 9.008e+03, 0.000e+00]])\n\n\n\nedge_index[:,2] = np.abs(edge_index[:,2])\n\n\ntheta = edge_index[:,2].mean()\ntheta\n\n10988.585252761077\n\n\n\nedge_index\n\narray([[0.000e+00, 0.000e+00, 0.000e+00],\n       [0.000e+00, 1.000e+00, 0.000e+00],\n       [0.000e+00, 2.000e+00, 0.000e+00],\n       ...,\n       [9.008e+03, 9.006e+03, 0.000e+00],\n       [9.008e+03, 9.007e+03, 0.000e+00],\n       [9.008e+03, 9.008e+03, 0.000e+00]])\n\n\n\nedge_index[:,2] = (np.exp(-edge_index[:,2]/theta)!=1) * np.exp(-edge_index[:,2]/theta)\n\n\nedge_index\n\narray([[0.000e+00, 0.000e+00, 0.000e+00],\n       [0.000e+00, 1.000e+00, 0.000e+00],\n       [0.000e+00, 2.000e+00, 0.000e+00],\n       ...,\n       [9.008e+03, 9.006e+03, 0.000e+00],\n       [9.008e+03, 9.007e+03, 0.000e+00],\n       [9.008e+03, 9.008e+03, 0.000e+00]])\n\n\n\nedge_index_list_updated = edge_index.tolist()\n\n\nnp.array(edge_index_list_updated)[:,2].mean()\n\n8.344409093328692e-05\n\n\n\nmm = np.array(edge_index_list_updated)[:,2].mean()\n\nedge_index_list_updated가 w\n\nselected_edges = [(int(row[0]), int(row[1])) for row in edge_index_list_updated if row[2] &gt; mm]\n\n\nedge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\n\n\nedge_index_selected.shape\n\ntorch.Size([2, 28472])\n\n\n\nedge_index\n\narray([[0.000e+00, 0.000e+00, 0.000e+00],\n       [0.000e+00, 1.000e+00, 0.000e+00],\n       [0.000e+00, 2.000e+00, 0.000e+00],\n       ...,\n       [9.008e+03, 9.006e+03, 0.000e+00],\n       [9.008e+03, 9.007e+03, 0.000e+00],\n       [9.008e+03, 9.008e+03, 0.000e+00]])\n\n\n- pyg lesson6\n\ngconv = torch_geometric.nn.GCNConv(1,4)\ngconv\n\nGCNConv(1, 4)\n\n\n\ngconv(data.x, data.edge_index)\n\ntensor([[ 4.0225e+02,  2.5312e+02, -2.9747e+02, -1.6831e+02],\n        [ 3.7246e+02,  2.3437e+02, -2.7543e+02, -1.5584e+02],\n        [ 1.5695e+02,  9.8760e+01, -1.1606e+02, -6.5670e+01],\n        ...,\n        [ 2.5448e+02,  1.6013e+02, -1.8818e+02, -1.0648e+02],\n        [ 5.4738e+02,  3.4444e+02, -4.0478e+02, -2.2903e+02],\n        [ 1.1670e+00,  7.3434e-01, -8.6299e-01, -4.8830e-01]],\n       grad_fn=&lt;AddBackward0&gt;)\n\n\n\nlist(gconv.parameters())\n\n[Parameter containing:\n tensor([0., 0., 0., 0.], requires_grad=True),\n Parameter containing:\n tensor([[ 0.7116],\n         [ 0.4478],\n         [-0.5262],\n         [-0.2977]], requires_grad=True)]\n\n\n\n_,W = list(gconv.parameters())\nW\n\nParameter containing:\ntensor([[-0.6724],\n        [ 0.7172],\n        [-0.3185],\n        [ 0.5363]], requires_grad=True)\n\n\n- pyg lesson5\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\n\nclass GCN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 16)\n        self.conv2 = GCNConv(16,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\n\nmodel = GCN()\n\n\nmodel\n\nGCN(\n  (conv1): GCNConv(1, 16)\n  (conv2): GCNConv(16, 2)\n)\n\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\n\nGCN(\n  (conv1): GCNConv(1, 16)\n  (conv2): GCNConv(16, 2)\n)\n\n\n\nout\n\ntensor([[-1.8963e+02,  0.0000e+00],\n        [-1.5192e+02,  0.0000e+00],\n        [-5.3630e+01,  0.0000e+00],\n        ...,\n        [-3.0590e+02,  0.0000e+00],\n        [-3.0298e+02,  0.0000e+00],\n        [-1.3924e+00, -2.8567e-01]], grad_fn=&lt;LogSoftmaxBackward0&gt;)\n\n\n\ndata.y\n\ntensor([1, 1, 0,  ..., 1, 1, 0])\n\n\n\nfor epoch in range(200):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out, data.y)\n    loss.backward()\n    optimizer.step()\n\n\nmodel.eval()\npred = model(data).argmax(dim=1)\ncorrect = (pred == data.y).sum() # 애큐러시는 test\nacc = int(correct) / 9009\nprint(f'Accuracy: {acc:.4f}')\n\nAccuracy: 0.9633\n\n\n\nfraud_mask = (data.y == 1)\n\n\nmodel.eval()\npred = model(data).argmax(dim=1)\ncorrect = (pred[fraud_mask] == data.y[fraud_mask]).sum() # 애큐러시는 test\nacc = int(correct) / int(fraud_mask.sum())\nprint(f'recall: {acc:.4f}')\n\nrecall: 0.9619\n\n\n\n위의 recall은 test가 없어서 train으로만 했던 거..!"
  },
  {
    "objectID": "posts/230814 fraud(2, tr,test_mask).html#시도",
    "href": "posts/230814 fraud(2, tr,test_mask).html#시도",
    "title": "[FRAUD] 데이터정리 시도(8.14-망함 tr/test_mask 만들어봄)",
    "section": "",
    "text": "_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\n12012*12012\n\n144288144\n\n\n\n\n\ndf50 의 shape이 12000개 이므로 9000개의 T, 3000개의 F를 train mask로 만들자.\n고객정보가 동일하면 edge를 1로, 아니면 0으로 놓고 1에대한 weight를 만들자.\ng(V,E,W)에서의 weight\n\n\ndf50 = df50.reset_index()\n\n\nN = len(df50)\n\n\n\n\n\nedge_index_list = []\nfor i in range(N):\n    for j in range(N):\n        time_difference = (df50['trans_date_trans_time'][i] - df50['trans_date_trans_time'][j]).total_seconds()\n        edge_index_list.append([i, j, time_difference])\n\n\nedge_index_list[:5]\n\n[[0, 0, 0.0],\n [0, 1, -2460.0],\n [0, 2, -7140.0],\n [0, 3, -9120.0],\n [0, 4, -10140.0]]\n\n\n\nnp.save('edge_index_list_50.npy', edge_index_list)\n\nloaded_data = np.load('edge_index_list_50.npy')\n\n\nedge_index = np.array(edge_index_list)\nedge_index[:,2] = np.abs(edge_index[:,2])\ntheta = edge_index[:,2].mean()\ntheta\n\n12238996.895508753\n\n\n\nedge_index[:,2] = (np.exp(-edge_index[:,2]/theta)!=1) * np.exp(-edge_index[:,2]/theta)\nedge_index\n\narray([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 1.00000000e+00, 9.99799023e-01],\n       [0.00000000e+00, 2.00000000e+00, 9.99416789e-01],\n       ...,\n       [1.20110000e+04, 1.20090000e+04, 4.19756312e-01],\n       [1.20110000e+04, 1.20100000e+04, 2.26811434e-01],\n       [1.20110000e+04, 1.20110000e+04, 0.00000000e+00]])\n\n\n\nedge_index[:,2]\n\narray([0.        , 0.99979902, 0.99941679, ..., 0.41975631, 0.22681143,\n       0.        ])\n\n\nQ. 그런데 밑에서 random으로 train하고 test로 나누게 되면.. wieght랑 edge를 어떻게 적용시키지?\n\n\n\n\nedge_index_list2 = []\nfor i in range(N):\n    for j in range(N):\n        if df50['cc_num'][i] != df50['cc_num'][j]:  \n            edge = 0\n        else:\n            edge = 1\n        edge_index_list2.append([i, j, edge])\n\n\nnp.save('edge_index_list2_50.npy', edge_index_list2)\n\nloaded_data = np.load('edge_index_list2_50.npy')\n\n\nedge_index_list2[:5]\n\n[[0, 0, 1], [0, 1, 0], [0, 2, 0], [0, 3, 1], [0, 4, 0]]\n\n\n\nedge_one = [(i, j) for i, j, edge in edge_index_list2 if edge == 1]\nedge_one[:5]\n\n[(0, 0), (0, 3), (0, 5), (0, 6), (0, 13)]\n\n\n\nlen(edge_one)\n\n200706\n\n\n\nedge_one_index = torch.tensor(edge_one, dtype=torch.long).t()\n\n\nedge_one_index.shape\n\ntorch.Size([2, 200706])\n\n\n\nedge_list2를 만든다 해도.. 밑에서 다시 적용하려면 index가 달라지면 못하ㅏ는거아닌가?\n\n\n\n\n\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.is_fraud.mean().round(5), df50_test.is_fraud.mean().round(5)\n\n(0.49828, 0.50516)\n\n\n\ndf50_tr.shape, df50_test.shape\n\n((9009, 23), (3003, 23))\n\n\n\ntrain_mask = np.concatenate((np.full(9009, True), np.full(3003, False)))\ntest_mask = np.concatenate((np.full(9009, False), np.full(3003, True)))\nprint(\"Train Mask:\", train_mask)\nprint(\"Test Mask:\", test_mask)\n\nTrain Mask: [ True  True  True ... False False False]\nTest Mask: [False False False ...  True  True  True]\n\n\n\ntrain_mask.shape, test_mask.shape\n\n((12012,), (12012,))\n\n\n\ntrain_mask.sum(), test_mask.sum()\n\n(9009, 3003)\n\n\n\n여기서 tr/test를 나눠서 하는건 안될거같어.\n\n\n\n\n\n\nx = df50['amt']\n\n\na = torch.tensor(x, dtype=torch.float)\n\n\na = a.reshape(-1,1)\na\n\ntensor([[281.0600],\n        [ 11.5200],\n        [276.3100],\n        ...,\n        [  8.1200],\n        [  3.5200],\n        [ 84.1500]])\n\n\n\ny = df50['is_fraud']\n\n\nb = torch.tensor(y,dtype=torch.int64)\n\n\nb\n\ntensor([1, 1, 1,  ..., 0, 0, 0])\n\n\n\nimport torch_geometric\n\n\ndata = torch_geometric.data.Data(x=a, edge_index = edge_one_index, y=b, train_mask = train_mask, test_mask = test_mask)\n\n\ndata\n\nData(x=[12012, 1], edge_index=[2, 200706], y=[12012], train_mask=[12012], test_mask=[12012])\n\n\n흠 .. 위의 edge_index에서. 각각의 w를 어떻게 연산해주려나\nx랑 y의 순서를 무작위로 바꿔야하눈뎀. &gt; 음 그럼 일단 edge_one_index이거부터 또 다싷..?\n\n\n\n\n\nx = np.concatenate((np.array(df50_tr['amt']), np.array(df50_test['amt'])))\n\n\na = torch.tensor(x2, dtype=torch.float)\n\n\na = a.reshape(-1,1)\na\n\ntensor([[921.2400],\n        [698.2800],\n        [220.5600],\n        ...,\n        [ 17.9700],\n        [  7.5800],\n        [824.9900]])\n\n\n\ny = np.concatenate((np.array(df50_tr['is_fraud']), np.array(df50_test['is_fraud'])))\n\n\nb = torch.tensor(y,dtype=torch.int64)\n\n\ndata = torch_geometric.data.Data(x=a, edge_index = edge_one_index, y=b, train_mask = train_mask, test_mask = test_mask)\n\n\n바꿔버리면 ..\n\n\n\n\n\n\n\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\n\nclass GCN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 16)\n        self.conv2 = GCNConv(16,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\n\nmodel = GCN()\n\n\nmodel\n\nGCN(\n  (conv1): GCNConv(1, 16)\n  (conv2): GCNConv(16, 2)\n)\n\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\n\nGCN(\n  (conv1): GCNConv(1, 16)\n  (conv2): GCNConv(16, 2)\n)\n\n\n\n\nfor epoch in range(200):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\n\n\nmodel.eval()\npred = model(data).argmax(dim=1)\ncorrect = (pred[data.test_mask] == data.y[data.test_mask]).sum()\nacc = int(correct) / int(data.test_mask.sum())\nprint(f'Accuracy: {acc:.4f}')\n\nAccuracy: 0.5758\n\n\n\nout[data.train_mask]\n\ntensor([[-0.7724, -0.6197],\n        [-0.9459, -0.4916],\n        [-0.9459, -0.4916],\n        ...,\n        [-0.7770, -0.6158],\n        [-0.6621, -0.7252],\n        [-0.6588, -0.7287]], grad_fn=&lt;IndexBackward0&gt;)\n\n\n\ndata.y[data.train_mask].sum()\n\ntensor(4489)\n\n\n\ndata.test_mask.sum()\n\n3003\n\n\n\ncorrect/3003\n\ntensor(0.5758)\n\n\n\n\n\n\n\n\nmodel.eval()\npred = model(data).argmax(dim=1)\ncorrect = (pred == data.y).sum() # 애큐러시는 test\nacc = int(correct) / 9009\nprint(f'Accuracy: {acc:.4f}')\n\nAccuracy: 0.9325\n\n\n\n\n\n\n현재 df50의 fraud 비율은 5:5 인데, 다른 비율을 가진 데이터로도 해보자\nGNN으로 돌려본 것과 다른 방법들과 비교를 해보자\nundersampling한 다른 데이터들과 비교해 볼 수 있을 듯(boost, logis, …)\n9000/3000 데이터를 통해 합성 데이터를 만드는데, 12000개를 그대로 만드는 방법, 고객별로(cc_num) 합성 데이터를 만드는 방법, 똑같은 cc_num로 특이한 데이터가 있다면 normal데이터와 특이 데이터를 생각해서 돌리는 방법 등을 고려하자.\n\n\nedge_index = np.array(edge_index_list)\n\n\nedge_index.shape\n\n(81162081, 3)\n\n\n\nedge_index[:,2] = np.abs(edge_index[:,2])\n\n\ntheta = edge_index[:,2].mean()\ntheta\n\n12230796.273867842\n\n\n\nedge_index[:,2] = (np.exp(-edge_index[:,2]/theta)!=1) * np.exp(-edge_index[:,2]/theta)\n\n\nedge_index\n\narray([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 1.00000000e+00, 1.90157975e-01],\n       [0.00000000e+00, 2.00000000e+00, 9.79646259e-02],\n       ...,\n       [9.00800000e+03, 9.00600000e+03, 6.60662164e-01],\n       [9.00800000e+03, 9.00700000e+03, 1.49150646e-01],\n       [9.00800000e+03, 9.00800000e+03, 0.00000000e+00]])\n\n\n\neee = edge_index[:,:]\n\n\neee[:,1]\n\narray([0.000e+00, 1.000e+00, 2.000e+00, ..., 9.006e+03, 9.007e+03,\n       9.008e+03])\n\n\n\nedge_index_list_updated = edge_index.tolist()\n\n\nedge_index_list_updated[:5]\n\n[[0.0, 0.0, 0.0],\n [0.0, 1.0, 0.19015797528259762],\n [0.0, 2.0, 0.09796462590589798],\n [0.0, 3.0, 0.1424157407389685],\n [0.0, 4.0, 0.11107338192969567]]\n\n\n\ncc_num로 그룹별로 묶자.\n\n\nedge_index = np.array(edge_index_list)\n\n\nedge_index.shape\n\n(81162081, 3)\n\n\n\nedge_index\n\narray([[0.000e+00, 0.000e+00, 0.000e+00],\n       [0.000e+00, 1.000e+00, 0.000e+00],\n       [0.000e+00, 2.000e+00, 0.000e+00],\n       ...,\n       [9.008e+03, 9.006e+03, 0.000e+00],\n       [9.008e+03, 9.007e+03, 0.000e+00],\n       [9.008e+03, 9.008e+03, 0.000e+00]])\n\n\n\nedge_index[:,2] = np.abs(edge_index[:,2])\n\n\ntheta = edge_index[:,2].mean()\ntheta\n\n10988.585252761077\n\n\n\nedge_index\n\narray([[0.000e+00, 0.000e+00, 0.000e+00],\n       [0.000e+00, 1.000e+00, 0.000e+00],\n       [0.000e+00, 2.000e+00, 0.000e+00],\n       ...,\n       [9.008e+03, 9.006e+03, 0.000e+00],\n       [9.008e+03, 9.007e+03, 0.000e+00],\n       [9.008e+03, 9.008e+03, 0.000e+00]])\n\n\n\nedge_index[:,2] = (np.exp(-edge_index[:,2]/theta)!=1) * np.exp(-edge_index[:,2]/theta)\n\n\nedge_index\n\narray([[0.000e+00, 0.000e+00, 0.000e+00],\n       [0.000e+00, 1.000e+00, 0.000e+00],\n       [0.000e+00, 2.000e+00, 0.000e+00],\n       ...,\n       [9.008e+03, 9.006e+03, 0.000e+00],\n       [9.008e+03, 9.007e+03, 0.000e+00],\n       [9.008e+03, 9.008e+03, 0.000e+00]])\n\n\n\nedge_index_list_updated = edge_index.tolist()\n\n\nnp.array(edge_index_list_updated)[:,2].mean()\n\n8.344409093328692e-05\n\n\n\nmm = np.array(edge_index_list_updated)[:,2].mean()\n\nedge_index_list_updated가 w\n\nselected_edges = [(int(row[0]), int(row[1])) for row in edge_index_list_updated if row[2] &gt; mm]\n\n\nedge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\n\n\nedge_index_selected.shape\n\ntorch.Size([2, 28472])\n\n\n\nedge_index\n\narray([[0.000e+00, 0.000e+00, 0.000e+00],\n       [0.000e+00, 1.000e+00, 0.000e+00],\n       [0.000e+00, 2.000e+00, 0.000e+00],\n       ...,\n       [9.008e+03, 9.006e+03, 0.000e+00],\n       [9.008e+03, 9.007e+03, 0.000e+00],\n       [9.008e+03, 9.008e+03, 0.000e+00]])\n\n\n- pyg lesson6\n\ngconv = torch_geometric.nn.GCNConv(1,4)\ngconv\n\nGCNConv(1, 4)\n\n\n\ngconv(data.x, data.edge_index)\n\ntensor([[ 4.0225e+02,  2.5312e+02, -2.9747e+02, -1.6831e+02],\n        [ 3.7246e+02,  2.3437e+02, -2.7543e+02, -1.5584e+02],\n        [ 1.5695e+02,  9.8760e+01, -1.1606e+02, -6.5670e+01],\n        ...,\n        [ 2.5448e+02,  1.6013e+02, -1.8818e+02, -1.0648e+02],\n        [ 5.4738e+02,  3.4444e+02, -4.0478e+02, -2.2903e+02],\n        [ 1.1670e+00,  7.3434e-01, -8.6299e-01, -4.8830e-01]],\n       grad_fn=&lt;AddBackward0&gt;)\n\n\n\nlist(gconv.parameters())\n\n[Parameter containing:\n tensor([0., 0., 0., 0.], requires_grad=True),\n Parameter containing:\n tensor([[ 0.7116],\n         [ 0.4478],\n         [-0.5262],\n         [-0.2977]], requires_grad=True)]\n\n\n\n_,W = list(gconv.parameters())\nW\n\nParameter containing:\ntensor([[-0.6724],\n        [ 0.7172],\n        [-0.3185],\n        [ 0.5363]], requires_grad=True)\n\n\n- pyg lesson5\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\n\nclass GCN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 16)\n        self.conv2 = GCNConv(16,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\n\nmodel = GCN()\n\n\nmodel\n\nGCN(\n  (conv1): GCNConv(1, 16)\n  (conv2): GCNConv(16, 2)\n)\n\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\n\nGCN(\n  (conv1): GCNConv(1, 16)\n  (conv2): GCNConv(16, 2)\n)\n\n\n\nout\n\ntensor([[-1.8963e+02,  0.0000e+00],\n        [-1.5192e+02,  0.0000e+00],\n        [-5.3630e+01,  0.0000e+00],\n        ...,\n        [-3.0590e+02,  0.0000e+00],\n        [-3.0298e+02,  0.0000e+00],\n        [-1.3924e+00, -2.8567e-01]], grad_fn=&lt;LogSoftmaxBackward0&gt;)\n\n\n\ndata.y\n\ntensor([1, 1, 0,  ..., 1, 1, 0])\n\n\n\nfor epoch in range(200):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out, data.y)\n    loss.backward()\n    optimizer.step()\n\n\nmodel.eval()\npred = model(data).argmax(dim=1)\ncorrect = (pred == data.y).sum() # 애큐러시는 test\nacc = int(correct) / 9009\nprint(f'Accuracy: {acc:.4f}')\n\nAccuracy: 0.9633\n\n\n\nfraud_mask = (data.y == 1)\n\n\nmodel.eval()\npred = model(data).argmax(dim=1)\ncorrect = (pred[fraud_mask] == data.y[fraud_mask]).sum() # 애큐러시는 test\nacc = int(correct) / int(fraud_mask.sum())\nprint(f'recall: {acc:.4f}')\n\nrecall: 0.9619\n\n\n\n위의 recall은 test가 없어서 train으로만 했던 거..!"
  },
  {
    "objectID": "posts/231002 데이터(15, df50 정리2).html",
    "href": "posts/231002 데이터(15, df50 정리2).html",
    "title": "[FRAUD] 데이터정리 시도(GCN_X범주)",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\nimport xgboost as xgb\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# gnn\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric\nfrom torch_geometric.nn import GCNConv\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_geometric/typing.py:18: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/libpyg.so: undefined symbol: _ZN2at4_ops12split_Tensor4callERKNS_6TensorEN3c106SymIntEl\n  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_geometric/typing.py:31: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_scatter/_scatter_cuda.so: undefined symbol: _ZNK3c107SymBool10guard_boolEPKcl\n  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_geometric/typing.py:42: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_sparse/_diag_cuda.so: undefined symbol: _ZN3c106detail19maybe_wrap_dim_slowIlEET_S2_S2_b\n  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n\n\n\ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef compute_time_difference(group):\n    n = len(group)\n    result = []\n    for i in range(n):\n        for j in range(n):\n            time_difference = abs(group.iloc[i].trans_date_trans_time.value - group.iloc[j].trans_date_trans_time.value)\n            result.append([group.iloc[i].name, group.iloc[j].name, time_difference])\n    return result\n\ndef mask(df):\n    df_tr,df_test = sklearn.model_selection.train_test_split(df, random_state=42)\n    N = len(df)\n    train_mask = [i in df_tr.index for i in range(N)]\n    test_mask = [i in df_test.index for i in range(N)]\n    train_mask = np.array(train_mask)\n    test_mask = np.array(test_mask)\n    return train_mask, test_mask\n\ndef edge_index_selected(edge_index):\n    theta = edge_index[:,2].mean()\n    edge_index[:,2] = (np.exp(-edge_index[:,2]/theta) != 1)*(np.exp(-edge_index[:,2]/theta))\n    edge_index = edge_index.tolist()\n    mean_ = np.array(edge_index)[:,2].mean()\n    selected_edges = [(int(row[0]), int(row[1])) for row in edge_index if row[2] &gt; mean_]\n    edge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\n    return edge_index_selected\n\n\n\n\nfraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\n\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50 = df50.reset_index()\ndf50.shape\n\n(12012, 23)\n\n\n\n\n\n\nmask(df50)\n\n(array([False,  True,  True, ...,  True, False,  True]),\n array([ True, False, False, ..., False,  True, False]))\n\n\n\ntrain_mask, test_mask = mask(df50)\n\n\n\n\n\n\n# groups = df50.groupby('cc_num')\n# edge_index_list_plus = [compute_time_difference(group) for _, group in groups]\n# edge_index_list_plus_flat = [item for sublist in edge_index_list_plus for item in sublist]\n# edge_index_list_plus_nparr = np.array(edge_index_list_plus_flat)\n# np.save('edge_index_list_plus50.npy', edge_index_list_plus_nparr)\n\n\nedge_index = np.load('edge_index_list_plus50.npy').astype(np.float64)\nedge_index.shape\n\n(200706, 3)\n\n\n\nedge_index_selected = edge_index_selected(edge_index)\n\n\n\n\n\n- x에 시간을 추가해 보자."
  },
  {
    "objectID": "posts/231002 데이터(15, df50 정리2).html#데이터정리",
    "href": "posts/231002 데이터(15, df50 정리2).html#데이터정리",
    "title": "[FRAUD] 데이터정리 시도(GCN_X범주)",
    "section": "",
    "text": "_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50 = df50.reset_index()\ndf50.shape\n\n(12012, 23)\n\n\n\n\n\n\nmask(df50)\n\n(array([False,  True,  True, ...,  True, False,  True]),\n array([ True, False, False, ..., False,  True, False]))\n\n\n\ntrain_mask, test_mask = mask(df50)\n\n\n\n\n\n\n# groups = df50.groupby('cc_num')\n# edge_index_list_plus = [compute_time_difference(group) for _, group in groups]\n# edge_index_list_plus_flat = [item for sublist in edge_index_list_plus for item in sublist]\n# edge_index_list_plus_nparr = np.array(edge_index_list_plus_flat)\n# np.save('edge_index_list_plus50.npy', edge_index_list_plus_nparr)\n\n\nedge_index = np.load('edge_index_list_plus50.npy').astype(np.float64)\nedge_index.shape\n\n(200706, 3)\n\n\n\nedge_index_selected = edge_index_selected(edge_index)\n\n\n\n\n\n- x에 시간을 추가해 보자."
  },
  {
    "objectID": "posts/231002 데이터(15, df50 정리2).html#분석-1gcn-amt",
    "href": "posts/231002 데이터(15, df50 정리2).html#분석-1gcn-amt",
    "title": "[FRAUD] 데이터정리 시도(GCN_X범주)",
    "section": "분석 1(GCN): amt",
    "text": "분석 1(GCN): amt\n\nx = torch.tensor(df50['amt'], dtype=torch.float).reshape(-1,1)\ny = torch.tensor(df50['is_fraud'],dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index = edge_index_selected, y=y, train_mask = train_mask, test_mask = test_mask)\ndata\n\n\n\ntorch.manual_seed(202250926)\nclass GCN2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 32)\n        self.conv2 = GCNConv(32,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\nX = (data.x[data.train_mask]).numpy()\nXX = (data.x[data.test_mask]).numpy()\ny = (data.y[data.train_mask]).numpy()\nyy = (data.y[data.test_mask]).numpy()\n\nmodel = GCN2()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(400):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\nmodel.eval()\n\npred = model(data).argmax(dim=1)\nyyhat = pred[data.test_mask]\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results1= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석1'])\n_results1\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석1\n0.902098\n0.862478\n0.95913\n0.90824\n\n\n\n\n\n\n\n\n\nb,W = list(model.conv1.parameters())\n\n\nb,W\n\n(Parameter containing:\n tensor([ 1.0399e+00, -2.5585e-01,  1.0566e-09, -4.7506e-03, -6.4289e-03,\n         -3.0055e-02,  3.6866e-01,  5.0163e-01,  1.0861e+00, -4.2169e-02,\n          2.1598e-07,  3.0382e-01, -1.5149e-01, -2.7623e-01,  6.8350e-01,\n         -1.5861e-03, -7.9415e-01, -7.1081e-03, -7.7902e-03,  6.8695e-01,\n         -1.5212e-01, -9.4722e-05, -8.9782e-01,  7.6822e-01, -5.0592e-02,\n          8.1692e-01, -5.2181e-02, -2.1817e-02, -9.1607e-01,  7.9294e-01,\n          1.0454e-09, -8.1069e-06], requires_grad=True),\n Parameter containing:\n tensor([[-4.8377e-03,  1.1379e-10],\n         [-1.4998e-01,  2.4131e-10],\n         [-2.1920e-01, -1.5126e-11],\n         [-2.2593e-01,  5.3614e-11],\n         [-3.0312e-01, -3.9329e-11],\n         [-1.4487e-01,  5.2859e-11],\n         [-4.1131e-02,  2.3188e-10],\n         [ 1.1339e-01,  2.6153e-11],\n         [-5.1434e-03,  1.2029e-10],\n         [-2.4512e-01,  4.0292e-11],\n         [-3.1688e-01,  4.6636e-11],\n         [ 5.1427e-02,  2.0959e-10],\n         [-2.6159e-01,  3.8738e-11],\n         [-2.7135e-02, -1.3194e-11],\n         [ 4.2538e-03,  1.4590e-10],\n         [-2.1030e-01, -1.7922e-10],\n         [ 2.4350e-02,  2.4147e-11],\n         [-2.5375e-01, -5.6721e-11],\n         [-2.4148e-01,  1.2034e-10],\n         [ 2.5918e-01,  7.0743e-11],\n         [-3.3576e-01, -7.5009e-11],\n         [-2.7123e-01, -3.5793e-11],\n         [ 1.7020e-01, -5.5290e-11],\n         [ 9.5707e-02, -8.3681e-11],\n         [-2.2416e-01, -4.0589e-11],\n         [ 3.9455e-02, -6.7173e-11],\n         [-3.2700e-01, -5.9575e-11],\n         [-3.1624e-01,  2.5560e-10],\n         [ 2.7330e-01, -2.0979e-10],\n         [ 1.3217e-01, -5.6469e-11],\n         [-1.9806e-01, -1.3773e-10],\n         [-1.7177e-01,  1.1836e-10]], requires_grad=True))"
  },
  {
    "objectID": "posts/231002 데이터(15, df50 정리2).html#분석-2gcn-amt-time",
    "href": "posts/231002 데이터(15, df50 정리2).html#분석-2gcn-amt-time",
    "title": "[FRAUD] 데이터정리 시도(GCN_X범주)",
    "section": "분석 2(GCN): amt, time",
    "text": "분석 2(GCN): amt, time\n\n\n\n# 'trans_date_trans_time' 열을 날짜/시간 형식으로 파싱\ndf50['trans_date_trans_time'] = pd.to_datetime(df50['trans_date_trans_time'])\n\n# Unix 타임스탬프로 변환\ndf50['trans_date_trans_time'] = (df50['trans_date_trans_time'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')\n\n# 나머지 코드는 이전과 동일하게 유지\nx = torch.tensor(df50[['amt', 'trans_date_trans_time']].values, dtype=torch.float)\ny = torch.tensor(df50['is_fraud'], dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index=edge_index_selected, y=y, train_mask=train_mask, test_mask=test_mask)\ndata\n\n\ntorch.manual_seed(202250926)\nclass GCN1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(data.num_node_features, 32)\n        self.conv2 = GCNConv(32,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\nX = (data.x[data.train_mask]).numpy()\nXX = (data.x[data.test_mask]).numpy()\ny = (data.y[data.train_mask]).numpy()\nyy = (data.y[data.test_mask]).numpy()\n\nmodel = GCN1()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.05, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(400):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\n\n    model.eval()\n\npred = model(data).argmax(dim=1)\nyyhat = pred[data.test_mask]\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results2= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석2'])\n_results2\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석2\n0.897436\n0.855798\n0.958471\n0.904229"
  },
  {
    "objectID": "posts/231002 데이터(15, df50 정리2).html#분석-3gcn-amt-category",
    "href": "posts/231002 데이터(15, df50 정리2).html#분석-3gcn-amt-category",
    "title": "[FRAUD] 데이터정리 시도(GCN_X범주)",
    "section": "분석 3(GCN): amt, category",
    "text": "분석 3(GCN): amt, category\n\nnp.count_nonzero(df50['category'].unique())\n\n14\n\n\n\n\n\ncategory_map = {category: index for index, category in enumerate(df50['category'].unique())}\ndf50['category'] = df50['category'].map(category_map)\n\n\nx = torch.tensor(df50[['amt', 'category']].values, dtype=torch.float)\ny = torch.tensor(df50['is_fraud'], dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index=edge_index_selected, y=y, train_mask=train_mask, test_mask=test_mask)\ndata\n\n\ntorch.manual_seed(202250926)\nclass GCN1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(data.num_node_features, 32)\n        self.conv2 = GCNConv(32,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\nX = (data.x[data.train_mask]).numpy()\nXX = (data.x[data.test_mask]).numpy()\ny = (data.y[data.train_mask]).numpy()\nyy = (data.y[data.test_mask]).numpy()\n\nmodel = GCN1()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.05, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(400):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\n\n    model.eval()\n\npred = model(data).argmax(dim=1)\nyyhat = pred[data.test_mask]\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results3= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석3'])\n_results3\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석3\n0.899767\n0.861905\n0.954515\n0.905849"
  },
  {
    "objectID": "posts/231002 데이터(15, df50 정리2).html#분석-4gcn-amt-distance",
    "href": "posts/231002 데이터(15, df50 정리2).html#분석-4gcn-amt-distance",
    "title": "[FRAUD] 데이터정리 시도(GCN_X범주)",
    "section": "분석 4(GCN): amt, distance",
    "text": "분석 4(GCN): amt, distance\n\ndef haversine(lat1, lon1, lat2, lon2):\n    # 지구의 반지름 (미터)\n    radius = 6371.0\n\n    # 라디안으로 변환\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n\n    # Haversine 공식 계산\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n    distance = radius * c\n\n    return distance\n\n# 데이터프레임(df50)에서 고객 위치 및 상점 위치의 위도와 경도 추출\ncustomer_lat = df50['lat']\ncustomer_lon = df50['long']\nstore_lat = df50['merch_lat']\nstore_lon = df50['merch_long']\n\n# 거리 계산\ndistances = haversine(customer_lat, customer_lon, store_lat, store_lon)\n\n# 거리를 데이터프레임에 추가\ndf50['distance_km'] = distances\n\n\n\nx = torch.tensor(df50[['amt', 'distance_km']].values, dtype=torch.float)\ny = torch.tensor(df50['is_fraud'], dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index=edge_index_selected, y=y, train_mask=train_mask, test_mask=test_mask)\ndata\n\n\ntorch.manual_seed(202250926)\nclass GCN1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(data.num_node_features, 32)\n        self.conv2 = GCNConv(32,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\nX = (data.x[data.train_mask]).numpy()\nXX = (data.x[data.test_mask]).numpy()\ny = (data.y[data.train_mask]).numpy()\nyy = (data.y[data.test_mask]).numpy()\n\nmodel = GCN1()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.05, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(400):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\n\n    model.eval()\n\npred = model(data).argmax(dim=1)\nyyhat = pred[data.test_mask]\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results4= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석4'])\n_results4\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석4\n0.913753\n0.866978\n0.979565\n0.919839"
  },
  {
    "objectID": "posts/231002 데이터(15, df50 정리2).html#분석-5gcn-amt-time-category",
    "href": "posts/231002 데이터(15, df50 정리2).html#분석-5gcn-amt-time-category",
    "title": "[FRAUD] 데이터정리 시도(GCN_X범주)",
    "section": "분석 5(GCN): amt, time, category",
    "text": "분석 5(GCN): amt, time, category\n\ncategory_map = {category: index for index, category in enumerate(df50['category'].unique())}\ndf50['category'] = df50['category'].map(category_map)\n\ndf50['trans_date_trans_time'] = pd.to_datetime(df50['trans_date_trans_time'])\ndf50['trans_date_trans_time'] = (df50['trans_date_trans_time'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')\n\nx = torch.tensor(df50[['amt', 'category', 'trans_date_trans_time']].values, dtype=torch.float)\ny = torch.tensor(df50['is_fraud'], dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index=edge_index_selected, y=y, train_mask=train_mask, test_mask=test_mask)\ndata\n\n\ntorch.manual_seed(202250926)\nclass GCN1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(data.num_node_features, 32)\n        self.conv2 = GCNConv(32,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\nX = (data.x[data.train_mask]).numpy()\nXX = (data.x[data.test_mask]).numpy()\ny = (data.y[data.train_mask]).numpy()\nyy = (data.y[data.test_mask]).numpy()\n\nmodel = GCN1()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.05, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(400):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\n\n    model.eval()\n\npred = model(data).argmax(dim=1)\nyyhat = pred[data.test_mask]\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results5= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석5'])\n_results5\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석5\n0.906094\n0.866904\n0.961767\n0.911875"
  },
  {
    "objectID": "posts/231002 데이터(15, df50 정리2).html#분석-6gcn-amt-time-distance",
    "href": "posts/231002 데이터(15, df50 정리2).html#분석-6gcn-amt-time-distance",
    "title": "[FRAUD] 데이터정리 시도(GCN_X범주)",
    "section": "분석 6(GCN): amt, time, distance",
    "text": "분석 6(GCN): amt, time, distance\n\n\n\ndf50['trans_date_trans_time'] = pd.to_datetime(df50['trans_date_trans_time'])\ndf50['trans_date_trans_time'] = (df50['trans_date_trans_time'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')\n\nx = torch.tensor(df50[['amt', 'trans_date_trans_time', 'distance_km']].values, dtype=torch.float)\ny = torch.tensor(df50['is_fraud'], dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index=edge_index_selected, y=y, train_mask=train_mask, test_mask=test_mask)\ndata\n\n\ntorch.manual_seed(202250926)\nclass GCN1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(data.num_node_features, 32)\n        self.conv2 = GCNConv(32,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\nX = (data.x[data.train_mask]).numpy()\nXX = (data.x[data.test_mask]).numpy()\ny = (data.y[data.train_mask]).numpy()\nyy = (data.y[data.test_mask]).numpy()\n\nmodel = GCN1()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.05, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(400):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\n\n    model.eval()\n\npred = model(data).argmax(dim=1)\nyyhat = pred[data.test_mask]\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results6= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석6'])\n_results6\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석6\n0.913753\n0.866978\n0.979565\n0.919839"
  },
  {
    "objectID": "posts/231002 데이터(15, df50 정리2).html#분석-7gcn-amt-category-distance",
    "href": "posts/231002 데이터(15, df50 정리2).html#분석-7gcn-amt-category-distance",
    "title": "[FRAUD] 데이터정리 시도(GCN_X범주)",
    "section": "분석 7(GCN): amt, category, distance",
    "text": "분석 7(GCN): amt, category, distance\n\n\nx = torch.tensor(df50[['amt', 'category', 'distance_km']].values, dtype=torch.float)\ny = torch.tensor(df50['is_fraud'], dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index=edge_index_selected, y=y, train_mask=train_mask, test_mask=test_mask)\ndata\n\n\ntorch.manual_seed(202250926)\nclass GCN1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(data.num_node_features, 32)\n        self.conv2 = GCNConv(32,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\nX = (data.x[data.train_mask]).numpy()\nXX = (data.x[data.test_mask]).numpy()\ny = (data.y[data.train_mask]).numpy()\nyy = (data.y[data.test_mask]).numpy()\n\nmodel = GCN1()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.05, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(400):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\n\n    model.eval()\n\npred = model(data).argmax(dim=1)\nyyhat = pred[data.test_mask]\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results7= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석7'])\n_results7\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석7\n0.914752\n0.868498\n0.979565\n0.920694"
  },
  {
    "objectID": "posts/231002 데이터(15, df50 정리2).html#분석-8gcn-amt-time-category-distance",
    "href": "posts/231002 데이터(15, df50 정리2).html#분석-8gcn-amt-time-category-distance",
    "title": "[FRAUD] 데이터정리 시도(GCN_X범주)",
    "section": "분석 8(GCN): amt, time, category, distance",
    "text": "분석 8(GCN): amt, time, category, distance\n\n\nx = torch.tensor(df50[['amt', 'trans_date_trans_time', 'category', 'distance_km']].values, dtype=torch.float)\ny = torch.tensor(df50['is_fraud'], dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index=edge_index_selected, y=y, train_mask=train_mask, test_mask=test_mask)\ndata\n\n\ntorch.manual_seed(202250926)\nclass GCN1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(data.num_node_features, 32)\n        self.conv2 = GCNConv(32,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\nX = (data.x[data.train_mask]).numpy()\nXX = (data.x[data.test_mask]).numpy()\ny = (data.y[data.train_mask]).numpy()\nyy = (data.y[data.test_mask]).numpy()\n\nmodel = GCN1()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.05, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(400):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\n\n    model.eval()\n\npred = model(data).argmax(dim=1)\nyyhat = pred[data.test_mask]\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results8= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석8'])\n_results8\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석8\n0.91342\n0.861415\n0.987475\n0.920147"
  },
  {
    "objectID": "posts/231106 mean보다 작은거선택.html",
    "href": "posts/231106 mean보다 작은거선택.html",
    "title": "[FRAUD] : <mean_",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\nimport xgboost as xgb\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# gnn\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric\nfrom torch_geometric.nn import GCNConv\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_geometric/typing.py:18: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/libpyg.so: undefined symbol: _ZN2at4_ops12split_Tensor4callERKNS_6TensorEN3c106SymIntEl\n  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_geometric/typing.py:31: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_scatter/_scatter_cuda.so: undefined symbol: _ZNK3c107SymBool10guard_boolEPKcl\n  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_geometric/typing.py:42: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_sparse/_diag_cuda.so: undefined symbol: _ZN3c106detail19maybe_wrap_dim_slowIlEET_S2_S2_b\n  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n\n\n\ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef compute_time_difference(group):\n    n = len(group)\n    result = []\n    for i in range(n):\n        for j in range(n):\n            time_difference = abs(group.iloc[i].trans_date_trans_time.value - group.iloc[j].trans_date_trans_time.value)\n            result.append([group.iloc[i].name, group.iloc[j].name, time_difference])\n    return result\n\ndef mask(df):\n    df_tr,df_test = sklearn.model_selection.train_test_split(df, random_state=42)\n    N = len(df)\n    train_mask = [i in df_tr.index for i in range(N)]\n    test_mask = [i in df_test.index for i in range(N)]\n    train_mask = np.array(train_mask)\n    test_mask = np.array(test_mask)\n    return train_mask, test_mask\n\n\n\n\nfraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\n\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50 = df50.reset_index()\ndf50.shape\n\n(12012, 23)\n\n\n\n\n\n\nmask(df50)\n\n(array([False,  True,  True, ...,  True, False,  True]),\n array([ True, False, False, ..., False,  True, False]))\n\n\n\ntrain_mask, test_mask = mask(df50)\n\n\n\n\n\n\n\n\n\n\n# groups = df50.groupby('cc_num')\n# edge_index_list_plus = [compute_time_difference(group) for _, group in groups]\n# edge_index_list_plus_flat = [item for sublist in edge_index_list_plus for item in sublist]\n# edge_index_list_plus_nparr = np.array(edge_index_list_plus_flat)\n# np.save('edge_index_list_plus50.npy', edge_index_list_plus_nparr)\n\n\nedge_index = np.load('edge_index_list_plus50.npy').astype(np.float64)\nedge_index.shape\n\n(200706, 3)\n\n\n\n\n\n\n\ndef edge_index_selected(edge_index):\n    theta = edge_index[:,2].mean()\n    edge_index[:,2] = (np.exp(-edge_index[:,2]/theta) != 1)*(np.exp(-edge_index[:,2]/theta))\n    edge_index = edge_index.tolist()\n    mean_ = np.array(edge_index)[:,2].mean()\n    selected_edges = [(int(row[0]), int(row[1])) for row in edge_index if row[2] &lt; mean_]\n    edge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\n    return edge_index_selected\n\nedge_index_selected = edge_index_selected(edge_index)\n\n\nx = torch.tensor(df50['amt'], dtype=torch.float).reshape(-1,1)\ny = torch.tensor(df50['is_fraud'],dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index = edge_index_selected, y=y, train_mask = train_mask, test_mask = test_mask)\ndata\n\nData(x=[12012, 1], edge_index=[2, 106976], y=[12012], train_mask=[12012], test_mask=[12012])\n\n\n\n\ntorch.manual_seed(202250926)\n\nclass GCN1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 32)\n        self.conv2 = GCNConv(32,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\nX = (data.x[data.train_mask]).numpy()\nXX = (data.x[data.test_mask]).numpy()\ny = (data.y[data.train_mask]).numpy()\nyy = (data.y[data.test_mask]).numpy()\n\nmodel = GCN1()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(400):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\nmodel.eval()\n\npred = model(data).argmax(dim=1)\nyyhat = pred[data.test_mask]\n\npred\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results1= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석1'])\n_results1\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석1\n0.8668\n0.851479\n0.891892\n0.871217"
  },
  {
    "objectID": "posts/231106 mean보다 작은거선택.html#데이터정리",
    "href": "posts/231106 mean보다 작은거선택.html#데이터정리",
    "title": "[FRAUD] : <mean_",
    "section": "",
    "text": "_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50 = df50.reset_index()\ndf50.shape\n\n(12012, 23)\n\n\n\n\n\n\nmask(df50)\n\n(array([False,  True,  True, ...,  True, False,  True]),\n array([ True, False, False, ..., False,  True, False]))\n\n\n\ntrain_mask, test_mask = mask(df50)"
  },
  {
    "objectID": "posts/231106 mean보다 작은거선택.html#분석-1gcn",
    "href": "posts/231106 mean보다 작은거선택.html#분석-1gcn",
    "title": "[FRAUD] : <mean_",
    "section": "",
    "text": "# groups = df50.groupby('cc_num')\n# edge_index_list_plus = [compute_time_difference(group) for _, group in groups]\n# edge_index_list_plus_flat = [item for sublist in edge_index_list_plus for item in sublist]\n# edge_index_list_plus_nparr = np.array(edge_index_list_plus_flat)\n# np.save('edge_index_list_plus50.npy', edge_index_list_plus_nparr)\n\n\nedge_index = np.load('edge_index_list_plus50.npy').astype(np.float64)\nedge_index.shape\n\n(200706, 3)\n\n\n\n\n\n\n\ndef edge_index_selected(edge_index):\n    theta = edge_index[:,2].mean()\n    edge_index[:,2] = (np.exp(-edge_index[:,2]/theta) != 1)*(np.exp(-edge_index[:,2]/theta))\n    edge_index = edge_index.tolist()\n    mean_ = np.array(edge_index)[:,2].mean()\n    selected_edges = [(int(row[0]), int(row[1])) for row in edge_index if row[2] &lt; mean_]\n    edge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\n    return edge_index_selected\n\nedge_index_selected = edge_index_selected(edge_index)\n\n\nx = torch.tensor(df50['amt'], dtype=torch.float).reshape(-1,1)\ny = torch.tensor(df50['is_fraud'],dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index = edge_index_selected, y=y, train_mask = train_mask, test_mask = test_mask)\ndata\n\nData(x=[12012, 1], edge_index=[2, 106976], y=[12012], train_mask=[12012], test_mask=[12012])\n\n\n\n\ntorch.manual_seed(202250926)\n\nclass GCN1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 32)\n        self.conv2 = GCNConv(32,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\nX = (data.x[data.train_mask]).numpy()\nXX = (data.x[data.test_mask]).numpy()\ny = (data.y[data.train_mask]).numpy()\nyy = (data.y[data.test_mask]).numpy()\n\nmodel = GCN1()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(400):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\nmodel.eval()\n\npred = model(data).argmax(dim=1)\nyyhat = pred[data.test_mask]\n\npred\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results1= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석1'])\n_results1\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석1\n0.8668\n0.851479\n0.891892\n0.871217"
  },
  {
    "objectID": "posts/230818 데이터(4, df50_com으로 93퍼 accuracy).html",
    "href": "posts/230818 데이터(4, df50_com으로 93퍼 accuracy).html",
    "title": "[FRAUD] 데이터정리 시도(8.18-df50com으로 93퍼 accuracy)",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\nimport torch\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics \n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부     \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\ndef build_graph_tripartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n                                                       df[\"cc_num\"].values.tolist() +\n                                                       df[\"merchant\"].values.tolist()))}\n    df[\"in_node\"]= df[\"cc_num\"].apply(lambda x: mapping[x])\n    df[\"out_node\"]=df[\"merchant\"].apply(lambda x:mapping[x])\n    \n        \n    G=nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\\\n                        [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")     \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")   \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")  \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n\n    return G\n    \n    \ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef embedding(Graph):\n    # Graph -&gt; X (feature)\n    _edgs = list(Graph.edges)\n    subGraph = Graph.edge_subgraph([_edgs[x] for x in range(len(Graph.edges))]).copy()\n    subGraph.add_nodes_from(list(set(Graph.nodes) - set(subGraph.nodes)))    \n    embedded = AverageEmbedder(Node2Vec(subGraph, weight_key='weight').fit(window=10).wv)\n    X = [embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in range(len(Graph.edges))]\n    # Graph -&gt; y (label)\n    y = np.array(list(nx.get_edge_attributes(Graph, \"label\").values()))\n    return X,y \n\ndef anal(df):\n    Graph = build_graph_bipartite(df)\n    X,XX,y,yy = embedding(Graph)\n    lrnr = RandomForestClassifier(n_estimators=100, random_state=42) \n    lrnr.fit(X,y)\n    yyhat = lrnr.predict(XX)\n    df = pd.DataFrame({\n        'acc':[sklearn.metrics.accuracy_score(yy,yyhat)], \n        'pre':[sklearn.metrics.precision_score(yy,yyhat)], \n        'rec':[sklearn.metrics.recall_score(yy,yyhat)],\n        'f1':[sklearn.metrics.f1_score(yy,yyhat)]}\n    )    \n    return df\n\ndef our_sampling1(df):\n    cus_list = set(df.query('is_fraud==1').cc_num.tolist())\n    return df.query(\"cc_num in @ cus_list\")\n\n\nfraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\n\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\n12012*12012\n\n144288144\n\n\n\ndf50 = df50.reset_index()\n\n\nN = len(df50)\n\n\n\n\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.is_fraud.mean().round(5), df50_test.is_fraud.mean().round(5)\n\n(0.49828, 0.50516)\n\n\n\ndf50_tr.shape, df50_test.shape\n\n((9009, 23), (3003, 23))\n\n\n\ntrain_mask = np.concatenate((np.full(9009, True), np.full(3003, False)))\ntest_mask = np.concatenate((np.full(9009, False), np.full(3003, True)))\nprint(\"Train Mask:\", train_mask)\nprint(\"Test Mask:\", test_mask)\n\nTrain Mask: [ True  True  True ... False False False]\nTest Mask: [False False False ...  True  True  True]\n\n\n\ntrain_mask.shape, test_mask.shape\n\n((12012,), (12012,))\n\n\n\ntrain_mask.sum(), test_mask.sum()\n\n(9009, 3003)\n\n\n\ndf50_com = pd.concat([df50_tr, df50_test])\n\n\ndf50_com = df50_com.reset_index()\n\n\n\n\n\n\n# edge_index_list2_com = []\n# for i in range(N):\n#     for j in range(N):\n#         if df50_com['cc_num'][i] != df50_com['cc_num'][j]:  \n#             edge = 0\n#         else:\n#             edge = 1\n#         edge_index_list2_com.append([i, j, edge])\n\n\n# #np.save('edge_index_list2_50_com.npy', edge_index_list2_com)\n\n# edge_index_list2_com = np.load('edge_index_list2_50_com.npy')\n\n\n# edge_index_list2_com\n\narray([[    0,     0,     1],\n       [    0,     1,     0],\n       [    0,     2,     0],\n       ...,\n       [12011, 12009,     0],\n       [12011, 12010,     0],\n       [12011, 12011,     1]])\n\n\n\n# edge_one_com = [(i, j) for i, j, edge in edge_index_list2_com if edge == 1]\n# edge_one_com[:5]\n\n\n# len(edge_one_com)\n\n\n# edge_one_index_com = torch.tensor(edge_one_com, dtype=torch.long).t()\n\n\n# edge_one_index_com.shape\n\n\n\n\n\n\n# edge_index_list = []\n# for i in range(N):\n#     for j in range(N):\n#         time_difference = (df50_com['trans_date_trans_time'][i] - df50_com['trans_date_trans_time'][j]).total_seconds()\n#         edge_index_list.append([i, j, time_difference])\n\n\n# # np.save('edge_index_list_50_com.npy', edge_index_list)\n\n# edge_index_list2_com = np.load('edge_index_list2_50_com.npy')\n\n\n# edge_index_list[:5]\n\n[[0, 0, 0.0],\n [0, 1, -20301900.0],\n [0, 2, -28413960.0],\n [0, 3, -23837880.0],\n [0, 4, -26877960.0]]\n\n\n\n# edge_index = np.array(edge_index_list)\n# edge_index[:,2] = np.abs(edge_index[:,2])\n# theta = edge_index[:,2].mean()\n# theta\n\n12238996.895508753\n\n\n\n# edge_index[:,2] = (np.exp(-edge_index[:,2]/theta)!=1) * np.exp(-edge_index[:,2]/theta)\n# edge_index\n\narray([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 1.00000000e+00, 1.90369587e-01],\n       [0.00000000e+00, 2.00000000e+00, 9.81172367e-02],\n       ...,\n       [1.20110000e+04, 1.20090000e+04, 9.25720620e-01],\n       [1.20110000e+04, 1.20100000e+04, 5.15585903e-01],\n       [1.20110000e+04, 1.20110000e+04, 0.00000000e+00]])\n\n\n\n# edge_index[:,2]\n\narray([0.        , 0.19036959, 0.09811724, ..., 0.92572062, 0.5155859 ,\n       0.        ])\n\n\n\n갑자기 헷갈리는데, 이 weight는 어디서 곱하는거지? GNN이 아니라 새로운 코드였던감..\n\n\n\n# edge_index_list_plus = []\n# for i in range(N):\n#     for j in range(N):\n#         if df50_com['cc_num'][i] != df50_com['cc_num'][j]:  # cc_num 값이 다르다면\n#             time_difference = 0\n#         else:\n#             time_difference = (df50_com['trans_date_trans_time'][i] - df50_com['trans_date_trans_time'][j]).total_seconds()\n#         edge_index_list_plus.append([i, j, time_difference])\n\n\n# np.save('edge_index_list_plus.npy', edge_index_list_plus)\n\nedge_index_list_plus = np.load('edge_index_list_plus.npy')\n\n\nedge_index = np.array(edge_index_list_plus)\n\n\nedge_index.shape\n\n(144288144, 3)\n\n\n\nedge_index\n\narray([[0.0000e+00, 0.0000e+00, 0.0000e+00],\n       [0.0000e+00, 1.0000e+00, 0.0000e+00],\n       [0.0000e+00, 2.0000e+00, 0.0000e+00],\n       ...,\n       [1.2011e+04, 1.2009e+04, 0.0000e+00],\n       [1.2011e+04, 1.2010e+04, 0.0000e+00],\n       [1.2011e+04, 1.2011e+04, 0.0000e+00]])\n\n\n\nedge_index[:,2] = np.abs(edge_index[:,2])\ntheta = edge_index[:,2].mean()\ntheta\n\n10973.519989002007\n\n\n\nedge_index[:,2] = (np.exp(-edge_index[:,2]/theta)!=1) * np.exp(-edge_index[:,2]/theta)\nedge_index\n\narray([[0.0000e+00, 0.0000e+00, 0.0000e+00],\n       [0.0000e+00, 1.0000e+00, 0.0000e+00],\n       [0.0000e+00, 2.0000e+00, 0.0000e+00],\n       ...,\n       [1.2011e+04, 1.2009e+04, 0.0000e+00],\n       [1.2011e+04, 1.2010e+04, 0.0000e+00],\n       [1.2011e+04, 1.2011e+04, 0.0000e+00]])\n\n\n\nedge_index_list_updated = edge_index.tolist()\nnp.array(edge_index_list_updated)[:,2].mean()\n\n8.443606280313275e-05\n\n\n\nmm = np.array(edge_index_list_updated)[:,2].mean()\n\n\n시간이 평균보다 짧다면? . 음..\n\n\nselected_edges = [(int(row[0]), int(row[1])) for row in edge_index_list_updated if row[2] &gt; mm]\n\n\nedge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\n\n\nedge_index_selected.shape\n\ntorch.Size([2, 51392])\n\n\n\n\n\n\n\nx = df50_com['amt']\n\n\na = torch.tensor(x, dtype=torch.float)\n\n\na = a.reshape(-1,1)\na\n\ntensor([[921.2400],\n        [698.2800],\n        [220.5600],\n        ...,\n        [ 17.9700],\n        [  7.5800],\n        [824.9900]])\n\n\n\ny = df50_com['is_fraud']\n\n\nb = torch.tensor(y,dtype=torch.int64)\n\n\nb\n\ntensor([1, 1, 0,  ..., 1, 0, 1])\n\n\n\nimport torch_geometric\n\n\ndata = torch_geometric.data.Data(x=a, edge_index = edge_index_selected, y=b, train_mask = train_mask, test_mask = test_mask)\n\n\ndata\n\nData(x=[12012, 1], edge_index=[2, 51392], y=[12012], train_mask=[12012], test_mask=[12012])\n\n\n\n\n\n\n\n\n\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\n\nclass GCN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 16)\n        self.conv2 = GCNConv(16,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\n\nmodel = GCN()\n\n\nmodel\n\nGCN(\n  (conv1): GCNConv(1, 16)\n  (conv2): GCNConv(16, 2)\n)\n\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\n\nGCN(\n  (conv1): GCNConv(1, 16)\n  (conv2): GCNConv(16, 2)\n)\n\n\n\n\nfor epoch in range(200):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\n\n\nmodel.eval()\npred = model(data).argmax(dim=1)\ncorrect = (pred[data.test_mask] == data.y[data.test_mask]).sum()\nacc = int(correct) / int(data.test_mask.sum())\nprint(f'Accuracy: {acc:.4f}')\n\nAccuracy: 0.9321\n\n\n\n`"
  },
  {
    "objectID": "posts/230818 데이터(4, df50_com으로 93퍼 accuracy).html#시도",
    "href": "posts/230818 데이터(4, df50_com으로 93퍼 accuracy).html#시도",
    "title": "[FRAUD] 데이터정리 시도(8.18-df50com으로 93퍼 accuracy)",
    "section": "",
    "text": "_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\n12012*12012\n\n144288144\n\n\n\ndf50 = df50.reset_index()\n\n\nN = len(df50)\n\n\n\n\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.is_fraud.mean().round(5), df50_test.is_fraud.mean().round(5)\n\n(0.49828, 0.50516)\n\n\n\ndf50_tr.shape, df50_test.shape\n\n((9009, 23), (3003, 23))\n\n\n\ntrain_mask = np.concatenate((np.full(9009, True), np.full(3003, False)))\ntest_mask = np.concatenate((np.full(9009, False), np.full(3003, True)))\nprint(\"Train Mask:\", train_mask)\nprint(\"Test Mask:\", test_mask)\n\nTrain Mask: [ True  True  True ... False False False]\nTest Mask: [False False False ...  True  True  True]\n\n\n\ntrain_mask.shape, test_mask.shape\n\n((12012,), (12012,))\n\n\n\ntrain_mask.sum(), test_mask.sum()\n\n(9009, 3003)\n\n\n\ndf50_com = pd.concat([df50_tr, df50_test])\n\n\ndf50_com = df50_com.reset_index()\n\n\n\n\n\n\n# edge_index_list2_com = []\n# for i in range(N):\n#     for j in range(N):\n#         if df50_com['cc_num'][i] != df50_com['cc_num'][j]:  \n#             edge = 0\n#         else:\n#             edge = 1\n#         edge_index_list2_com.append([i, j, edge])\n\n\n# #np.save('edge_index_list2_50_com.npy', edge_index_list2_com)\n\n# edge_index_list2_com = np.load('edge_index_list2_50_com.npy')\n\n\n# edge_index_list2_com\n\narray([[    0,     0,     1],\n       [    0,     1,     0],\n       [    0,     2,     0],\n       ...,\n       [12011, 12009,     0],\n       [12011, 12010,     0],\n       [12011, 12011,     1]])\n\n\n\n# edge_one_com = [(i, j) for i, j, edge in edge_index_list2_com if edge == 1]\n# edge_one_com[:5]\n\n\n# len(edge_one_com)\n\n\n# edge_one_index_com = torch.tensor(edge_one_com, dtype=torch.long).t()\n\n\n# edge_one_index_com.shape\n\n\n\n\n\n\n# edge_index_list = []\n# for i in range(N):\n#     for j in range(N):\n#         time_difference = (df50_com['trans_date_trans_time'][i] - df50_com['trans_date_trans_time'][j]).total_seconds()\n#         edge_index_list.append([i, j, time_difference])\n\n\n# # np.save('edge_index_list_50_com.npy', edge_index_list)\n\n# edge_index_list2_com = np.load('edge_index_list2_50_com.npy')\n\n\n# edge_index_list[:5]\n\n[[0, 0, 0.0],\n [0, 1, -20301900.0],\n [0, 2, -28413960.0],\n [0, 3, -23837880.0],\n [0, 4, -26877960.0]]\n\n\n\n# edge_index = np.array(edge_index_list)\n# edge_index[:,2] = np.abs(edge_index[:,2])\n# theta = edge_index[:,2].mean()\n# theta\n\n12238996.895508753\n\n\n\n# edge_index[:,2] = (np.exp(-edge_index[:,2]/theta)!=1) * np.exp(-edge_index[:,2]/theta)\n# edge_index\n\narray([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 1.00000000e+00, 1.90369587e-01],\n       [0.00000000e+00, 2.00000000e+00, 9.81172367e-02],\n       ...,\n       [1.20110000e+04, 1.20090000e+04, 9.25720620e-01],\n       [1.20110000e+04, 1.20100000e+04, 5.15585903e-01],\n       [1.20110000e+04, 1.20110000e+04, 0.00000000e+00]])\n\n\n\n# edge_index[:,2]\n\narray([0.        , 0.19036959, 0.09811724, ..., 0.92572062, 0.5155859 ,\n       0.        ])\n\n\n\n갑자기 헷갈리는데, 이 weight는 어디서 곱하는거지? GNN이 아니라 새로운 코드였던감..\n\n\n\n# edge_index_list_plus = []\n# for i in range(N):\n#     for j in range(N):\n#         if df50_com['cc_num'][i] != df50_com['cc_num'][j]:  # cc_num 값이 다르다면\n#             time_difference = 0\n#         else:\n#             time_difference = (df50_com['trans_date_trans_time'][i] - df50_com['trans_date_trans_time'][j]).total_seconds()\n#         edge_index_list_plus.append([i, j, time_difference])\n\n\n# np.save('edge_index_list_plus.npy', edge_index_list_plus)\n\nedge_index_list_plus = np.load('edge_index_list_plus.npy')\n\n\nedge_index = np.array(edge_index_list_plus)\n\n\nedge_index.shape\n\n(144288144, 3)\n\n\n\nedge_index\n\narray([[0.0000e+00, 0.0000e+00, 0.0000e+00],\n       [0.0000e+00, 1.0000e+00, 0.0000e+00],\n       [0.0000e+00, 2.0000e+00, 0.0000e+00],\n       ...,\n       [1.2011e+04, 1.2009e+04, 0.0000e+00],\n       [1.2011e+04, 1.2010e+04, 0.0000e+00],\n       [1.2011e+04, 1.2011e+04, 0.0000e+00]])\n\n\n\nedge_index[:,2] = np.abs(edge_index[:,2])\ntheta = edge_index[:,2].mean()\ntheta\n\n10973.519989002007\n\n\n\nedge_index[:,2] = (np.exp(-edge_index[:,2]/theta)!=1) * np.exp(-edge_index[:,2]/theta)\nedge_index\n\narray([[0.0000e+00, 0.0000e+00, 0.0000e+00],\n       [0.0000e+00, 1.0000e+00, 0.0000e+00],\n       [0.0000e+00, 2.0000e+00, 0.0000e+00],\n       ...,\n       [1.2011e+04, 1.2009e+04, 0.0000e+00],\n       [1.2011e+04, 1.2010e+04, 0.0000e+00],\n       [1.2011e+04, 1.2011e+04, 0.0000e+00]])\n\n\n\nedge_index_list_updated = edge_index.tolist()\nnp.array(edge_index_list_updated)[:,2].mean()\n\n8.443606280313275e-05\n\n\n\nmm = np.array(edge_index_list_updated)[:,2].mean()\n\n\n시간이 평균보다 짧다면? . 음..\n\n\nselected_edges = [(int(row[0]), int(row[1])) for row in edge_index_list_updated if row[2] &gt; mm]\n\n\nedge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\n\n\nedge_index_selected.shape\n\ntorch.Size([2, 51392])\n\n\n\n\n\n\n\nx = df50_com['amt']\n\n\na = torch.tensor(x, dtype=torch.float)\n\n\na = a.reshape(-1,1)\na\n\ntensor([[921.2400],\n        [698.2800],\n        [220.5600],\n        ...,\n        [ 17.9700],\n        [  7.5800],\n        [824.9900]])\n\n\n\ny = df50_com['is_fraud']\n\n\nb = torch.tensor(y,dtype=torch.int64)\n\n\nb\n\ntensor([1, 1, 0,  ..., 1, 0, 1])\n\n\n\nimport torch_geometric\n\n\ndata = torch_geometric.data.Data(x=a, edge_index = edge_index_selected, y=b, train_mask = train_mask, test_mask = test_mask)\n\n\ndata\n\nData(x=[12012, 1], edge_index=[2, 51392], y=[12012], train_mask=[12012], test_mask=[12012])\n\n\n\n\n\n\n\n\n\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\n\nclass GCN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 16)\n        self.conv2 = GCNConv(16,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\n\nmodel = GCN()\n\n\nmodel\n\nGCN(\n  (conv1): GCNConv(1, 16)\n  (conv2): GCNConv(16, 2)\n)\n\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\n\nGCN(\n  (conv1): GCNConv(1, 16)\n  (conv2): GCNConv(16, 2)\n)\n\n\n\n\nfor epoch in range(200):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\n\n\nmodel.eval()\npred = model(data).argmax(dim=1)\ncorrect = (pred[data.test_mask] == data.y[data.test_mask]).sum()\nacc = int(correct) / int(data.test_mask.sum())\nprint(f'Accuracy: {acc:.4f}')\n\nAccuracy: 0.9321\n\n\n\n`"
  },
  {
    "objectID": "posts/230822 데이터(5, matrix로 ls6시도해보기..실패).html",
    "href": "posts/230822 데이터(5, matrix로 ls6시도해보기..실패).html",
    "title": "[FRAUD] 데이터정리 시도(matrix로 lesson6따라하기 - 실패ㅎ)",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\nimport torch\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics \n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부     \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\ndef build_graph_tripartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n                                                       df[\"cc_num\"].values.tolist() +\n                                                       df[\"merchant\"].values.tolist()))}\n    df[\"in_node\"]= df[\"cc_num\"].apply(lambda x: mapping[x])\n    df[\"out_node\"]=df[\"merchant\"].apply(lambda x:mapping[x])\n    \n        \n    G=nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\\\n                        [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")     \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")   \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")  \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n\n    return G\n    \n    \ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef embedding(Graph):\n    # Graph -&gt; X (feature)\n    _edgs = list(Graph.edges)\n    subGraph = Graph.edge_subgraph([_edgs[x] for x in range(len(Graph.edges))]).copy()\n    subGraph.add_nodes_from(list(set(Graph.nodes) - set(subGraph.nodes)))    \n    embedded = AverageEmbedder(Node2Vec(subGraph, weight_key='weight').fit(window=10).wv)\n    X = [embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in range(len(Graph.edges))]\n    # Graph -&gt; y (label)\n    y = np.array(list(nx.get_edge_attributes(Graph, \"label\").values()))\n    return X,y \n\ndef anal(df):\n    Graph = build_graph_bipartite(df)\n    X,XX,y,yy = embedding(Graph)\n    lrnr = RandomForestClassifier(n_estimators=100, random_state=42) \n    lrnr.fit(X,y)\n    yyhat = lrnr.predict(XX)\n    df = pd.DataFrame({\n        'acc':[sklearn.metrics.accuracy_score(yy,yyhat)], \n        'pre':[sklearn.metrics.precision_score(yy,yyhat)], \n        'rec':[sklearn.metrics.recall_score(yy,yyhat)],\n        'f1':[sklearn.metrics.f1_score(yy,yyhat)]}\n    )    \n    return df\n\ndef our_sampling1(df):\n    cus_list = set(df.query('is_fraud==1').cc_num.tolist())\n    return df.query(\"cc_num in @ cus_list\")\n\n\nfraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\n\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\n12012*12012\n\n144288144\n\n\n\ndf50 = df50.reset_index()\n\n\nN = len(df50)\n\n\n\n\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.is_fraud.mean().round(5), df50_test.is_fraud.mean().round(5)\n\n(0.49828, 0.50516)\n\n\n\ndf50_tr.shape, df50_test.shape\n\n((9009, 23), (3003, 23))\n\n\n\ntrain_mask = np.concatenate((np.full(9009, True), np.full(3003, False)))\ntest_mask = np.concatenate((np.full(9009, False), np.full(3003, True)))\nprint(\"Train Mask:\", train_mask)\nprint(\"Test Mask:\", test_mask)\n\nTrain Mask: [ True  True  True ... False False False]\nTest Mask: [False False False ...  True  True  True]\n\n\n\ntrain_mask.shape, test_mask.shape\n\n((12012,), (12012,))\n\n\n\ntrain_mask.sum(), test_mask.sum()\n\n(9009, 3003)\n\n\n\ndf50_com = pd.concat([df50_tr, df50_test])\n\n\ndf50_com = df50_com.reset_index()\n\n\n\n\n\n\n# edge_index_list2_com = []\n# for i in range(N):\n#     for j in range(N):\n#         if df50_com['cc_num'][i] != df50_com['cc_num'][j]:  \n#             edge = 0\n#         else:\n#             edge = 1\n#         edge_index_list2_com.append([i, j, edge])\n\n\n#np.save('edge_index_list2_50_com.npy', edge_index_list2_com)\n\nedge_index_list2_com = np.load('edge_index_list2_50_com.npy')\n\n\n# edge_index_list2_com\n\narray([[    0,     0,     1],\n       [    0,     1,     0],\n       [    0,     2,     0],\n       ...,\n       [12011, 12009,     0],\n       [12011, 12010,     0],\n       [12011, 12011,     1]])\n\n\n\nedge_index_list2_com.shape\n\n(144288144, 3)\n\n\n\nnum_nodes = 12012\n\n\naj_matrix = np.zeros((num_nodes, num_nodes))\n\n\naj_matrix\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])\n\n\n\nfor i, j ,edge in edge_index_list2_com:\n    aj_matrix[i][j] = edge\n\n\naj_matrix\n\narray([[1., 0., 0., ..., 0., 0., 0.],\n       [0., 1., 0., ..., 0., 0., 0.],\n       [0., 0., 1., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 1., 0., 0.],\n       [0., 0., 0., ..., 0., 1., 0.],\n       [0., 0., 0., ..., 0., 0., 1.]])\n\n\n\n# aj_matrix.shape\n\n(12012, 12012)\n\n\n\n# np.save('aj_matrix.npy', aj_matrix)\n\n\n\n# aj_matrix = np.load('aj_matrix.npy')\n\n\n# aj_matrix\n\narray([[1., 0., 0., ..., 0., 0., 0.],\n       [0., 1., 0., ..., 0., 0., 0.],\n       [0., 0., 1., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 1., 0., 0.],\n       [0., 0., 0., ..., 0., 1., 0.],\n       [0., 0., 0., ..., 0., 0., 1.]])\n\n\n\n\n\n\n\n\n# edge_index_list = []\n# for i in range(N):\n#     for j in range(N):\n#         time_difference = (df50_com['trans_date_trans_time'][i] - df50_com['trans_date_trans_time'][j]).total_seconds()\n#         edge_index_list.append([i, j, time_difference])\n\n\n# np.save('edge_index_list_50_com.npy', edge_index_list)\n\nedge_index_list = np.load('edge_index_list_50_com.npy')\n\n\nedge_index_list[:5]\n\narray([[ 0.000000e+00,  0.000000e+00,  0.000000e+00],\n       [ 0.000000e+00,  1.000000e+00, -2.030190e+07],\n       [ 0.000000e+00,  2.000000e+00, -2.841396e+07],\n       [ 0.000000e+00,  3.000000e+00, -2.383788e+07],\n       [ 0.000000e+00,  4.000000e+00, -2.687796e+07]])\n\n\n\nedge_index = np.array(edge_index_list)\nedge_index[:,2] = np.abs(edge_index[:,2])\ntheta = edge_index[:,2].mean()\ntheta\n\n12238996.895508753\n\n\n\nedge_index[:,2] = (np.exp(-edge_index[:,2]/theta)!=1) * np.exp(-edge_index[:,2]/theta)\nedge_index\n\narray([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 1.00000000e+00, 1.90369587e-01],\n       [0.00000000e+00, 2.00000000e+00, 9.81172367e-02],\n       ...,\n       [1.20110000e+04, 1.20090000e+04, 9.25720620e-01],\n       [1.20110000e+04, 1.20100000e+04, 5.15585903e-01],\n       [1.20110000e+04, 1.20110000e+04, 0.00000000e+00]])\n\n\n\nw_matrix로 바꾸려고 하니까 형식이 [i][j]가 맞지 않는다.!\n\n\n# # 출력 형식 변경\n# np.set_printoptions(formatter={'int': '{:d}'.format})\n\n\n# # 원래 출력 형식으로 복원\n# np.set_printoptions(formatter=None)\n\n\nw_matrix = np.zeros((num_nodes, num_nodes))\n\n\nw_matrix\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])\n\n\n\nfor i, j ,time_difference in edge_index:\n    i, j = int(i), int(j)\n    w_matrix[i][j] = time_difference\n\n\nw_matrix\n\narray([[0.        , 0.19036959, 0.09811724, ..., 0.29671829, 0.14162023,\n        0.27467824],\n       [0.19036959, 0.        , 0.51540395, ..., 0.6415836 , 0.74392254,\n        0.69306396],\n       [0.09811724, 0.51540395, 0.        , ..., 0.33067472, 0.69281937,\n        0.3572079 ],\n       ...,\n       [0.29671829, 0.6415836 , 0.33067472, ..., 0.        , 0.4772885 ,\n        0.92572062],\n       [0.14162023, 0.74392254, 0.69281937, ..., 0.4772885 , 0.        ,\n        0.5155859 ],\n       [0.27467824, 0.69306396, 0.3572079 , ..., 0.92572062, 0.5155859 ,\n        0.        ]])\n\n\n\nw_matrix.shape\n\n(12012, 12012)\n\n\n\nnp.save('w_matrix.npy', w_matrix)\n\n\n\n# np.save('edge_index_list_plus.npy', edge_index_list_plus)\n\nedge_index_list_plus = np.load('edge_index_list_plus.npy')\n\n\nedge_index = np.array(edge_index_list_plus)\n\n\nedge_index.shape\n\n(144288144, 3)\n\n\n\nedge_index\n\narray([[0.0000e+00, 0.0000e+00, 0.0000e+00],\n       [0.0000e+00, 1.0000e+00, 0.0000e+00],\n       [0.0000e+00, 2.0000e+00, 0.0000e+00],\n       ...,\n       [1.2011e+04, 1.2009e+04, 0.0000e+00],\n       [1.2011e+04, 1.2010e+04, 0.0000e+00],\n       [1.2011e+04, 1.2011e+04, 0.0000e+00]])\n\n\n\nedge_index[:,2] = np.abs(edge_index[:,2])\ntheta = edge_index[:,2].mean()\ntheta\n\n10973.519989002007\n\n\n\nedge_index[:,2] = (np.exp(-edge_index[:,2]/theta)!=1) * np.exp(-edge_index[:,2]/theta)\nedge_index\n\narray([[0.0000e+00, 0.0000e+00, 0.0000e+00],\n       [0.0000e+00, 1.0000e+00, 0.0000e+00],\n       [0.0000e+00, 2.0000e+00, 0.0000e+00],\n       ...,\n       [1.2011e+04, 1.2009e+04, 0.0000e+00],\n       [1.2011e+04, 1.2010e+04, 0.0000e+00],\n       [1.2011e+04, 1.2011e+04, 0.0000e+00]])\n\n\n\nedge_index_list_updated = edge_index.tolist()\nmm = np.array(edge_index_list_updated)[:,2].mean()\n\n\nselected_edges = [(int(row[0]), int(row[1])) for row in edge_index_list_updated if row[2] &gt; mm]\n\n\nedge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\n\n\nedge_index_selected.shape\n\ntorch.Size([2, 51392])\n\n\n\nnp.save('edge_index_selected.npy', edge_index_selected)"
  },
  {
    "objectID": "posts/230822 데이터(5, matrix로 ls6시도해보기..실패).html#시도",
    "href": "posts/230822 데이터(5, matrix로 ls6시도해보기..실패).html#시도",
    "title": "[FRAUD] 데이터정리 시도(matrix로 lesson6따라하기 - 실패ㅎ)",
    "section": "",
    "text": "_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\n12012*12012\n\n144288144\n\n\n\ndf50 = df50.reset_index()\n\n\nN = len(df50)\n\n\n\n\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.is_fraud.mean().round(5), df50_test.is_fraud.mean().round(5)\n\n(0.49828, 0.50516)\n\n\n\ndf50_tr.shape, df50_test.shape\n\n((9009, 23), (3003, 23))\n\n\n\ntrain_mask = np.concatenate((np.full(9009, True), np.full(3003, False)))\ntest_mask = np.concatenate((np.full(9009, False), np.full(3003, True)))\nprint(\"Train Mask:\", train_mask)\nprint(\"Test Mask:\", test_mask)\n\nTrain Mask: [ True  True  True ... False False False]\nTest Mask: [False False False ...  True  True  True]\n\n\n\ntrain_mask.shape, test_mask.shape\n\n((12012,), (12012,))\n\n\n\ntrain_mask.sum(), test_mask.sum()\n\n(9009, 3003)\n\n\n\ndf50_com = pd.concat([df50_tr, df50_test])\n\n\ndf50_com = df50_com.reset_index()\n\n\n\n\n\n\n# edge_index_list2_com = []\n# for i in range(N):\n#     for j in range(N):\n#         if df50_com['cc_num'][i] != df50_com['cc_num'][j]:  \n#             edge = 0\n#         else:\n#             edge = 1\n#         edge_index_list2_com.append([i, j, edge])\n\n\n#np.save('edge_index_list2_50_com.npy', edge_index_list2_com)\n\nedge_index_list2_com = np.load('edge_index_list2_50_com.npy')\n\n\n# edge_index_list2_com\n\narray([[    0,     0,     1],\n       [    0,     1,     0],\n       [    0,     2,     0],\n       ...,\n       [12011, 12009,     0],\n       [12011, 12010,     0],\n       [12011, 12011,     1]])\n\n\n\nedge_index_list2_com.shape\n\n(144288144, 3)\n\n\n\nnum_nodes = 12012\n\n\naj_matrix = np.zeros((num_nodes, num_nodes))\n\n\naj_matrix\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])\n\n\n\nfor i, j ,edge in edge_index_list2_com:\n    aj_matrix[i][j] = edge\n\n\naj_matrix\n\narray([[1., 0., 0., ..., 0., 0., 0.],\n       [0., 1., 0., ..., 0., 0., 0.],\n       [0., 0., 1., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 1., 0., 0.],\n       [0., 0., 0., ..., 0., 1., 0.],\n       [0., 0., 0., ..., 0., 0., 1.]])\n\n\n\n# aj_matrix.shape\n\n(12012, 12012)\n\n\n\n# np.save('aj_matrix.npy', aj_matrix)\n\n\n\n# aj_matrix = np.load('aj_matrix.npy')\n\n\n# aj_matrix\n\narray([[1., 0., 0., ..., 0., 0., 0.],\n       [0., 1., 0., ..., 0., 0., 0.],\n       [0., 0., 1., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 1., 0., 0.],\n       [0., 0., 0., ..., 0., 1., 0.],\n       [0., 0., 0., ..., 0., 0., 1.]])\n\n\n\n\n\n\n\n\n# edge_index_list = []\n# for i in range(N):\n#     for j in range(N):\n#         time_difference = (df50_com['trans_date_trans_time'][i] - df50_com['trans_date_trans_time'][j]).total_seconds()\n#         edge_index_list.append([i, j, time_difference])\n\n\n# np.save('edge_index_list_50_com.npy', edge_index_list)\n\nedge_index_list = np.load('edge_index_list_50_com.npy')\n\n\nedge_index_list[:5]\n\narray([[ 0.000000e+00,  0.000000e+00,  0.000000e+00],\n       [ 0.000000e+00,  1.000000e+00, -2.030190e+07],\n       [ 0.000000e+00,  2.000000e+00, -2.841396e+07],\n       [ 0.000000e+00,  3.000000e+00, -2.383788e+07],\n       [ 0.000000e+00,  4.000000e+00, -2.687796e+07]])\n\n\n\nedge_index = np.array(edge_index_list)\nedge_index[:,2] = np.abs(edge_index[:,2])\ntheta = edge_index[:,2].mean()\ntheta\n\n12238996.895508753\n\n\n\nedge_index[:,2] = (np.exp(-edge_index[:,2]/theta)!=1) * np.exp(-edge_index[:,2]/theta)\nedge_index\n\narray([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 1.00000000e+00, 1.90369587e-01],\n       [0.00000000e+00, 2.00000000e+00, 9.81172367e-02],\n       ...,\n       [1.20110000e+04, 1.20090000e+04, 9.25720620e-01],\n       [1.20110000e+04, 1.20100000e+04, 5.15585903e-01],\n       [1.20110000e+04, 1.20110000e+04, 0.00000000e+00]])\n\n\n\nw_matrix로 바꾸려고 하니까 형식이 [i][j]가 맞지 않는다.!\n\n\n# # 출력 형식 변경\n# np.set_printoptions(formatter={'int': '{:d}'.format})\n\n\n# # 원래 출력 형식으로 복원\n# np.set_printoptions(formatter=None)\n\n\nw_matrix = np.zeros((num_nodes, num_nodes))\n\n\nw_matrix\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])\n\n\n\nfor i, j ,time_difference in edge_index:\n    i, j = int(i), int(j)\n    w_matrix[i][j] = time_difference\n\n\nw_matrix\n\narray([[0.        , 0.19036959, 0.09811724, ..., 0.29671829, 0.14162023,\n        0.27467824],\n       [0.19036959, 0.        , 0.51540395, ..., 0.6415836 , 0.74392254,\n        0.69306396],\n       [0.09811724, 0.51540395, 0.        , ..., 0.33067472, 0.69281937,\n        0.3572079 ],\n       ...,\n       [0.29671829, 0.6415836 , 0.33067472, ..., 0.        , 0.4772885 ,\n        0.92572062],\n       [0.14162023, 0.74392254, 0.69281937, ..., 0.4772885 , 0.        ,\n        0.5155859 ],\n       [0.27467824, 0.69306396, 0.3572079 , ..., 0.92572062, 0.5155859 ,\n        0.        ]])\n\n\n\nw_matrix.shape\n\n(12012, 12012)\n\n\n\nnp.save('w_matrix.npy', w_matrix)\n\n\n\n# np.save('edge_index_list_plus.npy', edge_index_list_plus)\n\nedge_index_list_plus = np.load('edge_index_list_plus.npy')\n\n\nedge_index = np.array(edge_index_list_plus)\n\n\nedge_index.shape\n\n(144288144, 3)\n\n\n\nedge_index\n\narray([[0.0000e+00, 0.0000e+00, 0.0000e+00],\n       [0.0000e+00, 1.0000e+00, 0.0000e+00],\n       [0.0000e+00, 2.0000e+00, 0.0000e+00],\n       ...,\n       [1.2011e+04, 1.2009e+04, 0.0000e+00],\n       [1.2011e+04, 1.2010e+04, 0.0000e+00],\n       [1.2011e+04, 1.2011e+04, 0.0000e+00]])\n\n\n\nedge_index[:,2] = np.abs(edge_index[:,2])\ntheta = edge_index[:,2].mean()\ntheta\n\n10973.519989002007\n\n\n\nedge_index[:,2] = (np.exp(-edge_index[:,2]/theta)!=1) * np.exp(-edge_index[:,2]/theta)\nedge_index\n\narray([[0.0000e+00, 0.0000e+00, 0.0000e+00],\n       [0.0000e+00, 1.0000e+00, 0.0000e+00],\n       [0.0000e+00, 2.0000e+00, 0.0000e+00],\n       ...,\n       [1.2011e+04, 1.2009e+04, 0.0000e+00],\n       [1.2011e+04, 1.2010e+04, 0.0000e+00],\n       [1.2011e+04, 1.2011e+04, 0.0000e+00]])\n\n\n\nedge_index_list_updated = edge_index.tolist()\nmm = np.array(edge_index_list_updated)[:,2].mean()\n\n\nselected_edges = [(int(row[0]), int(row[1])) for row in edge_index_list_updated if row[2] &gt; mm]\n\n\nedge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\n\n\nedge_index_selected.shape\n\ntorch.Size([2, 51392])\n\n\n\nnp.save('edge_index_selected.npy', edge_index_selected)"
  },
  {
    "objectID": "posts/230825 데이터(8, df02)커널죽음.out.html",
    "href": "posts/230825 데이터(8, df02)커널죽음.out.html",
    "title": "[FRAUD] 데이터정리 시도(8.25_df02 커널 죽음)",
    "section": "",
    "text": "import numpy as np\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\nimport torch\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics \n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부     \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\ndef build_graph_tripartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n                                                       df[\"cc_num\"].values.tolist() +\n                                                       df[\"merchant\"].values.tolist()))}\n    df[\"in_node\"]= df[\"cc_num\"].apply(lambda x: mapping[x])\n    df[\"out_node\"]=df[\"merchant\"].apply(lambda x:mapping[x])\n    \n        \n    G=nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\\\n                        [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")     \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")   \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")  \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n\n    return G\n    \n    \ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef embedding(Graph):\n    # Graph -&gt; X (feature)\n    _edgs = list(Graph.edges)\n    subGraph = Graph.edge_subgraph([_edgs[x] for x in range(len(Graph.edges))]).copy()\n    subGraph.add_nodes_from(list(set(Graph.nodes) - set(subGraph.nodes)))    \n    embedded = AverageEmbedder(Node2Vec(subGraph, weight_key='weight').fit(window=10).wv)\n    X = [embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in range(len(Graph.edges))]\n    # Graph -&gt; y (label)\n    y = np.array(list(nx.get_edge_attributes(Graph, \"label\").values()))\n    return X,y \n\ndef anal(df):\n    Graph = build_graph_bipartite(df)\n    X,XX,y,yy = embedding(Graph)\n    lrnr = RandomForestClassifier(n_estimators=100, random_state=42) \n    lrnr.fit(X,y)\n    yyhat = lrnr.predict(XX)\n    df = pd.DataFrame({\n        'acc':[sklearn.metrics.accuracy_score(yy,yyhat)], \n        'pre':[sklearn.metrics.precision_score(yy,yyhat)], \n        'rec':[sklearn.metrics.recall_score(yy,yyhat)],\n        'f1':[sklearn.metrics.f1_score(yy,yyhat)]}\n    )    \n    return df\n\ndef our_sampling1(df):\n    cus_list = set(df.query('is_fraud==1').cc_num.tolist())\n    return df.query(\"cc_num in @ cus_list\")\n\n\nfraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n\n214520*214520\n\n\ndf02.is_fraud.mean().round(5)\n\n\n사기거래 빈도..\n\n\ndf02 = df02.reset_index()\n\n\nN = len(df02)\n\n\n\n\n\ndf02_tr,df02_test = sklearn.model_selection.train_test_split(df02, random_state=42)\n\n\ndf02_tr.is_fraud.mean().round(5), df02_test.is_fraud.mean().round(5)\n\n\ndf02_tr.shape, df02_test.shape\n\n\ntrain_mask =  [i in df02_tr.index for i in range(N)] \ntest_mask =  [i in df02_test.index for i in range(N)] \n\n\nnp.array(train_mask).sum(), np.array(test_mask).sum()"
  },
  {
    "objectID": "posts/230825 데이터(8, df02)커널죽음.out.html#시도",
    "href": "posts/230825 데이터(8, df02)커널죽음.out.html#시도",
    "title": "[FRAUD] 데이터정리 시도(8.25_df02 커널 죽음)",
    "section": "",
    "text": "_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n\n214520*214520\n\n\ndf02.is_fraud.mean().round(5)\n\n\n사기거래 빈도..\n\n\ndf02 = df02.reset_index()\n\n\nN = len(df02)\n\n\n\n\n\ndf02_tr,df02_test = sklearn.model_selection.train_test_split(df02, random_state=42)\n\n\ndf02_tr.is_fraud.mean().round(5), df02_test.is_fraud.mean().round(5)\n\n\ndf02_tr.shape, df02_test.shape\n\n\ntrain_mask =  [i in df02_tr.index for i in range(N)] \ntest_mask =  [i in df02_test.index for i in range(N)] \n\n\nnp.array(train_mask).sum(), np.array(test_mask).sum()"
  },
  {
    "objectID": "posts/230920 데이터(12, df50 auto).html",
    "href": "posts/230920 데이터(12, df50 auto).html",
    "title": "[FRAUD] 데이터정리 시도(9.20_df50 autogluon)",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\nimport xgboost as xgb\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# gnn\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric\nfrom torch_geometric.nn import GCNConv\n\n# autogluon\nfrom autogluon.tabular import TabularDataset, TabularPredictor\n\n\ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef compute_time_difference(group):\n    n = len(group)\n    result = []\n    for i in range(n):\n        for j in range(n):\n            time_difference = abs(group.iloc[i].trans_date_trans_time.value - group.iloc[j].trans_date_trans_time.value)\n            result.append([group.iloc[i].name, group.iloc[j].name, time_difference])\n    return result\n\n\nclass GCN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 16)\n        self.conv2 = GCNConv(16,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\n\nfraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\n\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\ndf50 = df50.reset_index()\n\n\nN = len(df50)\n\n\ndf50 = df50[[\"amt\",\"is_fraud\"]]\n\n\ndf50[\"amt\"].mean()\n\n297.4638911088911\n\n\n\ndf50[\"amt\"].describe()\n\ncount    12012.000000\nmean       297.463891\nstd        384.130842\nmin          1.010000\n25%         19.917500\n50%         84.680000\n75%        468.295000\nmax      12025.300000\nName: amt, dtype: float64\n\n\n\n\n\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.shape, df50_test.shape\n\n((9009, 2), (3003, 2))\n\n\n\ntrain_mask = [i in df50_tr.index for i in range(N)]\ntest_mask = [i in df50_test.index for i in range(N)]\n\n\ntrain_mask = np.array(train_mask)\ntest_mask = np.array(test_mask)\n\n\ntrain_mask.sum(), test_mask.sum()\n\n(9009, 3003)\n\n\n\ntrain_mask.shape, test_mask.shape\n\n((12012,), (12012,))\n\n\n\n\n\n\n\n# groups = df50.groupby('cc_num')\n# edge_index_list_plus = [compute_time_difference(group) for _, group in groups]\n# edge_index_list_plus_flat = [item for sublist in edge_index_list_plus for item in sublist]\n# edge_index_list_plus_nparr = np.array(edge_index_list_plus_flat)\n# np.save('edge_index_list_plus50.npy', edge_index_list_plus_nparr)\n\n\nedge_index = np.load('edge_index_list_plus50.npy')\ntheta = edge_index[:,2].mean()\nedge_index = np.load('edge_index_list_plus50.npy').astype(np.float64)\nedge_index[:,2] = (np.exp(-edge_index[:,2]/theta) != 1)*(np.exp(-edge_index[:,2]/theta))\nedge_index = edge_index.tolist()\nmean_ = np.array(edge_index)[:,2].mean()\nselected_edges = [(int(row[0]), int(row[1])) for row in edge_index if row[2] &gt; mean_]\nedge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\n\n\n\n\n\n\nx = torch.tensor(df50['amt'], dtype=torch.float).reshape(-1,1)\ny = torch.tensor(df50['is_fraud'],dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index = edge_index_selected, y=y, train_mask = train_mask, test_mask = test_mask)\ndata\n\nData(x=[12012, 1], edge_index=[2, 93730], y=[12012], train_mask=[12012], test_mask=[12012])"
  },
  {
    "objectID": "posts/230920 데이터(12, df50 auto).html#데이터정리",
    "href": "posts/230920 데이터(12, df50 auto).html#데이터정리",
    "title": "[FRAUD] 데이터정리 시도(9.20_df50 autogluon)",
    "section": "",
    "text": "_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\ndf50 = df50.reset_index()\n\n\nN = len(df50)\n\n\ndf50 = df50[[\"amt\",\"is_fraud\"]]\n\n\ndf50[\"amt\"].mean()\n\n297.4638911088911\n\n\n\ndf50[\"amt\"].describe()\n\ncount    12012.000000\nmean       297.463891\nstd        384.130842\nmin          1.010000\n25%         19.917500\n50%         84.680000\n75%        468.295000\nmax      12025.300000\nName: amt, dtype: float64\n\n\n\n\n\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.shape, df50_test.shape\n\n((9009, 2), (3003, 2))\n\n\n\ntrain_mask = [i in df50_tr.index for i in range(N)]\ntest_mask = [i in df50_test.index for i in range(N)]\n\n\ntrain_mask = np.array(train_mask)\ntest_mask = np.array(test_mask)\n\n\ntrain_mask.sum(), test_mask.sum()\n\n(9009, 3003)\n\n\n\ntrain_mask.shape, test_mask.shape\n\n((12012,), (12012,))\n\n\n\n\n\n\n\n# groups = df50.groupby('cc_num')\n# edge_index_list_plus = [compute_time_difference(group) for _, group in groups]\n# edge_index_list_plus_flat = [item for sublist in edge_index_list_plus for item in sublist]\n# edge_index_list_plus_nparr = np.array(edge_index_list_plus_flat)\n# np.save('edge_index_list_plus50.npy', edge_index_list_plus_nparr)\n\n\nedge_index = np.load('edge_index_list_plus50.npy')\ntheta = edge_index[:,2].mean()\nedge_index = np.load('edge_index_list_plus50.npy').astype(np.float64)\nedge_index[:,2] = (np.exp(-edge_index[:,2]/theta) != 1)*(np.exp(-edge_index[:,2]/theta))\nedge_index = edge_index.tolist()\nmean_ = np.array(edge_index)[:,2].mean()\nselected_edges = [(int(row[0]), int(row[1])) for row in edge_index if row[2] &gt; mean_]\nedge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\n\n\n\n\n\n\nx = torch.tensor(df50['amt'], dtype=torch.float).reshape(-1,1)\ny = torch.tensor(df50['is_fraud'],dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index = edge_index_selected, y=y, train_mask = train_mask, test_mask = test_mask)\ndata\n\nData(x=[12012, 1], edge_index=[2, 93730], y=[12012], train_mask=[12012], test_mask=[12012])"
  },
  {
    "objectID": "posts/230920 데이터(12, df50 auto).html#a.-데이터",
    "href": "posts/230920 데이터(12, df50 auto).html#a.-데이터",
    "title": "[FRAUD] 데이터정리 시도(9.20_df50 autogluon)",
    "section": "A. 데이터",
    "text": "A. 데이터\n\ntr = TabularDataset(df50_tr)\ntst = TabularDataset(df50_test)"
  },
  {
    "objectID": "posts/230920 데이터(12, df50 auto).html#b.-predictor-생성",
    "href": "posts/230920 데이터(12, df50 auto).html#b.-predictor-생성",
    "title": "[FRAUD] 데이터정리 시도(9.20_df50 autogluon)",
    "section": "B. predictor 생성",
    "text": "B. predictor 생성\n\npredictr = TabularPredictor(\"is_fraud\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231101_160221/\""
  },
  {
    "objectID": "posts/230920 데이터(12, df50 auto).html#c.적합fit",
    "href": "posts/230920 데이터(12, df50 auto).html#c.적합fit",
    "title": "[FRAUD] 데이터정리 시도(9.20_df50 autogluon)",
    "section": "C.적합(fit)",
    "text": "C.적합(fit)\n\npredictr.fit(tr) \n\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20231101_160221/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.8.18\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   690.98 GB / 982.82 GB (70.3%)\nTrain Data Rows:    9009\nTrain Data Columns: 1\nLabel Column: is_fraud\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [1, 0]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    112324.67 MB\n    Train Data (Original)  Memory Usage: 0.07 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', []) : 1 | ['amt']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', []) : 1 | ['amt']\n    0.0s = Fit runtime\n    1 features in original data used to generate 1 features in processed data.\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.02s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.1, Train Rows: 8108, Val Rows: 901\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\n    0.8779   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: KNeighborsDist ...\n    0.8635   = Validation score   (accuracy)\n    0.0s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMXT ...\n    0.8768   = Validation score   (accuracy)\n    0.17s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n    0.8923   = Validation score   (accuracy)\n    0.25s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestGini ...\n    0.8513   = Validation score   (accuracy)\n    0.3s     = Training   runtime\n    0.03s    = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8513   = Validation score   (accuracy)\n    0.31s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: CatBoost ...\n    0.8946   = Validation score   (accuracy)\n    0.71s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.8602   = Validation score   (accuracy)\n    0.28s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.8579   = Validation score   (accuracy)\n    0.28s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: NeuralNetFastAI ...\nNo improvement since epoch 1: early stopping\n    0.8635   = Validation score   (accuracy)\n    2.82s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: XGBoost ...\n    0.8935   = Validation score   (accuracy)\n    0.1s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: NeuralNetTorch ...\n    0.8857   = Validation score   (accuracy)\n    4.62s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMLarge ...\n    0.8946   = Validation score   (accuracy)\n    0.31s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.9023   = Validation score   (accuracy)\n    0.48s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 10.98s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231101_160221/\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7fd21ca03b80&gt;\n\n\n\npredictr.leaderboard()\n\n                  model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0   WeightedEnsemble_L2   0.902331       0.047529  6.681587                0.001262           0.483885            2       True         14\n1         LightGBMLarge   0.894562       0.001690  0.307810                0.001690           0.307810            1       True         13\n2              CatBoost   0.894562       0.002187  0.705743                0.002187           0.705743            1       True          7\n3               XGBoost   0.893452       0.003370  0.104729                0.003370           0.104729            1       True         11\n4              LightGBM   0.892342       0.003417  0.250978                0.003417           0.250978            1       True          4\n5        NeuralNetTorch   0.885683       0.004299  4.621580                0.004299           4.621580            1       True         12\n6        KNeighborsUnif   0.877913       0.005800  0.008693                0.005800           0.008693            1       True          1\n7            LightGBMXT   0.876804       0.002110  0.170732                0.002110           0.170732            1       True          3\n8        KNeighborsDist   0.863485       0.004718  0.004118                0.004718           0.004118            1       True          2\n9       NeuralNetFastAI   0.863485       0.010838  2.824286                0.010838           2.824286            1       True         10\n10       ExtraTreesGini   0.860155       0.027893  0.282991                0.027893           0.282991            1       True          8\n11       ExtraTreesEntr   0.857936       0.027677  0.275942                0.027677           0.275942            1       True          9\n12     RandomForestEntr   0.851276       0.027698  0.310881                0.027698           0.310881            1       True          6\n13     RandomForestGini   0.851276       0.028313  0.296602                0.028313           0.296602            1       True          5\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nWeightedEnsemble_L2\n0.902331\n0.047529\n6.681587\n0.001262\n0.483885\n2\nTrue\n14\n\n\n1\nLightGBMLarge\n0.894562\n0.001690\n0.307810\n0.001690\n0.307810\n1\nTrue\n13\n\n\n2\nCatBoost\n0.894562\n0.002187\n0.705743\n0.002187\n0.705743\n1\nTrue\n7\n\n\n3\nXGBoost\n0.893452\n0.003370\n0.104729\n0.003370\n0.104729\n1\nTrue\n11\n\n\n4\nLightGBM\n0.892342\n0.003417\n0.250978\n0.003417\n0.250978\n1\nTrue\n4\n\n\n5\nNeuralNetTorch\n0.885683\n0.004299\n4.621580\n0.004299\n4.621580\n1\nTrue\n12\n\n\n6\nKNeighborsUnif\n0.877913\n0.005800\n0.008693\n0.005800\n0.008693\n1\nTrue\n1\n\n\n7\nLightGBMXT\n0.876804\n0.002110\n0.170732\n0.002110\n0.170732\n1\nTrue\n3\n\n\n8\nKNeighborsDist\n0.863485\n0.004718\n0.004118\n0.004718\n0.004118\n1\nTrue\n2\n\n\n9\nNeuralNetFastAI\n0.863485\n0.010838\n2.824286\n0.010838\n2.824286\n1\nTrue\n10\n\n\n10\nExtraTreesGini\n0.860155\n0.027893\n0.282991\n0.027893\n0.282991\n1\nTrue\n8\n\n\n11\nExtraTreesEntr\n0.857936\n0.027677\n0.275942\n0.027677\n0.275942\n1\nTrue\n9\n\n\n12\nRandomForestEntr\n0.851276\n0.027698\n0.310881\n0.027698\n0.310881\n1\nTrue\n6\n\n\n13\nRandomForestGini\n0.851276\n0.028313\n0.296602\n0.028313\n0.296602\n1\nTrue\n5"
  },
  {
    "objectID": "posts/230920 데이터(12, df50 auto).html#d.-예측predict",
    "href": "posts/230920 데이터(12, df50 auto).html#d.-예측predict",
    "title": "[FRAUD] 데이터정리 시도(9.20_df50 autogluon)",
    "section": "D. 예측(predict)",
    "text": "D. 예측(predict)\n\n(tr.is_fraud == predictr.predict(tr)).mean()\n\n0.9102009102009102\n\n\n\n(tst.is_fraud == predictr.predict(tst)).mean()\n\n0.8904428904428905\n\n\n\nyyhat = predictr.predict(tr)\n\n\nautogluon이렇게 하는게 맞는감…;;;\n\n\nfrom sklearn.metrics import f1_score\n\nf1_scores = {}\n\nfor model_name in model_list:\n    model = predictor.load_model(model_name)  # 각 모델을 불러옵니다.\n    y_pred = model.predict(tr)\n    f1 = f1_score(y_true, y_pred)  # F1 스코어를 계산합니다.\n    f1_scores[model_name] = f1\n\n# 개별 모델의 F1 스코어 출력\nfor model_name, f1 in f1_scores.items():\n    print(f\"Model: {model_name}, F1 Score: {f1}\")\n\nNameError: name 'model_list' is not defined"
  },
  {
    "objectID": "posts/231011 데이터(16, 15버넊에서df50 auto)-Copy2.html",
    "href": "posts/231011 데이터(16, 15버넊에서df50 auto)-Copy2.html",
    "title": "[FRAUD] 데이터정리 시도(df50 X범주_auto)",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\nimport xgboost as xgb\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# gnn\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric\nfrom torch_geometric.nn import GCNConv\n\n\n# autogluon\nfrom autogluon.tabular import TabularDataset, TabularPredictor\n\n\ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef compute_time_difference(group):\n    n = len(group)\n    result = []\n    for i in range(n):\n        for j in range(n):\n            time_difference = abs(group.iloc[i].trans_date_trans_time.value - group.iloc[j].trans_date_trans_time.value)\n            result.append([group.iloc[i].name, group.iloc[j].name, time_difference])\n    return result\n\n\nclass GCN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 16)\n        self.conv2 = GCNConv(16,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\n\nfraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\n\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\ndf50 = df50.reset_index()\n\n\nN = len(df50)"
  },
  {
    "objectID": "posts/231011 데이터(16, 15버넊에서df50 auto)-Copy2.html#데이터정리",
    "href": "posts/231011 데이터(16, 15버넊에서df50 auto)-Copy2.html#데이터정리",
    "title": "[FRAUD] 데이터정리 시도(df50 X범주_auto)",
    "section": "",
    "text": "_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\ndf50 = df50.reset_index()\n\n\nN = len(df50)"
  },
  {
    "objectID": "posts/231011 데이터(16, 15버넊에서df50 auto)-Copy2.html#a.-데이터",
    "href": "posts/231011 데이터(16, 15버넊에서df50 auto)-Copy2.html#a.-데이터",
    "title": "[FRAUD] 데이터정리 시도(df50 X범주_auto)",
    "section": "A. 데이터",
    "text": "A. 데이터\n\ntr = TabularDataset(df50_tr)\ntst = TabularDataset(df50_test)"
  },
  {
    "objectID": "posts/231011 데이터(16, 15버넊에서df50 auto)-Copy2.html#b.-predictor-생성",
    "href": "posts/231011 데이터(16, 15버넊에서df50 auto)-Copy2.html#b.-predictor-생성",
    "title": "[FRAUD] 데이터정리 시도(df50 X범주_auto)",
    "section": "B. predictor 생성",
    "text": "B. predictor 생성\n\npredictr = TabularPredictor(\"is_fraud\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231011_124800/\""
  },
  {
    "objectID": "posts/231011 데이터(16, 15버넊에서df50 auto)-Copy2.html#c.적합fit",
    "href": "posts/231011 데이터(16, 15버넊에서df50 auto)-Copy2.html#c.적합fit",
    "title": "[FRAUD] 데이터정리 시도(df50 X범주_auto)",
    "section": "C.적합(fit)",
    "text": "C.적합(fit)\n\npredictr.fit(tr) \n\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20231011_124800/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.8.18\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   746.82 GB / 982.82 GB (76.0%)\nTrain Data Rows:    9009\nTrain Data Columns: 1\nLabel Column: is_fraud\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [1, 0]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    14427.76 MB\n    Train Data (Original)  Memory Usage: 0.07 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', []) : 1 | ['amt']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', []) : 1 | ['amt']\n    0.0s = Fit runtime\n    1 features in original data used to generate 1 features in processed data.\n    Train Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.05s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.1, Train Rows: 8108, Val Rows: 901\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\n    0.8779   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: KNeighborsDist ...\n    0.8635   = Validation score   (accuracy)\n    0.0s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMXT ...\n    0.8768   = Validation score   (accuracy)\n    0.15s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n    0.8923   = Validation score   (accuracy)\n    0.23s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestGini ...\n    0.8513   = Validation score   (accuracy)\n    0.31s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8513   = Validation score   (accuracy)\n    0.32s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: CatBoost ...\n    0.8946   = Validation score   (accuracy)\n    0.64s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.8602   = Validation score   (accuracy)\n    0.28s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.8579   = Validation score   (accuracy)\n    0.28s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: NeuralNetFastAI ...\nNo improvement since epoch 1: early stopping\n    0.8635   = Validation score   (accuracy)\n    2.93s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: XGBoost ...\n    0.8935   = Validation score   (accuracy)\n    0.11s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: NeuralNetTorch ...\n    0.8857   = Validation score   (accuracy)\n    4.91s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMLarge ...\n    0.8946   = Validation score   (accuracy)\n    0.35s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.9023   = Validation score   (accuracy)\n    0.48s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 11.38s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231011_124800/\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7f1ff7efc670&gt;\n\n\n\npredictr.leaderboard()\n\n                  model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0   WeightedEnsemble_L2   0.902331       0.051067  7.458454                0.001309           0.513563            2       True         14\n1         LightGBMLarge   0.894562       0.001894  0.414943                0.001894           0.414943            1       True         13\n2              CatBoost   0.894562       0.001981  0.653966                0.001981           0.653966            1       True          7\n3               XGBoost   0.893452       0.003316  0.114061                0.003316           0.114061            1       True         11\n4              LightGBM   0.892342       0.003488  0.343734                0.003488           0.343734            1       True          4\n5        NeuralNetTorch   0.885683       0.005610  5.186066                0.005610           5.186066            1       True         12\n6        KNeighborsUnif   0.877913       0.006206  0.028794                0.006206           0.028794            1       True          1\n7            LightGBMXT   0.876804       0.002243  0.245948                0.002243           0.245948            1       True          3\n8        KNeighborsDist   0.863485       0.005649  0.024392                0.005649           0.024392            1       True          2\n9       NeuralNetFastAI   0.863485       0.008246  2.861539                0.008246           2.861539            1       True         10\n10       ExtraTreesGini   0.860155       0.029064  0.305516                0.029064           0.305516            1       True          8\n11       ExtraTreesEntr   0.857936       0.029304  0.306215                0.029304           0.306215            1       True          9\n12     RandomForestEntr   0.851276       0.028594  0.344763                0.028594           0.344763            1       True          6\n13     RandomForestGini   0.851276       0.028716  0.321282                0.028716           0.321282            1       True          5\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nWeightedEnsemble_L2\n0.902331\n0.051067\n7.458454\n0.001309\n0.513563\n2\nTrue\n14\n\n\n1\nLightGBMLarge\n0.894562\n0.001894\n0.414943\n0.001894\n0.414943\n1\nTrue\n13\n\n\n2\nCatBoost\n0.894562\n0.001981\n0.653966\n0.001981\n0.653966\n1\nTrue\n7\n\n\n3\nXGBoost\n0.893452\n0.003316\n0.114061\n0.003316\n0.114061\n1\nTrue\n11\n\n\n4\nLightGBM\n0.892342\n0.003488\n0.343734\n0.003488\n0.343734\n1\nTrue\n4\n\n\n5\nNeuralNetTorch\n0.885683\n0.005610\n5.186066\n0.005610\n5.186066\n1\nTrue\n12\n\n\n6\nKNeighborsUnif\n0.877913\n0.006206\n0.028794\n0.006206\n0.028794\n1\nTrue\n1\n\n\n7\nLightGBMXT\n0.876804\n0.002243\n0.245948\n0.002243\n0.245948\n1\nTrue\n3\n\n\n8\nKNeighborsDist\n0.863485\n0.005649\n0.024392\n0.005649\n0.024392\n1\nTrue\n2\n\n\n9\nNeuralNetFastAI\n0.863485\n0.008246\n2.861539\n0.008246\n2.861539\n1\nTrue\n10\n\n\n10\nExtraTreesGini\n0.860155\n0.029064\n0.305516\n0.029064\n0.305516\n1\nTrue\n8\n\n\n11\nExtraTreesEntr\n0.857936\n0.029304\n0.306215\n0.029304\n0.306215\n1\nTrue\n9\n\n\n12\nRandomForestEntr\n0.851276\n0.028594\n0.344763\n0.028594\n0.344763\n1\nTrue\n6\n\n\n13\nRandomForestGini\n0.851276\n0.028716\n0.321282\n0.028716\n0.321282\n1\nTrue\n5"
  },
  {
    "objectID": "posts/231011 데이터(16, 15버넊에서df50 auto)-Copy2.html#a.-데이터-1",
    "href": "posts/231011 데이터(16, 15버넊에서df50 auto)-Copy2.html#a.-데이터-1",
    "title": "[FRAUD] 데이터정리 시도(df50 X범주_auto)",
    "section": "A. 데이터",
    "text": "A. 데이터\n\ntr = TabularDataset(df50_tr)\ntst = TabularDataset(df50_test)"
  },
  {
    "objectID": "posts/231011 데이터(16, 15버넊에서df50 auto)-Copy2.html#b.-predictor-생성-1",
    "href": "posts/231011 데이터(16, 15버넊에서df50 auto)-Copy2.html#b.-predictor-생성-1",
    "title": "[FRAUD] 데이터정리 시도(df50 X범주_auto)",
    "section": "B. predictor 생성",
    "text": "B. predictor 생성\n\npredictr = TabularPredictor(\"is_fraud\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231011_125208/\""
  },
  {
    "objectID": "posts/231011 데이터(16, 15버넊에서df50 auto)-Copy2.html#c.적합fit-1",
    "href": "posts/231011 데이터(16, 15버넊에서df50 auto)-Copy2.html#c.적합fit-1",
    "title": "[FRAUD] 데이터정리 시도(df50 X범주_auto)",
    "section": "C.적합(fit)",
    "text": "C.적합(fit)\n\npredictr.fit(tr) \n\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20231011_125208/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.8.18\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   746.64 GB / 982.82 GB (76.0%)\nTrain Data Rows:    9009\nTrain Data Columns: 2\nLabel Column: is_fraud\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [1, 0]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    14301.5 MB\n    Train Data (Original)  Memory Usage: 0.14 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', []) : 2 | ['amt', 'distance_km']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', []) : 2 | ['amt', 'distance_km']\n    0.0s = Fit runtime\n    2 features in original data used to generate 2 features in processed data.\n    Train Data (Processed) Memory Usage: 0.14 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.04s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.1, Train Rows: 8108, Val Rows: 901\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\n    0.8646   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: KNeighborsDist ...\n    0.8535   = Validation score   (accuracy)\n    0.0s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMXT ...\n    0.8879   = Validation score   (accuracy)\n    0.33s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBM ...\n    0.8912   = Validation score   (accuracy)\n    0.22s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestGini ...\n    0.8701   = Validation score   (accuracy)\n    0.32s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8735   = Validation score   (accuracy)\n    0.35s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: CatBoost ...\n    0.899    = Validation score   (accuracy)\n    0.51s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.8613   = Validation score   (accuracy)\n    0.28s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.8646   = Validation score   (accuracy)\n    0.28s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: NeuralNetFastAI ...\nNo improvement since epoch 1: early stopping\n    0.8624   = Validation score   (accuracy)\n    2.94s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: XGBoost ...\n    0.889    = Validation score   (accuracy)\n    0.18s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: NeuralNetTorch ...\n    0.8857   = Validation score   (accuracy)\n    4.49s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMLarge ...\n    0.8779   = Validation score   (accuracy)\n    0.38s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.899    = Validation score   (accuracy)\n    0.5s     = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 11.21s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231011_125208/\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7f201b90b790&gt;\n\n\n\npredictr.leaderboard()\n\n                  model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0              CatBoost   0.899001       0.001843  0.508956                0.001843           0.508956            1       True          7\n1   WeightedEnsemble_L2   0.899001       0.003138  1.012600                0.001295           0.503644            2       True         14\n2              LightGBM   0.891232       0.002272  0.223588                0.002272           0.223588            1       True          4\n3               XGBoost   0.889012       0.003450  0.180574                0.003450           0.180574            1       True         11\n4            LightGBMXT   0.887902       0.006675  0.331872                0.006675           0.331872            1       True          3\n5        NeuralNetTorch   0.885683       0.005594  4.485880                0.005594           4.485880            1       True         12\n6         LightGBMLarge   0.877913       0.001962  0.382782                0.001962           0.382782            1       True         13\n7      RandomForestEntr   0.873474       0.030227  0.347987                0.030227           0.347987            1       True          6\n8      RandomForestGini   0.870144       0.028919  0.315052                0.028919           0.315052            1       True          5\n9        KNeighborsUnif   0.864595       0.006738  0.009886                0.006738           0.009886            1       True          1\n10       ExtraTreesEntr   0.864595       0.029063  0.283869                0.029063           0.283869            1       True          9\n11      NeuralNetFastAI   0.862375       0.011110  2.937038                0.011110           2.937038            1       True         10\n12       ExtraTreesGini   0.861265       0.028793  0.284426                0.028793           0.284426            1       True          8\n13       KNeighborsDist   0.853496       0.004887  0.004544                0.004887           0.004544            1       True          2\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nCatBoost\n0.899001\n0.001843\n0.508956\n0.001843\n0.508956\n1\nTrue\n7\n\n\n1\nWeightedEnsemble_L2\n0.899001\n0.003138\n1.012600\n0.001295\n0.503644\n2\nTrue\n14\n\n\n2\nLightGBM\n0.891232\n0.002272\n0.223588\n0.002272\n0.223588\n1\nTrue\n4\n\n\n3\nXGBoost\n0.889012\n0.003450\n0.180574\n0.003450\n0.180574\n1\nTrue\n11\n\n\n4\nLightGBMXT\n0.887902\n0.006675\n0.331872\n0.006675\n0.331872\n1\nTrue\n3\n\n\n5\nNeuralNetTorch\n0.885683\n0.005594\n4.485880\n0.005594\n4.485880\n1\nTrue\n12\n\n\n6\nLightGBMLarge\n0.877913\n0.001962\n0.382782\n0.001962\n0.382782\n1\nTrue\n13\n\n\n7\nRandomForestEntr\n0.873474\n0.030227\n0.347987\n0.030227\n0.347987\n1\nTrue\n6\n\n\n8\nRandomForestGini\n0.870144\n0.028919\n0.315052\n0.028919\n0.315052\n1\nTrue\n5\n\n\n9\nKNeighborsUnif\n0.864595\n0.006738\n0.009886\n0.006738\n0.009886\n1\nTrue\n1\n\n\n10\nExtraTreesEntr\n0.864595\n0.029063\n0.283869\n0.029063\n0.283869\n1\nTrue\n9\n\n\n11\nNeuralNetFastAI\n0.862375\n0.011110\n2.937038\n0.011110\n2.937038\n1\nTrue\n10\n\n\n12\nExtraTreesGini\n0.861265\n0.028793\n0.284426\n0.028793\n0.284426\n1\nTrue\n8\n\n\n13\nKNeighborsDist\n0.853496\n0.004887\n0.004544\n0.004887\n0.004544\n1\nTrue\n2"
  },
  {
    "objectID": "posts/231011 데이터(16, 15버넊에서df50 auto)-Copy2.html#a.-데이터-2",
    "href": "posts/231011 데이터(16, 15버넊에서df50 auto)-Copy2.html#a.-데이터-2",
    "title": "[FRAUD] 데이터정리 시도(df50 X범주_auto)",
    "section": "A. 데이터",
    "text": "A. 데이터\n\ntr = TabularDataset(df50_tr)\ntst = TabularDataset(df50_test)"
  },
  {
    "objectID": "posts/231011 데이터(16, 15버넊에서df50 auto)-Copy2.html#b.-predictor-생성-2",
    "href": "posts/231011 데이터(16, 15버넊에서df50 auto)-Copy2.html#b.-predictor-생성-2",
    "title": "[FRAUD] 데이터정리 시도(df50 X범주_auto)",
    "section": "B. predictor 생성",
    "text": "B. predictor 생성\n\npredictr = TabularPredictor(\"is_fraud\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231011_124455/\""
  },
  {
    "objectID": "posts/231011 데이터(16, 15버넊에서df50 auto)-Copy2.html#c.적합fit-2",
    "href": "posts/231011 데이터(16, 15버넊에서df50 auto)-Copy2.html#c.적합fit-2",
    "title": "[FRAUD] 데이터정리 시도(df50 X범주_auto)",
    "section": "C.적합(fit)",
    "text": "C.적합(fit)\n\npredictr.fit(tr) \n\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20231011_124455/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.8.18\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   747.06 GB / 982.82 GB (76.0%)\nTrain Data Rows:    9009\nTrain Data Columns: 3\nLabel Column: is_fraud\nPreprocessing data ...\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [1, 0]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    14638.47 MB\n    Train Data (Original)  Memory Usage: 0.22 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', []) : 2 | ['amt', 'distance_km']\n        ('int', [])   : 1 | ['trans_date_trans_time']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', []) : 2 | ['amt', 'distance_km']\n        ('int', [])   : 1 | ['trans_date_trans_time']\n    0.0s = Fit runtime\n    3 features in original data used to generate 3 features in processed data.\n    Train Data (Processed) Memory Usage: 0.22 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.04s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.1, Train Rows: 8108, Val Rows: 901\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ...\n    0.727    = Validation score   (accuracy)\n    0.02s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: KNeighborsDist ...\n    0.7236   = Validation score   (accuracy)\n    0.01s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMXT ...\n    0.8812   = Validation score   (accuracy)\n    0.27s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n    0.8912   = Validation score   (accuracy)\n    0.19s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestGini ...\n    0.8757   = Validation score   (accuracy)\n    0.33s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: RandomForestEntr ...\n    0.8835   = Validation score   (accuracy)\n    0.36s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: CatBoost ...\n    0.8923   = Validation score   (accuracy)\n    0.89s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.8701   = Validation score   (accuracy)\n    0.29s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.8724   = Validation score   (accuracy)\n    0.3s     = Training   runtime\n    0.03s    = Validation runtime\nFitting model: NeuralNetFastAI ...\nNo improvement since epoch 4: early stopping\n    0.8602   = Validation score   (accuracy)\n    3.48s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: XGBoost ...\n    0.8923   = Validation score   (accuracy)\n    0.14s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: NeuralNetTorch ...\n    0.8746   = Validation score   (accuracy)\n    3.71s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMLarge ...\n    0.8768   = Validation score   (accuracy)\n    0.35s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    0.9279   = Validation score   (accuracy)\n    0.5s     = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 11.23s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231011_124455/\")\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7f205bd3bc70&gt;\n\n\n\npredictr.leaderboard()\n\n                  model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0   WeightedEnsemble_L2   0.927858       0.034292  9.131091                0.001279           0.502710            2       True         14\n1              CatBoost   0.892342       0.002311  0.885833                0.002311           0.885833            1       True          7\n2               XGBoost   0.892342       0.003765  0.143192                0.003765           0.143192            1       True         11\n3              LightGBM   0.891232       0.002339  0.189846                0.002339           0.189846            1       True          4\n4      RandomForestEntr   0.883463       0.029521  0.356271                0.029521           0.356271            1       True          6\n5            LightGBMXT   0.881243       0.003287  0.271952                0.003287           0.271952            1       True          3\n6         LightGBMLarge   0.876804       0.001867  0.351746                0.001867           0.351746            1       True         13\n7      RandomForestGini   0.875694       0.030285  0.330684                0.030285           0.330684            1       True          5\n8        NeuralNetTorch   0.874584       0.005663  3.705228                0.005663           3.705228            1       True         12\n9        ExtraTreesEntr   0.872364       0.030204  0.300827                0.030204           0.300827            1       True          9\n10       ExtraTreesGini   0.870144       0.029252  0.293159                0.029252           0.293159            1       True          8\n11      NeuralNetFastAI   0.860155       0.008793  3.475210                0.008793           3.475210            1       True         10\n12       KNeighborsUnif   0.726970       0.007053  0.015347                0.007053           0.015347            1       True          1\n13       KNeighborsDist   0.723640       0.004987  0.005171                0.004987           0.005171            1       True          2\n\n\n\n\n\n\n\n\n\nmodel\nscore_val\npred_time_val\nfit_time\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nWeightedEnsemble_L2\n0.927858\n0.034292\n9.131091\n0.001279\n0.502710\n2\nTrue\n14\n\n\n1\nCatBoost\n0.892342\n0.002311\n0.885833\n0.002311\n0.885833\n1\nTrue\n7\n\n\n2\nXGBoost\n0.892342\n0.003765\n0.143192\n0.003765\n0.143192\n1\nTrue\n11\n\n\n3\nLightGBM\n0.891232\n0.002339\n0.189846\n0.002339\n0.189846\n1\nTrue\n4\n\n\n4\nRandomForestEntr\n0.883463\n0.029521\n0.356271\n0.029521\n0.356271\n1\nTrue\n6\n\n\n5\nLightGBMXT\n0.881243\n0.003287\n0.271952\n0.003287\n0.271952\n1\nTrue\n3\n\n\n6\nLightGBMLarge\n0.876804\n0.001867\n0.351746\n0.001867\n0.351746\n1\nTrue\n13\n\n\n7\nRandomForestGini\n0.875694\n0.030285\n0.330684\n0.030285\n0.330684\n1\nTrue\n5\n\n\n8\nNeuralNetTorch\n0.874584\n0.005663\n3.705228\n0.005663\n3.705228\n1\nTrue\n12\n\n\n9\nExtraTreesEntr\n0.872364\n0.030204\n0.300827\n0.030204\n0.300827\n1\nTrue\n9\n\n\n10\nExtraTreesGini\n0.870144\n0.029252\n0.293159\n0.029252\n0.293159\n1\nTrue\n8\n\n\n11\nNeuralNetFastAI\n0.860155\n0.008793\n3.475210\n0.008793\n3.475210\n1\nTrue\n10\n\n\n12\nKNeighborsUnif\n0.726970\n0.007053\n0.015347\n0.007053\n0.015347\n1\nTrue\n1\n\n\n13\nKNeighborsDist\n0.723640\n0.004987\n0.005171\n0.004987\n0.005171\n1\nTrue\n2"
  },
  {
    "objectID": "posts/231102.html",
    "href": "posts/231102.html",
    "title": "[FRAUD] 데이터정리",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\nimport xgboost as xgb\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# gnn\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric\nfrom torch_geometric.nn import GCNConv\n\n\n\ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef compute_time_difference(group):\n    n = len(group)\n    result = []\n    for i in range(n):\n        for j in range(n):\n            time_difference = abs(group.iloc[i].trans_date_trans_time.value - group.iloc[j].trans_date_trans_time.value)\n            result.append([group.iloc[i].name, group.iloc[j].name, time_difference])\n    return result\n\ndef mask(df):\n    df_tr,df_test = sklearn.model_selection.train_test_split(df, random_state=42)\n    N = len(df)\n    train_mask = [i in df_tr.index for i in range(N)]\n    test_mask = [i in df_test.index for i in range(N)]\n    train_mask = np.array(train_mask)\n    test_mask = np.array(test_mask)\n    return train_mask, test_mask\n\ndef edge_index_selected(edge_index):\n    theta = edge_index[:,2].mean()\n    edge_index[:,2] = (np.exp(-edge_index[:,2]/theta) != 1)*(np.exp(-edge_index[:,2]/theta))\n    edge_index = edge_index.tolist()\n    mean_ = np.array(edge_index)[:,2].mean()\n    selected_edges = [(int(row[0]), int(row[1])) for row in edge_index if row[2] &gt; mean_]\n    edge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\n    return edge_index_selected\n\n\n\n\nfraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\n\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50 = df50.reset_index()\ndf50.shape\n\n(12012, 23)\n\n\n\n\n\n\nmask(df50)\n\n(array([False,  True,  True, ...,  True, False,  True]),\n array([ True, False, False, ..., False,  True, False]))\n\n\n\ntrain_mask, test_mask = mask(df50)\n\n\n\n\n\n\n# groups = df50.groupby('cc_num')\n# edge_index_list_plus = [compute_time_difference(group) for _, group in groups]\n# edge_index_list_plus_flat = [item for sublist in edge_index_list_plus for item in sublist]\n# edge_index_list_plus_nparr = np.array(edge_index_list_plus_flat)\n# np.save('edge_index_list_plus50.npy', edge_index_list_plus_nparr)\n\n\nedge_index = np.load('edge_index_list_plus50.npy').astype(np.float64)\nedge_index.shape\n\n(200706, 3)\n\n\n\nedge_index_selected = edge_index_selected(edge_index)\n\n\n\n\n\n\nx = torch.tensor(df50['amt'], dtype=torch.float).reshape(-1,1)\ny = torch.tensor(df50['is_fraud'],dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index = edge_index_selected, y=y, train_mask = train_mask, test_mask = test_mask)\ndata\n\nData(x=[12012, 1], edge_index=[2, 93730], y=[12012], train_mask=[12012], test_mask=[12012])\n\n\n\ndata.node_attrs\n\n&lt;bound method BaseData.node_attrs of Data(x=[12012, 1], edge_index=[2, 93730], y=[12012], train_mask=[12012], test_mask=[12012])&gt;"
  },
  {
    "objectID": "posts/231102.html#데이터정리",
    "href": "posts/231102.html#데이터정리",
    "title": "[FRAUD] 데이터정리",
    "section": "",
    "text": "_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50 = df50.reset_index()\ndf50.shape\n\n(12012, 23)\n\n\n\n\n\n\nmask(df50)\n\n(array([False,  True,  True, ...,  True, False,  True]),\n array([ True, False, False, ..., False,  True, False]))\n\n\n\ntrain_mask, test_mask = mask(df50)\n\n\n\n\n\n\n# groups = df50.groupby('cc_num')\n# edge_index_list_plus = [compute_time_difference(group) for _, group in groups]\n# edge_index_list_plus_flat = [item for sublist in edge_index_list_plus for item in sublist]\n# edge_index_list_plus_nparr = np.array(edge_index_list_plus_flat)\n# np.save('edge_index_list_plus50.npy', edge_index_list_plus_nparr)\n\n\nedge_index = np.load('edge_index_list_plus50.npy').astype(np.float64)\nedge_index.shape\n\n(200706, 3)\n\n\n\nedge_index_selected = edge_index_selected(edge_index)\n\n\n\n\n\n\nx = torch.tensor(df50['amt'], dtype=torch.float).reshape(-1,1)\ny = torch.tensor(df50['is_fraud'],dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index = edge_index_selected, y=y, train_mask = train_mask, test_mask = test_mask)\ndata\n\nData(x=[12012, 1], edge_index=[2, 93730], y=[12012], train_mask=[12012], test_mask=[12012])\n\n\n\ndata.node_attrs\n\n&lt;bound method BaseData.node_attrs of Data(x=[12012, 1], edge_index=[2, 93730], y=[12012], train_mask=[12012], test_mask=[12012])&gt;"
  },
  {
    "objectID": "posts/231102.html#분석-1gcn",
    "href": "posts/231102.html#분석-1gcn",
    "title": "[FRAUD] 데이터정리",
    "section": "분석 1(GCN)",
    "text": "분석 1(GCN)\n\ntorch.manual_seed(202250926)\n\nclass GCN1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 32)\n        self.conv2 = GCNConv(32,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\nX = (data.x[data.train_mask]).numpy()\nXX = (data.x[data.test_mask]).numpy()\ny = (data.y[data.train_mask]).numpy()\nyy = (data.y[data.test_mask]).numpy()\n\nmodel = GCN1()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(400):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\nmodel.eval()\n\npred = model(data).argmax(dim=1)\nyyhat = pred[data.test_mask]\n\npred\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results1= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석1'])\n_results1\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석1\n0.902098\n0.862478\n0.95913\n0.90824\n\n\n\n\n\n\n\n\nb1,W1 = list(model.conv1.parameters())\nb1,W1\n\n(Parameter containing:\n tensor([ 3.3281e-01, -5.2539e-01,  1.1277e+00,  2.8239e-01, -1.2373e-01,\n          9.6607e-01, -1.7652e-02,  8.3960e-01,  3.3252e-01, -4.6080e-01,\n          8.6407e-01,  1.3294e+00,  3.2962e-01, -1.3215e-06,  2.1968e+00,\n         -1.8843e-01,  7.6852e-10, -5.0779e-01, -7.7613e-09, -7.8956e-04,\n         -4.6320e-01, -6.8497e-02,  3.4533e-02, -4.4006e-01, -3.8074e-01,\n         -4.5996e-01,  4.4989e-13, -3.9142e-08,  4.3476e-01,  1.5457e+00,\n         -3.1396e-01,  4.3208e-01], requires_grad=True),\n Parameter containing:\n tensor([[ 1.6974e-01],\n         [ 1.4139e-01],\n         [-6.2153e-03],\n         [ 1.3422e-01],\n         [-8.4340e-02],\n         [-5.0439e-03],\n         [-5.0210e-02],\n         [-4.3200e-03],\n         [ 2.5801e-01],\n         [ 2.1217e-01],\n         [-4.4886e-03],\n         [-7.1453e-03],\n         [ 1.8556e-01],\n         [-3.8936e-02],\n         [-1.2772e-02],\n         [-3.0888e-03],\n         [-1.9216e-05],\n         [ 4.9480e-02],\n         [-6.8162e-02],\n         [-8.9236e-02],\n         [ 1.3753e-01],\n         [-6.8890e-02],\n         [-6.3743e-02],\n         [ 2.1896e-01],\n         [ 2.9288e-03],\n         [ 4.8694e-03],\n         [-4.5865e-02],\n         [-5.2090e-02],\n         [ 1.2880e-01],\n         [-8.3668e-03],\n         [ 9.4231e-03],\n         [ 1.3015e-01]], requires_grad=True))\n\n\n\nb2,W2 = list(model.conv2.parameters())\nb2,W2\n\n(Parameter containing:\n tensor([ 0.8103, -0.8103], requires_grad=True),\n Parameter containing:\n tensor([[-3.1862e-02,  1.2264e-01,  2.5667e-01, -2.9070e-01,  1.9933e-02,\n           2.8287e-01, -8.1168e-03,  1.7061e-01,  3.1362e-02, -1.3049e-01,\n           2.4063e-02,  1.5218e-01,  1.7274e-01,  5.8768e-06,  2.9730e-01,\n           3.7174e-02, -1.1418e-09, -2.0023e-01, -1.3184e-02,  5.8934e-02,\n           2.0550e-01, -5.7726e-02,  2.8572e-01, -1.4259e-01, -2.8006e-01,\n          -6.7342e-02, -3.5160e-02, -7.0944e-02, -5.3993e-02,  1.6846e-01,\n           5.3713e-02, -1.7743e-01],\n         [-2.9673e-02,  1.2650e-01, -1.8649e-01, -2.8863e-01,  2.7340e-01,\n          -2.5555e-02, -2.4640e-02, -9.0935e-02,  3.2923e-02, -1.2820e-01,\n          -2.4358e-01, -2.7795e-01,  1.7514e-01, -4.4449e-06, -3.2877e-01,\n           5.9096e-02,  1.0013e-09, -1.9043e-01,  1.0161e-02, -4.9584e-02,\n           2.0870e-01, -6.1236e-03,  2.2979e-01, -1.4060e-01,  1.1504e-01,\n           2.0197e-01,  4.5961e-02, -1.0499e-01, -5.0285e-02, -3.6542e-01,\n           1.1680e-01, -1.7159e-01]], requires_grad=True))"
  },
  {
    "objectID": "posts/231102.html#분석2로지스틱-회귀",
    "href": "posts/231102.html#분석2로지스틱-회귀",
    "title": "[FRAUD] 데이터정리",
    "section": "분석2(로지스틱 회귀)",
    "text": "분석2(로지스틱 회귀)\n\ntorch.manual_seed(202250926)\nX = np.array(df50_tr.loc[:,['amt']])\nXX = np.array(df50_test.loc[:,['amt']])\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\nlrnr = sklearn.linear_model.LogisticRegression()\n\nlrnr.fit(X,y)\n\n#thresh = y.mean()\n#yyhat = (lrnr.predict_proba(XX)&gt; thresh)[:,-1]\nyyhat = lrnr.predict(XX) \n\nyyhat\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results2= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석2'])\n_results2\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석2\n0.849484\n0.933279\n0.756098\n0.835397"
  },
  {
    "objectID": "posts/231102.html#분석3xgboost",
    "href": "posts/231102.html#분석3xgboost",
    "title": "[FRAUD] 데이터정리",
    "section": "분석3(XGBoost)",
    "text": "분석3(XGBoost)\n\nimport xgboost as xgb\n\n\ntorch.manual_seed(202250926)\nX = np.array(df50_tr.loc[:, ['amt']])\nXX = np.array(df50_test.loc[:, ['amt']])\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\nlrnr = xgb.XGBClassifier()\n \nlrnr.fit(X,y)\nyyhat = lrnr.predict(XX)\n\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results3= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석3'])\n_results3\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석3\n0.88012\n0.886957\n0.874094\n0.880478"
  },
  {
    "objectID": "posts/231102.html#분석4light-gbm",
    "href": "posts/231102.html#분석4light-gbm",
    "title": "[FRAUD] 데이터정리",
    "section": "분석4(Light GBM)",
    "text": "분석4(Light GBM)\n\nimport lightgbm as lgb\n\ntorch.manual_seed(202250926)\nX = np.array(df50_tr.loc[:, ['amt']])\nXX = np.array(df50_test.loc[:, ['amt']])\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n\nlrnr = lgb.LGBMClassifier()\n\nlrnr.fit(X, y)\nyyhat = lrnr.predict(XX)\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results4 = pd.DataFrame({m.__name__: [m(yy, yyhat).round(6)] for m in metrics}, index=['분석4'])\n_results4\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석4\n0.885115\n0.893817\n0.87673\n0.885191"
  },
  {
    "objectID": "posts/230823 데이터(7, df50_com으로 93퍼 accuracy)_guebin.html",
    "href": "posts/230823 데이터(7, df50_com으로 93퍼 accuracy)_guebin.html",
    "title": "[FRAUD] 데이터정리 시도(8.23_df50 다시)",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\nimport torch\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics \n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부     \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\ndef build_graph_tripartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n                                                       df[\"cc_num\"].values.tolist() +\n                                                       df[\"merchant\"].values.tolist()))}\n    df[\"in_node\"]= df[\"cc_num\"].apply(lambda x: mapping[x])\n    df[\"out_node\"]=df[\"merchant\"].apply(lambda x:mapping[x])\n    \n        \n    G=nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\\\n                        [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")     \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")   \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")  \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n\n    return G\n    \n    \ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef embedding(Graph):\n    # Graph -&gt; X (feature)\n    _edgs = list(Graph.edges)\n    subGraph = Graph.edge_subgraph([_edgs[x] for x in range(len(Graph.edges))]).copy()\n    subGraph.add_nodes_from(list(set(Graph.nodes) - set(subGraph.nodes)))    \n    embedded = AverageEmbedder(Node2Vec(subGraph, weight_key='weight').fit(window=10).wv)\n    X = [embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in range(len(Graph.edges))]\n    # Graph -&gt; y (label)\n    y = np.array(list(nx.get_edge_attributes(Graph, \"label\").values()))\n    return X,y \n\ndef anal(df):\n    Graph = build_graph_bipartite(df)\n    X,XX,y,yy = embedding(Graph)\n    lrnr = RandomForestClassifier(n_estimators=100, random_state=42) \n    lrnr.fit(X,y)\n    yyhat = lrnr.predict(XX)\n    df = pd.DataFrame({\n        'acc':[sklearn.metrics.accuracy_score(yy,yyhat)], \n        'pre':[sklearn.metrics.precision_score(yy,yyhat)], \n        'rec':[sklearn.metrics.recall_score(yy,yyhat)],\n        'f1':[sklearn.metrics.f1_score(yy,yyhat)]}\n    )    \n    return df\n\ndef our_sampling1(df):\n    cus_list = set(df.query('is_fraud==1').cc_num.tolist())\n    return df.query(\"cc_num in @ cus_list\")\n\n\nfraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\n\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\n12012*12012\n\n144288144\n\n\n\ndf50 = df50.reset_index()\n\n\nN = len(df50)\n\n\n\n\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.is_fraud.mean().round(5), df50_test.is_fraud.mean().round(5)\n\n(0.49828, 0.50516)\n\n\n\ndf50_tr.shape, df50_test.shape\n\n((9009, 23), (3003, 23))\n\n\n\n# [i in df50_tr.index for i in range(9009+3003)] # train_mask \n\n\ntrain_mask = np.concatenate((np.full(9009, True), np.full(3003, False)))\ntest_mask = np.concatenate((np.full(9009, False), np.full(3003, True)))\nprint(\"Train Mask:\", train_mask)\nprint(\"Test Mask:\", test_mask)\n\nTrain Mask: [ True  True  True ... False False False]\nTest Mask: [False False False ...  True  True  True]\n\n\n\ntrain_mask.shape, test_mask.shape\n\n((12012,), (12012,))\n\n\n\ntrain_mask.sum(), test_mask.sum()\n\n(9009, 3003)\n\n\n\ndf50_com = pd.concat([df50_tr, df50_test])\n\n\ndf50_com = df50_com.reset_index()\n\n\n\n\n\n\n# edge_index_list2_com = []\n# for i in range(N):\n#     for j in range(N):\n#         if df50_com['cc_num'][i] != df50_com['cc_num'][j]:  \n#             edge = 0\n#         else:\n#             edge = 1\n#         edge_index_list2_com.append([i, j, edge])\n\n\n# #np.save('edge_index_list2_50_com.npy', edge_index_list2_com)\n\n# edge_index_list2_com = np.load('edge_index_list2_50_com.npy')\n\n\n# edge_index_list2_com\n\narray([[    0,     0,     1],\n       [    0,     1,     0],\n       [    0,     2,     0],\n       ...,\n       [12011, 12009,     0],\n       [12011, 12010,     0],\n       [12011, 12011,     1]])\n\n\n\n# edge_one_com = [(i, j) for i, j, edge in edge_index_list2_com if edge == 1]\n# edge_one_com[:5]\n\n\n# len(edge_one_com)\n\n\n# edge_one_index_com = torch.tensor(edge_one_com, dtype=torch.long).t()\n\n\n# edge_one_index_com.shape\n\n\n\n\n\n\n# edge_index_list = []\n# for i in range(N):\n#     for j in range(N):\n#         time_difference = (df50_com['trans_date_trans_time'][i] - df50_com['trans_date_trans_time'][j]).total_seconds()\n#         edge_index_list.append([i, j, time_difference])\n\n\n# # np.save('edge_index_list_50_com.npy', edge_index_list)\n\n# edge_index_list2_com = np.load('edge_index_list2_50_com.npy')\n\n\n# edge_index_list[:5]\n\n[[0, 0, 0.0],\n [0, 1, -20301900.0],\n [0, 2, -28413960.0],\n [0, 3, -23837880.0],\n [0, 4, -26877960.0]]\n\n\n\n# edge_index = np.array(edge_index_list)\n# edge_index[:,2] = np.abs(edge_index[:,2])\n# theta = edge_index[:,2].mean()\n# theta\n\n12238996.895508753\n\n\n\n# edge_index[:,2] = (np.exp(-edge_index[:,2]/theta)!=1) * np.exp(-edge_index[:,2]/theta)\n# edge_index\n\narray([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 1.00000000e+00, 1.90369587e-01],\n       [0.00000000e+00, 2.00000000e+00, 9.81172367e-02],\n       ...,\n       [1.20110000e+04, 1.20090000e+04, 9.25720620e-01],\n       [1.20110000e+04, 1.20100000e+04, 5.15585903e-01],\n       [1.20110000e+04, 1.20110000e+04, 0.00000000e+00]])\n\n\n\n# edge_index[:,2]\n\narray([0.        , 0.19036959, 0.09811724, ..., 0.92572062, 0.5155859 ,\n       0.        ])\n\n\n\n갑자기 헷갈리는데, 이 weight는 어디서 곱하는거지? GNN이 아니라 새로운 코드였던감..\n\n\n\n# edge_index_list_plus = []\n# for i in range(N):\n#     for j in range(N):\n#         if df50_com['cc_num'][i] != df50_com['cc_num'][j]:  # cc_num 값이 다르다면\n#             time_difference = 0\n#         else:\n#             time_difference = (df50_com['trans_date_trans_time'][i] - df50_com['trans_date_trans_time'][j]).total_seconds()\n#         edge_index_list_plus.append([i, j, time_difference])\n\n\n# np.save('edge_index_list_plus.npy', edge_index_list_plus)\n\nedge_index_list_plus = np.load('edge_index_list_plus.npy')\n\n\nedge_index = np.array(edge_index_list_plus)\n\n\nedge_index.shape\n\n(144288144, 3)\n\n\n\nedge_index\n\narray([[0.0000e+00, 0.0000e+00, 0.0000e+00],\n       [0.0000e+00, 1.0000e+00, 0.0000e+00],\n       [0.0000e+00, 2.0000e+00, 0.0000e+00],\n       ...,\n       [1.2011e+04, 1.2009e+04, 0.0000e+00],\n       [1.2011e+04, 1.2010e+04, 0.0000e+00],\n       [1.2011e+04, 1.2011e+04, 0.0000e+00]])\n\n\n\nedge_index[:,2] = np.abs(edge_index[:,2])\ntheta = edge_index[:,2].mean()\ntheta\n\n10973.519989002007\n\n\n\nedge_index[:,2] = (np.exp(-edge_index[:,2]/theta)!=1) * np.exp(-edge_index[:,2]/theta)\nedge_index\n\narray([[0.0000e+00, 0.0000e+00, 0.0000e+00],\n       [0.0000e+00, 1.0000e+00, 0.0000e+00],\n       [0.0000e+00, 2.0000e+00, 0.0000e+00],\n       ...,\n       [1.2011e+04, 1.2009e+04, 0.0000e+00],\n       [1.2011e+04, 1.2010e+04, 0.0000e+00],\n       [1.2011e+04, 1.2011e+04, 0.0000e+00]])\n\n\n\nedge_index_list_updated = edge_index.tolist()\nnp.array(edge_index_list_updated)[:,2].mean()\n\n8.443606280313275e-05\n\n\n\nmm = np.array(edge_index_list_updated)[:,2].mean()\n\n\n시간이 평균보다 짧다면? . 음..\n\n\nselected_edges = [(int(row[0]), int(row[1])) for row in edge_index_list_updated if row[2] &gt; mm]\n\n\nedge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\n\n\nedge_index_selected.shape\n\ntorch.Size([2, 51392])\n\n\n\n\n\n\n\nx = df50_com['amt']\n\n\na = torch.tensor(x, dtype=torch.float)\n\n\na = a.reshape(-1,1)\na\n\ntensor([[921.2400],\n        [698.2800],\n        [220.5600],\n        ...,\n        [ 17.9700],\n        [  7.5800],\n        [824.9900]])\n\n\n\ny = df50_com['is_fraud']\n\n\nb = torch.tensor(y,dtype=torch.int64)\n\n\nb\n\ntensor([1, 1, 0,  ..., 1, 0, 1])\n\n\n\nimport torch_geometric\n\n\ndata = torch_geometric.data.Data(x=a, edge_index = edge_index_selected, y=b, train_mask = train_mask, test_mask = test_mask)\n\n\ndata\n\nData(x=[12012, 1], edge_index=[2, 51392], y=[12012], train_mask=[12012], test_mask=[12012])\n\n\n\n\n\n\n\n\n\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\n\nclass GCN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 16)\n        self.conv2 = GCNConv(16,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\n\nmodel = GCN()\n\n\nmodel\n\nGCN(\n  (conv1): GCNConv(1, 16)\n  (conv2): GCNConv(16, 2)\n)\n\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\n\nGCN(\n  (conv1): GCNConv(1, 16)\n  (conv2): GCNConv(16, 2)\n)\n\n\n\n\nfor epoch in range(200):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\n\n\nmodel.eval()\npred = model(data).argmax(dim=1)\ncorrect = (pred[data.test_mask] == data.y[data.test_mask]).sum()\nacc = int(correct) / int(data.test_mask.sum())\nprint(f'Accuracy: {acc:.4f}')\n\nAccuracy: 0.9321\n\n\n\n`"
  },
  {
    "objectID": "posts/230823 데이터(7, df50_com으로 93퍼 accuracy)_guebin.html#시도",
    "href": "posts/230823 데이터(7, df50_com으로 93퍼 accuracy)_guebin.html#시도",
    "title": "[FRAUD] 데이터정리 시도(8.23_df50 다시)",
    "section": "",
    "text": "_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\n12012*12012\n\n144288144\n\n\n\ndf50 = df50.reset_index()\n\n\nN = len(df50)\n\n\n\n\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.is_fraud.mean().round(5), df50_test.is_fraud.mean().round(5)\n\n(0.49828, 0.50516)\n\n\n\ndf50_tr.shape, df50_test.shape\n\n((9009, 23), (3003, 23))\n\n\n\n# [i in df50_tr.index for i in range(9009+3003)] # train_mask \n\n\ntrain_mask = np.concatenate((np.full(9009, True), np.full(3003, False)))\ntest_mask = np.concatenate((np.full(9009, False), np.full(3003, True)))\nprint(\"Train Mask:\", train_mask)\nprint(\"Test Mask:\", test_mask)\n\nTrain Mask: [ True  True  True ... False False False]\nTest Mask: [False False False ...  True  True  True]\n\n\n\ntrain_mask.shape, test_mask.shape\n\n((12012,), (12012,))\n\n\n\ntrain_mask.sum(), test_mask.sum()\n\n(9009, 3003)\n\n\n\ndf50_com = pd.concat([df50_tr, df50_test])\n\n\ndf50_com = df50_com.reset_index()\n\n\n\n\n\n\n# edge_index_list2_com = []\n# for i in range(N):\n#     for j in range(N):\n#         if df50_com['cc_num'][i] != df50_com['cc_num'][j]:  \n#             edge = 0\n#         else:\n#             edge = 1\n#         edge_index_list2_com.append([i, j, edge])\n\n\n# #np.save('edge_index_list2_50_com.npy', edge_index_list2_com)\n\n# edge_index_list2_com = np.load('edge_index_list2_50_com.npy')\n\n\n# edge_index_list2_com\n\narray([[    0,     0,     1],\n       [    0,     1,     0],\n       [    0,     2,     0],\n       ...,\n       [12011, 12009,     0],\n       [12011, 12010,     0],\n       [12011, 12011,     1]])\n\n\n\n# edge_one_com = [(i, j) for i, j, edge in edge_index_list2_com if edge == 1]\n# edge_one_com[:5]\n\n\n# len(edge_one_com)\n\n\n# edge_one_index_com = torch.tensor(edge_one_com, dtype=torch.long).t()\n\n\n# edge_one_index_com.shape\n\n\n\n\n\n\n# edge_index_list = []\n# for i in range(N):\n#     for j in range(N):\n#         time_difference = (df50_com['trans_date_trans_time'][i] - df50_com['trans_date_trans_time'][j]).total_seconds()\n#         edge_index_list.append([i, j, time_difference])\n\n\n# # np.save('edge_index_list_50_com.npy', edge_index_list)\n\n# edge_index_list2_com = np.load('edge_index_list2_50_com.npy')\n\n\n# edge_index_list[:5]\n\n[[0, 0, 0.0],\n [0, 1, -20301900.0],\n [0, 2, -28413960.0],\n [0, 3, -23837880.0],\n [0, 4, -26877960.0]]\n\n\n\n# edge_index = np.array(edge_index_list)\n# edge_index[:,2] = np.abs(edge_index[:,2])\n# theta = edge_index[:,2].mean()\n# theta\n\n12238996.895508753\n\n\n\n# edge_index[:,2] = (np.exp(-edge_index[:,2]/theta)!=1) * np.exp(-edge_index[:,2]/theta)\n# edge_index\n\narray([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 1.00000000e+00, 1.90369587e-01],\n       [0.00000000e+00, 2.00000000e+00, 9.81172367e-02],\n       ...,\n       [1.20110000e+04, 1.20090000e+04, 9.25720620e-01],\n       [1.20110000e+04, 1.20100000e+04, 5.15585903e-01],\n       [1.20110000e+04, 1.20110000e+04, 0.00000000e+00]])\n\n\n\n# edge_index[:,2]\n\narray([0.        , 0.19036959, 0.09811724, ..., 0.92572062, 0.5155859 ,\n       0.        ])\n\n\n\n갑자기 헷갈리는데, 이 weight는 어디서 곱하는거지? GNN이 아니라 새로운 코드였던감..\n\n\n\n# edge_index_list_plus = []\n# for i in range(N):\n#     for j in range(N):\n#         if df50_com['cc_num'][i] != df50_com['cc_num'][j]:  # cc_num 값이 다르다면\n#             time_difference = 0\n#         else:\n#             time_difference = (df50_com['trans_date_trans_time'][i] - df50_com['trans_date_trans_time'][j]).total_seconds()\n#         edge_index_list_plus.append([i, j, time_difference])\n\n\n# np.save('edge_index_list_plus.npy', edge_index_list_plus)\n\nedge_index_list_plus = np.load('edge_index_list_plus.npy')\n\n\nedge_index = np.array(edge_index_list_plus)\n\n\nedge_index.shape\n\n(144288144, 3)\n\n\n\nedge_index\n\narray([[0.0000e+00, 0.0000e+00, 0.0000e+00],\n       [0.0000e+00, 1.0000e+00, 0.0000e+00],\n       [0.0000e+00, 2.0000e+00, 0.0000e+00],\n       ...,\n       [1.2011e+04, 1.2009e+04, 0.0000e+00],\n       [1.2011e+04, 1.2010e+04, 0.0000e+00],\n       [1.2011e+04, 1.2011e+04, 0.0000e+00]])\n\n\n\nedge_index[:,2] = np.abs(edge_index[:,2])\ntheta = edge_index[:,2].mean()\ntheta\n\n10973.519989002007\n\n\n\nedge_index[:,2] = (np.exp(-edge_index[:,2]/theta)!=1) * np.exp(-edge_index[:,2]/theta)\nedge_index\n\narray([[0.0000e+00, 0.0000e+00, 0.0000e+00],\n       [0.0000e+00, 1.0000e+00, 0.0000e+00],\n       [0.0000e+00, 2.0000e+00, 0.0000e+00],\n       ...,\n       [1.2011e+04, 1.2009e+04, 0.0000e+00],\n       [1.2011e+04, 1.2010e+04, 0.0000e+00],\n       [1.2011e+04, 1.2011e+04, 0.0000e+00]])\n\n\n\nedge_index_list_updated = edge_index.tolist()\nnp.array(edge_index_list_updated)[:,2].mean()\n\n8.443606280313275e-05\n\n\n\nmm = np.array(edge_index_list_updated)[:,2].mean()\n\n\n시간이 평균보다 짧다면? . 음..\n\n\nselected_edges = [(int(row[0]), int(row[1])) for row in edge_index_list_updated if row[2] &gt; mm]\n\n\nedge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\n\n\nedge_index_selected.shape\n\ntorch.Size([2, 51392])\n\n\n\n\n\n\n\nx = df50_com['amt']\n\n\na = torch.tensor(x, dtype=torch.float)\n\n\na = a.reshape(-1,1)\na\n\ntensor([[921.2400],\n        [698.2800],\n        [220.5600],\n        ...,\n        [ 17.9700],\n        [  7.5800],\n        [824.9900]])\n\n\n\ny = df50_com['is_fraud']\n\n\nb = torch.tensor(y,dtype=torch.int64)\n\n\nb\n\ntensor([1, 1, 0,  ..., 1, 0, 1])\n\n\n\nimport torch_geometric\n\n\ndata = torch_geometric.data.Data(x=a, edge_index = edge_index_selected, y=b, train_mask = train_mask, test_mask = test_mask)\n\n\ndata\n\nData(x=[12012, 1], edge_index=[2, 51392], y=[12012], train_mask=[12012], test_mask=[12012])\n\n\n\n\n\n\n\n\n\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\n\nclass GCN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 16)\n        self.conv2 = GCNConv(16,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\n\nmodel = GCN()\n\n\nmodel\n\nGCN(\n  (conv1): GCNConv(1, 16)\n  (conv2): GCNConv(16, 2)\n)\n\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\n\nGCN(\n  (conv1): GCNConv(1, 16)\n  (conv2): GCNConv(16, 2)\n)\n\n\n\n\nfor epoch in range(200):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\n\n\nmodel.eval()\npred = model(data).argmax(dim=1)\ncorrect = (pred[data.test_mask] == data.y[data.test_mask]).sum()\nacc = int(correct) / int(data.test_mask.sum())\nprint(f'Accuracy: {acc:.4f}')\n\nAccuracy: 0.9321\n\n\n\n`"
  },
  {
    "objectID": "posts/231111.html",
    "href": "posts/231111.html",
    "title": "[FRAUD] 그래프 그림 그리기(graft_정리)",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# gnn\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric\nfrom torch_geometric.nn import GCNConv\n\nimport graft\n\n\ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef compute_time_difference(group):\n    n = len(group)\n    result = []\n    for i in range(n):\n        for j in range(n):\n            time_difference = abs(group.iloc[i].trans_date_trans_time.value - group.iloc[j].trans_date_trans_time.value)\n            result.append([group.iloc[i].name, group.iloc[j].name, time_difference])\n    return result\n\ndef mask(df):\n    df_tr,df_test = sklearn.model_selection.train_test_split(df, random_state=42)\n    N = len(df)\n    train_mask = [i in df_tr.index for i in range(N)]\n    test_mask = [i in df_test.index for i in range(N)]\n    train_mask = np.array(train_mask)\n    test_mask = np.array(test_mask)\n    return train_mask, test_mask\n\ndef edge_index_selected(edge_index):\n    theta = edge_index[:,2].mean()\n    edge_index[:,2] = (np.exp(-edge_index[:,2]/theta) != 1)*(np.exp(-edge_index[:,2]/theta))\n    edge_index = edge_index.tolist()\n    mean_ = np.array(edge_index)[:,2].mean()\n    selected_edges = [(int(row[0]), int(row[1])) for row in edge_index if row[2] &gt; mean_]\n    edge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\n    return edge_index_selected\n\n\n\n\nfraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\n\n\n# # fraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\n# fraudTrain = fraudTrain.assign(\n#     trans_date_trans_time= fraudTrain.trans_date_trans_time.apply(pd.to_datetime)\n# )\nfraudTrain = pd.read_pickle('temp.pkl')\n\n\n\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50 = df50.reset_index()\ndf50.shape\n\n(12012, 23)\n\n\n\n\n\n\nmask(df50)\n\ntrain_mask, test_mask = mask(df50)"
  },
  {
    "objectID": "posts/231111.html#데이터정리",
    "href": "posts/231111.html#데이터정리",
    "title": "[FRAUD] 그래프 그림 그리기(graft_정리)",
    "section": "",
    "text": "_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50 = df50.reset_index()\ndf50.shape\n\n(12012, 23)\n\n\n\n\n\n\nmask(df50)\n\ntrain_mask, test_mask = mask(df50)"
  },
  {
    "objectID": "posts/230827 데이터(9, df50 과 다른 것 시도).out.html",
    "href": "posts/230827 데이터(9, df50 과 다른 것 시도).out.html",
    "title": "[FRAUD] 데이터 (9.13_df50 edge다르게)",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\nimport xgboost as xgb\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n# gnn\nimport torch\nimport torch_geometric\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부     \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\ndef build_graph_tripartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n                                                       df[\"cc_num\"].values.tolist() +\n                                                       df[\"merchant\"].values.tolist()))}\n    df[\"in_node\"]= df[\"cc_num\"].apply(lambda x: mapping[x])\n    df[\"out_node\"]=df[\"merchant\"].apply(lambda x:mapping[x])\n    \n        \n    G=nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\\\n                        [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")     \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")   \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")  \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n\n    return G\n    \n    \ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef embedding(Graph):\n    # Graph -&gt; X (feature)\n    _edgs = list(Graph.edges)\n    subGraph = Graph.edge_subgraph([_edgs[x] for x in range(len(Graph.edges))]).copy()\n    subGraph.add_nodes_from(list(set(Graph.nodes) - set(subGraph.nodes)))    \n    embedded = AverageEmbedder(Node2Vec(subGraph, weight_key='weight').fit(window=10).wv)\n    X = [embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in range(len(Graph.edges))]\n    # Graph -&gt; y (label)\n    y = np.array(list(nx.get_edge_attributes(Graph, \"label\").values()))\n    return X,y \n\ndef anal(df):\n    Graph = build_graph_bipartite(df)\n    X,XX,y,yy = embedding(Graph)\n    lrnr = RandomForestClassifier(n_estimators=100, random_state=42) \n    lrnr.fit(X,y)\n    yyhat = lrnr.predict(XX)\n    df = pd.DataFrame({\n        'acc':[sklearn.metrics.accuracy_score(yy,yyhat)], \n        'pre':[sklearn.metrics.precision_score(yy,yyhat)], \n        'rec':[sklearn.metrics.recall_score(yy,yyhat)],\n        'f1':[sklearn.metrics.f1_score(yy,yyhat)]}\n    )    \n    return df\n\ndef our_sampling1(df):\n    cus_list = set(df.query('is_fraud==1').cc_num.tolist())\n    return df.query(\"cc_num in @ cus_list\")\n\n\nfraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\n\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\ndf50 = df50.reset_index()\n\n\nN = len(df50)\n\n\n\n\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.shape, df50_test.shape\n\n((9009, 23), (3003, 23))\n\n\n\ntrain_mask = [i in df50_tr.index for i in range(N)]\ntest_mask = [i in df50_test.index for i in range(N)]\n\n\ntrain_mask = np.array(train_mask)\ntest_mask = np.array(test_mask)\n\n\ntrain_mask.sum(), test_mask.sum()\n\n(9009, 3003)\n\n\n\ntrain_mask.shape, test_mask.shape\n\n((12012,), (12012,))\n\n\n\n\n\n\n\ndef compute_time_difference(group):\n    n = len(group)\n    result = []\n    for i in range(n):\n        for j in range(n):\n            time_difference = abs(group.iloc[i].trans_date_trans_time.value - group.iloc[j].trans_date_trans_time.value)\n            result.append([group.iloc[i].name, group.iloc[j].name, time_difference])\n    return result\n\n\ngroups = df50.groupby('cc_num')\nedge_index_list_plus = [compute_time_difference(group) for _, group in groups]\nedge_index_list_plus_flat = [item for sublist in edge_index_list_plus for item in sublist]\nedge_index_list_plus_nparr = np.array(edge_index_list_plus_flat)\nnp.save('edge_index_list_plus50.npy', edge_index_list_plus_nparr)\n\n\n# edge_index_list_plus = []\n# for i in range(N):\n#     for j in range(N):\n#         if df50['cc_num'][i] != df50['cc_num'][j]:  # cc_num 값이 다르다면\n#             time_difference = 0\n#         else:\n#             time_difference = (df50['trans_date_trans_time'][i] - df50['trans_date_trans_time'][j]).total_seconds()\n#         edge_index_list_plus.append([i, j, time_difference])\n#         np.save('edge_index_list_plus50.npy', edge_index_list_plus)\n\n# # edge_index_list_plus = np.load('edge_index_list_plus.npy')\n\n\nedge_index = np.array(edge_index_list_plus_nparr)\n\n\nedge_index.shape\n\n(200706, 3)\n\n\n\nweight = (np.exp(-edge_index[:,2]/theta) != 1)*(np.exp(-edge_index[:,2]/theta))\nweight\n\narray([0.        , 0.99946775, 0.99020659, ..., 0.98799491, 0.02078565,\n       0.        ])\n\n\n\nedge_index = np.column_stack((edge_index, weight))\nedge_index = np.delete(edge_index, 2, axis=1)\n\n\nedge_index\n\narray([[1.02300000e+03, 1.02300000e+03, 0.00000000e+00],\n       [1.02300000e+03, 1.02400000e+03, 9.99467748e-01],\n       [1.02300000e+03, 1.02800000e+03, 9.90206590e-01],\n       ...,\n       [1.19440000e+04, 9.78200000e+03, 9.87994908e-01],\n       [1.19440000e+04, 1.17670000e+04, 2.07856509e-02],\n       [1.19440000e+04, 1.19440000e+04, 0.00000000e+00]])\n\n\n\nedge_index.shape\n\n(200706, 3)\n\n\n\nedge_index = edge_index.tolist()\n\n\nmean_ = np.array(edge_index)[:,2].mean()\n\n- median\n\nmedi_ = np.median(np.array(edge_index)[:,2])\n\n\nselected_edges = [(int(row[0]), int(row[1])) for row in edge_index if row[2] &gt; medi_]\nedge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\nedge_index_selected.shape\n\ntorch.Size([2, 100350])\n\n\n\n\n\n\n\nx = torch.tensor(df50['amt'], dtype=torch.float).reshape(-1,1)\n\n\ny = torch.tensor(df50['is_fraud'],dtype=torch.int64)\n\n\ndata = torch_geometric.data.Data(x=x, edge_index = edge_index_selected, y=y, train_mask = train_mask, test_mask = test_mask)\n\n\ndata\n\nData(x=[12012, 1], edge_index=[2, 100350], y=[12012], train_mask=[12012], test_mask=[12012])\n\n\n\n\n\n\n\n\nclass GCN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 16)\n        self.conv2 = GCNConv(16,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\n\nmodel = GCN()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(400):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\n\n\nmodel.eval()\npred = model(data).argmax(dim=1)\ncorrect = (pred[data.test_mask] == data.y[data.test_mask]).sum()\nacc = int(correct) / int(np.array(data.test_mask).sum())\nprint(f'Accuracy: {acc:.4f}')\n\nAccuracy: 0.8768\n\n\n\npredicted_labels = pred[data.test_mask]\ntrue_labels = data.y[data.test_mask]\n\n\nprecision = precision_score(true_labels, predicted_labels, average='macro')\nrecall = recall_score(true_labels, predicted_labels, average='macro')\nf1 = f1_score(true_labels, predicted_labels, average='macro')\nprint(f'Precision: {precision:.4f}')\nprint(f'Recall: {recall:.4f}')\nprint(f'F1 Score: {f1:.4f}')\n\nPrecision: 0.8799\nRecall: 0.8763\nF1 Score: 0.8764"
  },
  {
    "objectID": "posts/230827 데이터(9, df50 과 다른 것 시도).out.html#gnn시도",
    "href": "posts/230827 데이터(9, df50 과 다른 것 시도).out.html#gnn시도",
    "title": "[FRAUD] 데이터 (9.13_df50 edge다르게)",
    "section": "",
    "text": "_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\ndf50 = df50.reset_index()\n\n\nN = len(df50)\n\n\n\n\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.shape, df50_test.shape\n\n((9009, 23), (3003, 23))\n\n\n\ntrain_mask = [i in df50_tr.index for i in range(N)]\ntest_mask = [i in df50_test.index for i in range(N)]\n\n\ntrain_mask = np.array(train_mask)\ntest_mask = np.array(test_mask)\n\n\ntrain_mask.sum(), test_mask.sum()\n\n(9009, 3003)\n\n\n\ntrain_mask.shape, test_mask.shape\n\n((12012,), (12012,))\n\n\n\n\n\n\n\ndef compute_time_difference(group):\n    n = len(group)\n    result = []\n    for i in range(n):\n        for j in range(n):\n            time_difference = abs(group.iloc[i].trans_date_trans_time.value - group.iloc[j].trans_date_trans_time.value)\n            result.append([group.iloc[i].name, group.iloc[j].name, time_difference])\n    return result\n\n\ngroups = df50.groupby('cc_num')\nedge_index_list_plus = [compute_time_difference(group) for _, group in groups]\nedge_index_list_plus_flat = [item for sublist in edge_index_list_plus for item in sublist]\nedge_index_list_plus_nparr = np.array(edge_index_list_plus_flat)\nnp.save('edge_index_list_plus50.npy', edge_index_list_plus_nparr)\n\n\n# edge_index_list_plus = []\n# for i in range(N):\n#     for j in range(N):\n#         if df50['cc_num'][i] != df50['cc_num'][j]:  # cc_num 값이 다르다면\n#             time_difference = 0\n#         else:\n#             time_difference = (df50['trans_date_trans_time'][i] - df50['trans_date_trans_time'][j]).total_seconds()\n#         edge_index_list_plus.append([i, j, time_difference])\n#         np.save('edge_index_list_plus50.npy', edge_index_list_plus)\n\n# # edge_index_list_plus = np.load('edge_index_list_plus.npy')\n\n\nedge_index = np.array(edge_index_list_plus_nparr)\n\n\nedge_index.shape\n\n(200706, 3)\n\n\n\nweight = (np.exp(-edge_index[:,2]/theta) != 1)*(np.exp(-edge_index[:,2]/theta))\nweight\n\narray([0.        , 0.99946775, 0.99020659, ..., 0.98799491, 0.02078565,\n       0.        ])\n\n\n\nedge_index = np.column_stack((edge_index, weight))\nedge_index = np.delete(edge_index, 2, axis=1)\n\n\nedge_index\n\narray([[1.02300000e+03, 1.02300000e+03, 0.00000000e+00],\n       [1.02300000e+03, 1.02400000e+03, 9.99467748e-01],\n       [1.02300000e+03, 1.02800000e+03, 9.90206590e-01],\n       ...,\n       [1.19440000e+04, 9.78200000e+03, 9.87994908e-01],\n       [1.19440000e+04, 1.17670000e+04, 2.07856509e-02],\n       [1.19440000e+04, 1.19440000e+04, 0.00000000e+00]])\n\n\n\nedge_index.shape\n\n(200706, 3)\n\n\n\nedge_index = edge_index.tolist()\n\n\nmean_ = np.array(edge_index)[:,2].mean()\n\n- median\n\nmedi_ = np.median(np.array(edge_index)[:,2])\n\n\nselected_edges = [(int(row[0]), int(row[1])) for row in edge_index if row[2] &gt; medi_]\nedge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\nedge_index_selected.shape\n\ntorch.Size([2, 100350])\n\n\n\n\n\n\n\nx = torch.tensor(df50['amt'], dtype=torch.float).reshape(-1,1)\n\n\ny = torch.tensor(df50['is_fraud'],dtype=torch.int64)\n\n\ndata = torch_geometric.data.Data(x=x, edge_index = edge_index_selected, y=y, train_mask = train_mask, test_mask = test_mask)\n\n\ndata\n\nData(x=[12012, 1], edge_index=[2, 100350], y=[12012], train_mask=[12012], test_mask=[12012])\n\n\n\n\n\n\n\n\nclass GCN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 16)\n        self.conv2 = GCNConv(16,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\n\nmodel = GCN()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(400):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\n\n\nmodel.eval()\npred = model(data).argmax(dim=1)\ncorrect = (pred[data.test_mask] == data.y[data.test_mask]).sum()\nacc = int(correct) / int(np.array(data.test_mask).sum())\nprint(f'Accuracy: {acc:.4f}')\n\nAccuracy: 0.8768\n\n\n\npredicted_labels = pred[data.test_mask]\ntrue_labels = data.y[data.test_mask]\n\n\nprecision = precision_score(true_labels, predicted_labels, average='macro')\nrecall = recall_score(true_labels, predicted_labels, average='macro')\nf1 = f1_score(true_labels, predicted_labels, average='macro')\nprint(f'Precision: {precision:.4f}')\nprint(f'Recall: {recall:.4f}')\nprint(f'F1 Score: {f1:.4f}')\n\nPrecision: 0.8799\nRecall: 0.8763\nF1 Score: 0.8764"
  },
  {
    "objectID": "posts/230827 데이터(9, df50 과 다른 것 시도).out.html#분석2로지스틱-회귀",
    "href": "posts/230827 데이터(9, df50 과 다른 것 시도).out.html#분석2로지스틱-회귀",
    "title": "[FRAUD] 데이터 (9.13_df50 edge다르게)",
    "section": "분석2(로지스틱 회귀)",
    "text": "분석2(로지스틱 회귀)\n\nX = np.array(df50_tr.loc[:,['amt']])\nXX = np.array(df50_test.loc[:,['amt']])\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n\nlrnr = sklearn.linear_model.LogisticRegression()\n\n\nlrnr.fit(X,y)\n\n\n#thresh = y.mean()\n#yyhat = (lrnr.predict_proba(XX)&gt; thresh)[:,-1]\nyyhat = lrnr.predict(XX) \n\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n\n_results2= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석2'])\n_results2"
  },
  {
    "objectID": "posts/230827 데이터(9, df50 과 다른 것 시도).out.html#분석3서포트-벡터-머신",
    "href": "posts/230827 데이터(9, df50 과 다른 것 시도).out.html#분석3서포트-벡터-머신",
    "title": "[FRAUD] 데이터 (9.13_df50 edge다르게)",
    "section": "분석3(서포트 벡터 머신)",
    "text": "분석3(서포트 벡터 머신)\n\nX = np.array(df50_tr.loc[:, ['amt']])\nXX = np.array(df50_test.loc[:, ['amt']])\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n\nlrnr = SVC(kernel='linear')  \nlrnr.fit(X,y)\nyyhat = lrnr.predict(XX)\n\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n\n_results3= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석3'])\n_results3"
  },
  {
    "objectID": "posts/230827 데이터(9, df50 과 다른 것 시도).out.html#분석4랜덤-포레스트",
    "href": "posts/230827 데이터(9, df50 과 다른 것 시도).out.html#분석4랜덤-포레스트",
    "title": "[FRAUD] 데이터 (9.13_df50 edge다르게)",
    "section": "분석4(랜덤 포레스트)",
    "text": "분석4(랜덤 포레스트)\n\nX = np.array(df50_tr.loc[:, ['amt']])\nXX = np.array(df50_test.loc[:, ['amt']])\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n\nlrnr = RandomForestClassifier()  \nlrnr.fit(X, y)\nyyhat = lrnr.predict(XX)\n\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n\n_results4= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석4'])\n_results4"
  },
  {
    "objectID": "posts/230827 데이터(9, df50 과 다른 것 시도).out.html#분석5부스팅",
    "href": "posts/230827 데이터(9, df50 과 다른 것 시도).out.html#분석5부스팅",
    "title": "[FRAUD] 데이터 (9.13_df50 edge다르게)",
    "section": "분석5(부스팅)",
    "text": "분석5(부스팅)\n\nX = np.array(df50_tr.loc[:, ['amt']])\nXX = np.array(df50_test.loc[:, ['amt']])\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n\nlrnr = xgb.XGBClassifier()  \nlrnr.fit(X, y)\nyyhat = lrnr.predict(XX)\n\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n\n_results5= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석5'])\n_results5"
  },
  {
    "objectID": "posts/230827 데이터(9, df50 과 다른 것 시도).out.html#분석6naive-bayes",
    "href": "posts/230827 데이터(9, df50 과 다른 것 시도).out.html#분석6naive-bayes",
    "title": "[FRAUD] 데이터 (9.13_df50 edge다르게)",
    "section": "분석6(Naive Bayes)",
    "text": "분석6(Naive Bayes)\n\nX = np.array(df50_tr.loc[:, ['amt']])\nXX = np.array(df50_test.loc[:, ['amt']])\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n\nlrnr = GaussianNB() \nlrnr.fit(X, y)\nyyhat = lrnr.predict(XX)\n\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n\n_results6= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석6'])\n_results6\n\n\n!git add .\n\n\n!git commit -m .\n\n[main f9769619] .\n 8 files changed, 8800 insertions(+), 2514 deletions(-)\n create mode 100644 \"posts/GNN/FRAUD/.ipynb_checkpoints/230822 \\353\\215\\260\\354\\235\\264\\355\\204\\260(6, df02)-Copy1-checkpoint.ipynb\"\n create mode 100644 \"posts/GNN/FRAUD/.ipynb_checkpoints/230822 \\353\\215\\260\\354\\235\\264\\355\\204\\260(6, df02)-checkpoint.ipynb\"\n create mode 100644 \"posts/GNN/FRAUD/.ipynb_checkpoints/230823 \\353\\215\\260\\354\\235\\264\\355\\204\\260(7, df50_com\\354\\234\\274\\353\\241\\234 93\\355\\215\\274 accuracy)_guebin-checkpoint.ipynb\"\n create mode 100644 \"posts/GNN/FRAUD/.ipynb_checkpoints/230825 \\353\\215\\260\\354\\235\\264\\355\\204\\260(8, df02)\\354\\273\\244\\353\\204\\220\\354\\243\\275\\354\\235\\214.out-checkpoint.ipynb\"\n create mode 100644 \"posts/GNN/FRAUD/.ipynb_checkpoints/230827 \\353\\215\\260\\354\\235\\264\\355\\204\\260(9, df50 mask\\353\\247\\214\\353\\223\\244\\354\\227\\210\\353\\212\\224\\353\\215\\260 \\352\\262\\260\\352\\263\\274\\352\\260\\222\\354\\235\\264 \\353\\213\\254\\353\\235\\274).out-checkpoint.ipynb\"\n delete mode 100644 \"posts/GNN/FRAUD/230823 \\353\\215\\260\\354\\235\\264\\355\\204\\260(7, df50_com\\354\\234\\274\\353\\241\\234 93\\355\\215\\274 accuracy)_guebin-Copy1.out.ipynb\"\n rewrite \"posts/GNN/FRAUD/230825 \\353\\215\\260\\354\\235\\264\\355\\204\\260(8, df02)\\354\\273\\244\\353\\204\\220\\354\\243\\275\\354\\235\\214.out.ipynb\" (99%)\n rewrite \"posts/GNN/FRAUD/230827 \\353\\215\\260\\354\\235\\264\\355\\204\\260(9, df50 mask\\353\\247\\214\\353\\223\\244\\354\\227\\210\\353\\212\\224\\353\\215\\260 \\352\\262\\260\\352\\263\\274\\352\\260\\222\\354\\235\\264 \\353\\213\\254\\353\\235\\274).out.ipynb\" (99%)\n\n\n\n!git push\n\nEnumerating objects: 14, done.\nCounting objects: 100% (14/14), done.\nDelta compression using up to 16 threads\nCompressing objects: 100% (9/9), done.\nWriting objects: 100% (9/9), 5.06 KiB | 5.06 MiB/s, done.\nTotal 9 (delta 7), reused 0 (delta 0), pack-reused 0\nremote: Resolving deltas: 100% (7/7), completed with 5 local objects.\nTo https://github.com/boram-coco/coco.git\n   1bbb61bf..f9769619  main -&gt; main\n\n\n\n!quarto publish gh-pages --no-prompt --no-browser\n\nFrom https://github.com/boram-coco/coco\n * branch              gh-pages   -&gt; FETCH_HEAD\nRendering for publish:\n\n[  1/182] posts/Python/Summer Program-Python Programming Day1 Quiz.ipynb\n[  2/182] posts/Python/4. Class/python 11_0511.ipynb\n[  3/182] posts/Python/4. Class/python 13_0530.ipynb\n[  4/182] posts/Python/4. Class/python 10_0509.ipynb\n[  5/182] posts/Python/4. Class/python 12_0523.ipynb\n[  6/182] posts/Python/4. Class/python 13_0525.ipynb\n[  7/182] posts/Python/4. Class/python 12_0518.ipynb\n[  8/182] posts/Python/4. Class/python 11_0516.ipynb\n[  9/182] posts/Python/4. Class/python 14_0606.ipynb\n[ 10/182] posts/Python/1. Basic/python 3_0321.ipynb\n[ 11/182] posts/Python/1. Basic/python 1_0307.ipynb\n[ 12/182] posts/Python/1. Basic/python 4_0323.ipynb\n[ 13/182] posts/Python/1. Basic/python 3_0316.ipynb\n[ 14/182] posts/Python/1. Basic/python 4_0328.ipynb\n[ 15/182] posts/Python/1. Basic/python 2_0314.ipynb\n[ 16/182] posts/Python/3. Pandas/python 10_0506 .ipynb\n[ 17/182] posts/Python/2. Numpy/python 7_0418.ipynb\n[ 18/182] posts/Python/2. Numpy/python 5_0406.ipynb\n[ 19/182] posts/Python/2. Numpy/python 5_0404.ipynb\n[ 20/182] posts/Python/2. Numpy/python 7_0413.ipynb\n[ 21/182] posts/Python/2. Numpy/python 6_0411.ipynb\n[ 22/182] posts/Python/Summer Program-Python Programming Day2 Quiz.ipynb\n[ 23/182] posts/Special Topics in Big Data Analysis/2022-05-23-(12주차) 5월23일.ipynb\n[ 24/182] posts/Special Topics in Big Data Analysis/2022_03_07_(1주차)_3월7일.ipynb\n[ 25/182] posts/Special Topics in Big Data Analysis/2022_04_11_(6주차)_4월11일.ipynb\n[ 26/182] posts/Special Topics in Big Data Analysis/2022_03_14_(2주차)_3월14일.ipynb\n[ 27/182] posts/Special Topics in Big Data Analysis/2022_05_02_(9주차)_5월2일(2).ipynb\n[ 28/182] posts/Special Topics in Big Data Analysis/2022_03_28_(4주차)_3월28일.ipynb\n[ 29/182] posts/Special Topics in Big Data Analysis/2022_05_30_(13주차)_5월30일.ipynb\n[ 30/182] posts/Special Topics in Big Data Analysis/2022_05_09_(10주차)_5월9일.ipynb\n[ 31/182] posts/Special Topics in Big Data Analysis/2022_04_04_(5주차)_4월4일.ipynb\n[ 32/182] posts/Special Topics in Big Data Analysis/2022_05_16_(11주차)_5월16일.ipynb\n[ 33/182] posts/Special Topics in Big Data Analysis/2022_03_21_(3주차)_3월21일.ipynb\n[ 34/182] posts/Special Topics in Big Data Analysis/2022-06-09-(14주차) 6월9일.ipynb\n[ 35/182] posts/Special Topics in Big Data Analysis/2022_04_18_(7주차)_4월18일.ipynb\n[ 36/182] posts/Applied statistics/AS3_3.ipynb\n[ 37/182] posts/Applied statistics/AS4.ipynb\n[ 38/182] posts/Applied statistics/AS1_3.ipynb\n[ 39/182] posts/Applied statistics/AS4_5.ipynb\n[ 40/182] posts/Applied statistics/02. CH0304.ipynb\n[ 41/182] posts/Applied statistics/01. Simple Linear Regression.ipynb\n[ 42/182] posts/Applied statistics/AS1.ipynb\n[ 43/182] posts/Applied statistics/08. 다항회귀실습.ipynb\n[ 44/182] posts/Applied statistics/10. GLS실습.ipynb\n[ 45/182] posts/Applied statistics/alpha.ipynb\n[ 46/182] posts/Applied statistics/06. 회귀진단 실습.ipynb\n[ 47/182] posts/Applied statistics/11. 편의추정 실습.ipynb\n[ 48/182] posts/Applied statistics/04. 선형회귀분석 CH0607.ipynb\n[ 49/182] posts/Applied statistics/12. 로지스틱 회귀분석.ipynb\n[ 50/182] posts/Applied statistics/03. CH0304_simulation.ipynb\n[ 51/182] posts/Applied statistics/AS3.ipynb\n[ 52/182] posts/Applied statistics/05. 가변수 실습.ipynb\n[ 53/182] posts/Applied statistics/AS4_5-Copy1.ipynb\n[ 54/182] posts/Applied statistics/09. 변수변환.ipynb\n[ 55/182] posts/Applied statistics/07. 변수선택 실습.ipynb\n[ 56/182] posts/Applied statistics/AS2.ipynb\n[ 57/182] posts/Synthetic data/synthetic data.ipynb\n[ 58/182] posts/Synthetic data/Practical Synthetic Data Generation.ipynb\n[ 59/182] posts/Synthetic data/A Comparison of Synthetic Data Approaches Using Utility and Disclosure Risk Measures.ipynb\n[ 60/182] posts/Synthetic data/2023-07-01-CTGAN.ipynb\n[ 61/182] posts/Synthetic data/[R] synthpop.ipynb\n[ 62/182] posts/Synthetic data/Modeling Tabular Data using Conditional GAN.ipynb\n[ 63/182] posts/Synthetic data/Advaced Deep Learning with TensorFlow 2 and Keras.ipynb\n[ 64/182] posts/Synthetic data/2023-07-02-CTGAN-TOY.ipynb\n[ 65/182] posts/ref/Ref.ipynb\n[ 66/182] posts/study/주성분 분석.ipynb\n[ 67/182] posts/study/콰트로 블로그 만드는 법.ipynb\n[ 68/182] posts/study/Python Data Analysis/경사하강법.ipynb\n[ 69/182] posts/study/Python Data Analysis/딥러닝 회귀분석.ipynb\n[ 70/182] posts/study/Python Data Analysis/다층 퍼셉트론과 딥러닝.ipynb\n[ 71/182] posts/study/Python Data Analysis/인공신경망과 퍼셉트론.ipynb\n[ 72/182] posts/study/KSS-DataFrame.ipynb\n[ 73/182] posts/study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 7.ipynb\n[ 74/182] posts/study/선형대수와 통계학으로 배우는 머신러닝 with 파이썬/ml with python 9.ipynb\n[ 75/182] posts/study/tutorial_hand_on.ipynb\n[ 76/182] posts/study/imbalaced data/plot_comparison_under_sampling.ipynb\n[ 77/182] posts/study/imbalaced data/imbalaced data.ipynb\n[ 78/182] posts/study/imbalaced data/plot_comparison_over_sampling.ipynb\n[ 79/182] posts/study/boostcourse/2. 데이터 분석 준비하기.ipynb\n[ 80/182] posts/study/boostcourse/0. jupyter basic.ipynb\n[ 81/182] posts/study/boostcourse/3. 서울 종합병원 분포 확인하기.ipynb\n[ 82/182] posts/study/boostcourse/1. file-path-setting.ipynb\n[ 83/182] posts/study/boostcourse/5. K-beauty.ipynb\n[ 84/182] posts/study/boostcourse/4. 건강검진 데이터로 가설검정.ipynb\n[ 85/182] posts/study/데이터 개념 공부.ipynb\n[ 86/182] posts/study/Deep learning with pytorch/Untitled.ipynb\n[ 87/182] posts/study/Deep learning with pytorch/tensor basic.ipynb\n[ 88/182] posts/study/baseball-salary-prediction.ipynb\n[ 89/182] posts/study/sklearn.ipynb\n[ 90/182] posts/Graph Machine Learning/graph3-2.ipynb\n[ 91/182] posts/Graph Machine Learning/graph4-1.ipynb\n[ 92/182] posts/Graph Machine Learning/9999.ipynb\n[ 93/182] posts/Graph Machine Learning/graph basic.ipynb\n[ 94/182] posts/Graph Machine Learning/graph3-3.ipynb\n[ 95/182] posts/Graph Machine Learning/graph8(logistic-amt+time+citypop+lat+merchlat).ipynb\n[ 96/182] posts/Graph Machine Learning/graph8(logistic, amt+time).ipynb\n[ 97/182] posts/Graph Machine Learning/graph8(logistic-amt+time+lat+merchlat).ipynb\n[ 98/182] posts/Graph Machine Learning/graph3-1.ipynb\n[ 99/182] posts/Graph Machine Learning/graph4-2.ipynb\n[100/182] posts/Graph Machine Learning/graph5-1.ipynb\n[101/182] posts/Graph Machine Learning/graph8(frac=0.4).ipynb\n[102/182] posts/Graph Machine Learning/graph5-2.ipynb\n[103/182] posts/Graph Machine Learning/graph8.ipynb\n[104/182] posts/Graph Machine Learning/graph8(frac=0.3).ipynb\n[105/182] posts/Graph Machine Learning/graph2.ipynb\n[106/182] posts/Graph Machine Learning/graph8(logistic+graph).ipynb\n[107/182] posts/Graph Machine Learning/graph8(logistic-amt+time+citypop).ipynb\n[108/182] posts/Graph Machine Learning/graph8.사기거래=0필터.ipynb\n[109/182] posts/Graph Machine Learning/graph8.df원본에서진행.ipynb\n[110/182] posts/Machine Learning/MachineLearning_midterm(202250926).ipynb\n[111/182] posts/Machine Learning/2022_11_29_13wk_2_final_checkpoint_ipynb의_사본.ipynb\n[112/182] posts/Machine Learning/2. CNN/2022_10_26_(8주차)_10월26일(2)_ipynb의_사본.ipynb\n[113/182] posts/Machine Learning/2. CNN/기계학습특강2022_10_19_ipynb의_사본.ipynb\n[114/182] posts/Machine Learning/2022_12_21_Extra_1_ipynb의_사본.ipynb\n[115/182] posts/Machine Learning/2022_09_07_(1주차)_9월7일_ipynb의_사본.ipynb\n[116/182] posts/Machine Learning/3. RNN/2022_11_09_(10주차)_11월9일_ipynb의_사본.ipynb\n[117/182] posts/Machine Learning/3. RNN/2022_10_31_(9주차)_10월31일_ipynb의_사본.ipynb\n[118/182] posts/Machine Learning/3. RNN/2022_11_16_11wk_ipynb의_사본.ipynb\n[119/182] posts/Machine Learning/3. RNN/2022_11_30_12wk_checkpoint_ipynb의_사본.ipynb\n[120/182] posts/Machine Learning/3. RNN/2022_12_08_13wk_checkpoint.ipynb\n[121/182] posts/Machine Learning/2022_09_14_(2주차)_9월14일_ipynb의_사본.ipynb\n[122/182] posts/Machine Learning/(202250926)기계학습특강_final (2).ipynb\n[123/182] posts/Machine Learning/1. DNN/2022_10_12_6wk_checkpoint.ipynb\n[124/182] posts/Machine Learning/1. DNN/2022_09_28_(4주차)_9월 28일__ipynb의_사본.ipynb\n[125/182] posts/Machine Learning/1. DNN/2022_09_21_(3주차)_9월21일_ipynb의_사본.ipynb\n[126/182] posts/Machine Learning/1. DNN/2022_09_28_(5주차)_10월05일_ipynb의_사본.ipynb\n[127/182] posts/Advanved Probability Theory/2. 확률론 기초/2023-05-09-10wk-checkpoint.ipynb\n[128/182] posts/Advanved Probability Theory/2. 확률론 기초/2023-06-06-14wk.ipynb\n[129/182] posts/Advanved Probability Theory/2. 확률론 기초/2023-05-16-11wk-checkpoint.ipynb\n[WARNING] Citeproc: citation durrett2019probability not found\n[130/182] posts/Advanved Probability Theory/2. 확률론 기초/2023-05-23-12wk.ipynb\n[WARNING] Citeproc: citation cybenko1989approximation not found\n[WARNING] Citeproc: citation durrett2019probability not found\n[WARNING] Citeproc: citation makarov2013real not found\n[131/182] posts/Advanved Probability Theory/2. 확률론 기초/2023-05-30-13wk.ipynb\n[WARNING] Citeproc: citation durrett2019probability not found\n[132/182] posts/Advanved Probability Theory/2. 확률론 기초/2023-05-02-9wk-checkpoint.ipynb\n[133/182] posts/Advanved Probability Theory/fin.ipynb\n[134/182] posts/Advanved Probability Theory/1. 측도론/2023-04-25-8wk.ipynb\n[135/182] posts/Advanved Probability Theory/1. 측도론/2023-04-11-6wk-checkpoint.ipynb\n[136/182] posts/Advanved Probability Theory/1. 측도론/2023_03_14_2wk_checkpoint.ipynb\n[137/182] posts/Advanved Probability Theory/1. 측도론/2023_03_07_1wk_checkpoint.ipynb\n[138/182] posts/Advanved Probability Theory/1. 측도론/2023_04_05_5wk_checkpoint.ipynb\n[139/182] posts/Advanved Probability Theory/1. 측도론/2023_03_28_4wk_checkpoint.ipynb\n[140/182] posts/Advanved Probability Theory/1. 측도론/2023-04-18-7wk-checkpoint.ipynb\n[141/182] posts/Advanved Probability Theory/1. 측도론/2023_03_21_3wk_checkpoint.ipynb\n[142/182] posts/Review/Synthetic data/synthetic data.ipynb\n[143/182] posts/Review/Synthetic data/Practical Synthetic Data Generation.ipynb\n[144/182] posts/Review/Synthetic data/A Comparison of Synthetic Data Approaches Using Utility and Disclosure Risk Measures.ipynb\n[145/182] posts/Review/Synthetic data/[R] synthpop.ipynb\n[146/182] posts/Review/Synthetic data/Modeling Tabular Data using Conditional GAN.ipynb\n[147/182] posts/Review/Synthetic data/Advaced Deep Learning with TensorFlow 2 and Keras.ipynb\n[148/182] posts/GNN/PyG/ls2.ipynb\n[149/182] posts/GNN/PyG/ls6.out.ipynb\n[150/182] posts/GNN/PyG/ls5.ipynb\n[151/182] posts/GNN/PyG/2023-07-02-lesson1.ipynb\n[152/182] posts/GNN/PyG/ls3.ipynb\n[153/182] posts/GNN/PyG/ls4.ipynb\n[154/182] posts/GNN/An Introduction to Graph Neural Network(GNN) For Analysing Structured Data.ipynb\n[155/182] posts/GNN/GNN논문.ipynb\n[156/182] posts/GNN/FRAUD/230818 데이터(4, df50_com으로 93퍼 accuracy).ipynb\n[157/182] posts/GNN/FRAUD/230827 데이터(9, df50 mask만들었는데 결과값이 달라).out.ipynb\n[158/182] posts/GNN/FRAUD/230822 데이터(5, matrix로 ls6시도해보기..실패).ipynb\n[159/182] posts/GNN/FRAUD/230825 데이터(8, df02)커널죽음.out.ipynb\n[160/182] posts/GNN/FRAUD/230814 fraud(2, tr,test_mask).ipynb\n[161/182] posts/GNN/FRAUD/230823 데이터(7, df50_com으로 93퍼 accuracy)_guebin.ipynb\n[162/182] posts/GNN/FRAUD/230816 fraud(3, df50_com, tr,test합치기).ipynb\n[163/182] posts/GNN/FRAUD/230822 데이터(6, df02).ipynb\n[164/182] posts/GNN/FRAUD/230810 fraud(1, tr로만 96퍼 accruacy).ipynb\n[165/182] posts/GNN/FRAUD/230822 데이터(6, df02)-Copy1.ipynb\n[166/182] posts/GNN/Neural Network.ipynb\n[167/182] posts/GNN/230810 데이터정리.ipynb\n[168/182] posts/GNN/0810.ipynb\n[169/182] posts/GNN/Graph basic.ipynb\n[170/182] posts/GNN/Laplacian.ipynb\n[171/182] posts/Theoretical statistics/TS5.ipynb\n[172/182] posts/Theoretical statistics/TS7.ipynb\n[173/182] posts/Theoretical statistics/TS4.ipynb\n[174/182] posts/Theoretical statistics/TS3.ipynb\n[175/182] posts/Theoretical statistics/TS1.ipynb\n[176/182] posts/Theoretical statistics/TS8.ipynb\n[177/182] posts/Theoretical statistics/TS9.ipynb\n[178/182] posts/Theoretical statistics/TS final.ipynb\n[179/182] posts/Theoretical statistics/TS2.ipynb\n[180/182] posts/Theoretical statistics/TS6.ipynb\n[181/182] about.qmd\n[182/182] index.qmd\n\nPreparing worktree (resetting branch 'gh-pages'; was at ee6ded06)\nBranch 'gh-pages' set up to track remote branch 'gh-pages' from 'origin'.\nHEAD is now at ee6ded06 Built site for gh-pages\nerror: the following files have local modifications:\n    posts/Advanved Probability Theory/2023_04_05_5wk_checkpoint.html\n    posts/Applied statistics/03. CH0304_simulation_files/figure-html/cell-53-output-1.png\n    posts/Applied statistics/03. CH0304_simulation_files/figure-html/cell-6-output-1.png\n    posts/Applied statistics/05. 가변수 실습.html\n    posts/Applied statistics/06. 회귀진단 실습.html\n    posts/Applied statistics/06. 회귀진단 실습.out.ipynb\n    posts/Applied statistics/06. 회귀진단 실습_files/figure-html/cell-33-output-1.png\n    posts/Applied statistics/06. 회귀진단 실습_files/figure-html/cell-46-output-1.png\n    posts/Applied statistics/06. 회귀진단 실습_files/figure-html/cell-47-output-1.png\n    posts/Applied statistics/06. 회귀진단 실습_files/figure-html/cell-5-output-1.png\n    posts/Applied statistics/06. 회귀진단 실습_files/figure-html/cell-7-output-1.png\n    posts/Applied statistics/07. 변수선택 실습.html\n    posts/Applied statistics/07. 변수선택 실습.out.ipynb\n(use --cached to keep the file, or -f to force removal)\n[gh-pages db1a3b4d] Built site for gh-pages\n 186 files changed, 5197 insertions(+), 6147 deletions(-)\norigin  https://github.com/boram-coco/coco.git (fetch)\norigin  https://github.com/boram-coco/coco.git (push)\nTo https://github.com/boram-coco/coco.git\n   ee6ded06..db1a3b4d  HEAD -&gt; gh-pages\n\nNOTE: GitHub Pages sites use caching so you might need to click the refresh\nbutton within your web browser to see changes after deployment.\n\n[✓] Published to https://boram-coco.github.io/coco/\n\nNOTE: GitHub Pages deployments normally take a few minutes (your site updates\nwill be visible once the deploy completes)"
  },
  {
    "objectID": "posts/230920 데이터(12, df50 auto)-Copy1.html",
    "href": "posts/230920 데이터(12, df50 auto)-Copy1.html",
    "title": "[FRAUD] 데이터정리 시도(9.20_df50 autogluon, seed)",
    "section": "",
    "text": "import sklearn\nfrom sklearn import model_selection\n\nfrom autogluon.tabular import TabularDataset, TabularPredictor\n\n/home/coco/anaconda3/envs/ag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\nimport xgboost as xgb\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# gnn\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric\nfrom torch_geometric.nn import GCNConv\n\n# autogluon\n\n\ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\n\n\nimport pandas as pd\n\n\nfraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\n\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\ndf50 = df50.reset_index()\n\n\nN = len(df50)\n\n\ndf50 = df50[[\"amt\",\"is_fraud\"]]\n\n\ndf50[\"amt\"].mean()\n\n297.4638911088911\n\n\n\ndf50[\"amt\"].describe()\n\ncount    12012.000000\nmean       297.463891\nstd        384.130842\nmin          1.010000\n25%         19.917500\n50%         84.680000\n75%        468.295000\nmax      12025.300000\nName: amt, dtype: float64\n\n\n\n\n\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)"
  },
  {
    "objectID": "posts/230920 데이터(12, df50 auto)-Copy1.html#데이터정리",
    "href": "posts/230920 데이터(12, df50 auto)-Copy1.html#데이터정리",
    "title": "[FRAUD] 데이터정리 시도(9.20_df50 autogluon, seed)",
    "section": "",
    "text": "_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\ndf50 = df50.reset_index()\n\n\nN = len(df50)\n\n\ndf50 = df50[[\"amt\",\"is_fraud\"]]\n\n\ndf50[\"amt\"].mean()\n\n297.4638911088911\n\n\n\ndf50[\"amt\"].describe()\n\ncount    12012.000000\nmean       297.463891\nstd        384.130842\nmin          1.010000\n25%         19.917500\n50%         84.680000\n75%        468.295000\nmax      12025.300000\nName: amt, dtype: float64\n\n\n\n\n\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)"
  },
  {
    "objectID": "posts/230920 데이터(12, df50 auto)-Copy1.html#a.-데이터",
    "href": "posts/230920 데이터(12, df50 auto)-Copy1.html#a.-데이터",
    "title": "[FRAUD] 데이터정리 시도(9.20_df50 autogluon, seed)",
    "section": "A. 데이터",
    "text": "A. 데이터\n\ntr = TabularDataset(df50_tr)\ntst = TabularDataset(df50_test)"
  },
  {
    "objectID": "posts/230920 데이터(12, df50 auto)-Copy1.html#b.-predictor-생성",
    "href": "posts/230920 데이터(12, df50 auto)-Copy1.html#b.-predictor-생성",
    "title": "[FRAUD] 데이터정리 시도(9.20_df50 autogluon, seed)",
    "section": "B. predictor 생성",
    "text": "B. predictor 생성\n\nㅇㅇㅇㅇㅇㅇㅇㅇ으잉?\n\n\npredictr = TabularPredictor(\"is_fraud\")\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20231002_072102\""
  },
  {
    "objectID": "posts/230920 데이터(12, df50 auto)-Copy1.html#c.적합fit",
    "href": "posts/230920 데이터(12, df50 auto)-Copy1.html#c.적합fit",
    "title": "[FRAUD] 데이터정리 시도(9.20_df50 autogluon, seed)",
    "section": "C.적합(fit)",
    "text": "C.적합(fit)\n\n&gt; seed 적\n\n\nseed = 202250926\n\n\npredictr.fit(tr,seed=seed)\n\nValueError: Unknown `.fit` keyword argument specified: 'seed'\nValid kwargs: ['ag_args', 'ag_args_ensemble', 'ag_args_fit', 'auto_stack', 'calibrate', 'excluded_model_types', 'feature_generator', 'feature_prune_kwargs', 'holdout_frac', 'hyperparameter_tune_kwargs', 'included_model_types', 'keep_only_best', 'name_suffix', 'num_bag_folds', 'num_bag_sets', 'num_stack_levels', 'pseudo_data', 'refit_full', 'save_space', 'set_best_to_refit_full', 'unlabeled_data', 'use_bag_holdout', 'verbosity']\n\n\n\npredictr.leaderboard()\n\nAssertionError: Predictor is not fit. Call `.fit` before calling `.leaderboard`."
  },
  {
    "objectID": "posts/230920 데이터(12, df50 auto)-Copy1.html#d.-예측predict",
    "href": "posts/230920 데이터(12, df50 auto)-Copy1.html#d.-예측predict",
    "title": "[FRAUD] 데이터정리 시도(9.20_df50 autogluon, seed)",
    "section": "D. 예측(predict)",
    "text": "D. 예측(predict)\n\n(tr.is_fraud == predictr.predict(tr)).mean()\n\n0.9102009102009102\n\n\n\n(tst.is_fraud == predictr.predict(tst)).mean()\n\n0.8904428904428905\n\n\n\nyyhat = predictr.predict(tr)\n\n\nautogluon이렇게 하는게 맞는감…;;;"
  },
  {
    "objectID": "posts/231011 데이터(18, df50 정리3).html",
    "href": "posts/231011 데이터(18, df50 정리3).html",
    "title": "[FRAUD] 데이터정리 시도(GCNConv3추가,dropout조정)",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\nimport xgboost as xgb\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# gnn\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric\nfrom torch_geometric.nn import GCNConv\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_geometric/typing.py:18: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/libpyg.so: undefined symbol: _ZN2at4_ops12split_Tensor4callERKNS_6TensorEN3c106SymIntEl\n  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_geometric/typing.py:31: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_scatter/_scatter_cuda.so: undefined symbol: _ZNK3c107SymBool10guard_boolEPKcl\n  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_geometric/typing.py:42: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_sparse/_diag_cuda.so: undefined symbol: _ZN3c106detail19maybe_wrap_dim_slowIlEET_S2_S2_b\n  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n\n\n\ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef compute_time_difference(group):\n    n = len(group)\n    result = []\n    for i in range(n):\n        for j in range(n):\n            time_difference = abs(group.iloc[i].trans_date_trans_time.value - group.iloc[j].trans_date_trans_time.value)\n            result.append([group.iloc[i].name, group.iloc[j].name, time_difference])\n    return result\n\ndef mask(df):\n    df_tr,df_test = sklearn.model_selection.train_test_split(df, random_state=42)\n    N = len(df)\n    train_mask = [i in df_tr.index for i in range(N)]\n    test_mask = [i in df_test.index for i in range(N)]\n    train_mask = np.array(train_mask)\n    test_mask = np.array(test_mask)\n    return train_mask, test_mask\n\ndef edge_index_selected(edge_index):\n    theta = edge_index[:,2].mean()\n    edge_index[:,2] = (np.exp(-edge_index[:,2]/theta) != 1)*(np.exp(-edge_index[:,2]/theta))\n    edge_index = edge_index.tolist()\n    mean_ = np.array(edge_index)[:,2].mean()\n    selected_edges = [(int(row[0]), int(row[1])) for row in edge_index if row[2] &gt; mean_]\n    edge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\n    return edge_index_selected\n\n\n\n\nfraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\n\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50 = df50.reset_index()\ndf50.shape\n\n(12012, 23)\n\n\n\n\n\n\nmask(df50)\n\n(array([False,  True,  True, ...,  True, False,  True]),\n array([ True, False, False, ..., False,  True, False]))\n\n\n\ntrain_mask, test_mask = mask(df50)\n\n\n\n\n\n\n# groups = df50.groupby('cc_num')\n# edge_index_list_plus = [compute_time_difference(group) for _, group in groups]\n# edge_index_list_plus_flat = [item for sublist in edge_index_list_plus for item in sublist]\n# edge_index_list_plus_nparr = np.array(edge_index_list_plus_flat)\n# np.save('edge_index_list_plus50.npy', edge_index_list_plus_nparr)\n\n\nedge_index = np.load('edge_index_list_plus50.npy').astype(np.float64)\nedge_index.shape\n\n(200706, 3)\n\n\n\nedge_index_selected = edge_index_selected(edge_index)\n\n\n\n\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    # 지구의 반지름 (미터)\n    radius = 6371.0\n\n    # 라디안으로 변환\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n\n    # Haversine 공식 계산\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n    distance = radius * c\n\n    return distance\n\n# 데이터프레임(df50)에서 고객 위치 및 상점 위치의 위도와 경도 추출\ncustomer_lat = df50['lat']\ncustomer_lon = df50['long']\nstore_lat = df50['merch_lat']\nstore_lon = df50['merch_long']\n\n# 거리 계산\ndistances = haversine(customer_lat, customer_lon, store_lat, store_lon)\n\n# 거리를 데이터프레임에 추가\ndf50['distance_km'] = distances\n\n\ncategory_map = {category: index for index, category in enumerate(df50['category'].unique())}\ndf50['category'] = df50['category'].map(category_map)\n\n\n\n\nx = torch.tensor(df50[['amt', 'category', 'distance_km']].values, dtype=torch.float)\ny = torch.tensor(df50['is_fraud'], dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index=edge_index_selected, y=y, train_mask=train_mask, test_mask=test_mask)\ndata\n\nData(x=[12012, 3], edge_index=[2, 93730], y=[12012], train_mask=[12012], test_mask=[12012])"
  },
  {
    "objectID": "posts/231011 데이터(18, df50 정리3).html#데이터정리",
    "href": "posts/231011 데이터(18, df50 정리3).html#데이터정리",
    "title": "[FRAUD] 데이터정리 시도(GCNConv3추가,dropout조정)",
    "section": "",
    "text": "_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50 = df50.reset_index()\ndf50.shape\n\n(12012, 23)\n\n\n\n\n\n\nmask(df50)\n\n(array([False,  True,  True, ...,  True, False,  True]),\n array([ True, False, False, ..., False,  True, False]))\n\n\n\ntrain_mask, test_mask = mask(df50)\n\n\n\n\n\n\n# groups = df50.groupby('cc_num')\n# edge_index_list_plus = [compute_time_difference(group) for _, group in groups]\n# edge_index_list_plus_flat = [item for sublist in edge_index_list_plus for item in sublist]\n# edge_index_list_plus_nparr = np.array(edge_index_list_plus_flat)\n# np.save('edge_index_list_plus50.npy', edge_index_list_plus_nparr)\n\n\nedge_index = np.load('edge_index_list_plus50.npy').astype(np.float64)\nedge_index.shape\n\n(200706, 3)\n\n\n\nedge_index_selected = edge_index_selected(edge_index)\n\n\n\n\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    # 지구의 반지름 (미터)\n    radius = 6371.0\n\n    # 라디안으로 변환\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n\n    # Haversine 공식 계산\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n    distance = radius * c\n\n    return distance\n\n# 데이터프레임(df50)에서 고객 위치 및 상점 위치의 위도와 경도 추출\ncustomer_lat = df50['lat']\ncustomer_lon = df50['long']\nstore_lat = df50['merch_lat']\nstore_lon = df50['merch_long']\n\n# 거리 계산\ndistances = haversine(customer_lat, customer_lon, store_lat, store_lon)\n\n# 거리를 데이터프레임에 추가\ndf50['distance_km'] = distances\n\n\ncategory_map = {category: index for index, category in enumerate(df50['category'].unique())}\ndf50['category'] = df50['category'].map(category_map)\n\n\n\n\nx = torch.tensor(df50[['amt', 'category', 'distance_km']].values, dtype=torch.float)\ny = torch.tensor(df50['is_fraud'], dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index=edge_index_selected, y=y, train_mask=train_mask, test_mask=test_mask)\ndata\n\nData(x=[12012, 3], edge_index=[2, 93730], y=[12012], train_mask=[12012], test_mask=[12012])"
  },
  {
    "objectID": "posts/231011 데이터(18, df50 정리3).html#분석-1gcn",
    "href": "posts/231011 데이터(18, df50 정리3).html#분석-1gcn",
    "title": "[FRAUD] 데이터정리 시도(GCNConv3추가,dropout조정)",
    "section": "분석 1(GCN)",
    "text": "분석 1(GCN)\n\ntorch.manual_seed(202250926)\nclass GCN1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(data.num_node_features, 32)\n        self.conv2 = GCNConv(32,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\nX = (data.x[data.train_mask]).numpy()\nXX = (data.x[data.test_mask]).numpy()\ny = (data.y[data.train_mask]).numpy()\nyy = (data.y[data.test_mask]).numpy()\n\nmodel = GCN1()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.05, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(400):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\n\n    model.eval()\n\npred = model(data).argmax(dim=1)\nyyhat = pred[data.test_mask]\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results1= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석1'])\n_results1\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석1\n0.914752\n0.868498\n0.979565\n0.920694"
  },
  {
    "objectID": "posts/231011 데이터(18, df50 정리3).html#분석-2gcn-gnnconv3개",
    "href": "posts/231011 데이터(18, df50 정리3).html#분석-2gcn-gnnconv3개",
    "title": "[FRAUD] 데이터정리 시도(GCNConv3추가,dropout조정)",
    "section": "분석 2(GCN): GNNConv3개",
    "text": "분석 2(GCN): GNNConv3개\n- dropout: 0.5\n\n\nx = torch.tensor(df50[['amt', 'category', 'distance_km']].values, dtype=torch.float)\ny = torch.tensor(df50['is_fraud'], dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index=edge_index_selected, y=y, train_mask=train_mask, test_mask=test_mask)\ndata\n\n\ntorch.manual_seed(202250926)\nclass GCN2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(data.num_node_features, 32)\n        self.conv2 = GCNConv(32,64)\n        self.conv3 = GCNConv(64,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)  \n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n\n        x = self.conv3(x, edge_index) \n        return F.log_softmax(x, dim=1)\n\nX = (data.x[data.train_mask]).numpy()\nXX = (data.x[data.test_mask]).numpy()\ny = (data.y[data.train_mask]).numpy()\nyy = (data.y[data.test_mask]).numpy()\n\nmodel = GCN2()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.05, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(400):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\n\n    model.eval()\n\npred = model(data).argmax(dim=1)\nyyhat = pred[data.test_mask]\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results2= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석2'])\n_results2\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석2\n0.900766\n0.848884\n0.977587\n0.908701"
  },
  {
    "objectID": "posts/231011 데이터(18, df50 정리3).html#분석3",
    "href": "posts/231011 데이터(18, df50 정리3).html#분석3",
    "title": "[FRAUD] 데이터정리 시도(GCNConv3추가,dropout조정)",
    "section": "분석3",
    "text": "분석3\n- 분석2 에서 dropout: 0.3\n\n\n\ntorch.manual_seed(202250926)\nclass GCN2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(data.num_node_features, 32)\n        self.conv2 = GCNConv(32,64)\n        self.conv3 = GCNConv(64,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, p=0.3, training=self.training)\n        x = self.conv2(x, edge_index)  \n        x = F.relu(x)\n        x = F.dropout(x, p=0.3, training=self.training)\n\n        x = self.conv3(x, edge_index) \n        return F.log_softmax(x, dim=1)\n\nX = (data.x[data.train_mask]).numpy()\nXX = (data.x[data.test_mask]).numpy()\ny = (data.y[data.train_mask]).numpy()\nyy = (data.y[data.test_mask]).numpy()\n\nmodel = GCN2()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.05, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(400):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\n\n    model.eval()\n\npred = model(data).argmax(dim=1)\nyyhat = pred[data.test_mask]\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results3= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석3'])\n_results3\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석3\n0.879454\n0.869482\n0.895847\n0.882468"
  },
  {
    "objectID": "posts/231011 데이터(18, df50 정리3).html#분석4",
    "href": "posts/231011 데이터(18, df50 정리3).html#분석4",
    "title": "[FRAUD] 데이터정리 시도(GCNConv3추가,dropout조정)",
    "section": "분석4",
    "text": "분석4\n- dropout: 0.2\n\n\nx = torch.tensor(df50[['amt', 'category', 'distance_km']].values, dtype=torch.float)\ny = torch.tensor(df50['is_fraud'], dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index=edge_index_selected, y=y, train_mask=train_mask, test_mask=test_mask)\ndata\n\n\ntorch.manual_seed(202250926)\nclass GCN2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(data.num_node_features, 32)\n        self.conv2 = GCNConv(32,64)\n        self.conv3 = GCNConv(64,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv2(x, edge_index)  \n        x = F.relu(x)\n        x = F.dropout(x, p=0.2, training=self.training)\n\n        x = self.conv3(x, edge_index) \n        return F.log_softmax(x, dim=1)\n\nX = (data.x[data.train_mask]).numpy()\nXX = (data.x[data.test_mask]).numpy()\ny = (data.y[data.train_mask]).numpy()\nyy = (data.y[data.test_mask]).numpy()\n\nmodel = GCN2()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.05, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(400):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\n\n    model.eval()\n\npred = model(data).argmax(dim=1)\nyyhat = pred[data.test_mask]\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results4= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석4'])\n_results4\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석4\n0.912421\n0.862428\n0.98352\n0.919002"
  },
  {
    "objectID": "posts/231011 데이터(18, df50 정리3).html#분석-5",
    "href": "posts/231011 데이터(18, df50 정리3).html#분석-5",
    "title": "[FRAUD] 데이터정리 시도(GCNConv3추가,dropout조정)",
    "section": "분석 5",
    "text": "분석 5\n\ntorch.manual_seed(202250926)\nclass GCN1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(data.num_node_features, 32)\n        self.conv2 = GCNConv(32,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\nX = (data.x[data.train_mask]).numpy()\nXX = (data.x[data.test_mask]).numpy()\ny = (data.y[data.train_mask]).numpy()\nyy = (data.y[data.test_mask]).numpy()\n\nmodel = GCN1()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.05, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(400):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\n\n    model.eval()\n\npred = model(data).argmax(dim=1)\nyyhat = pred[data.test_mask]\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results5= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석5'])\n_results5\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석5\n0.915418\n0.866938\n0.98352\n0.921557"
  },
  {
    "objectID": "posts/231011 데이터(18, df50 정리3).html#분석-6",
    "href": "posts/231011 데이터(18, df50 정리3).html#분석-6",
    "title": "[FRAUD] 데이터정리 시도(GCNConv3추가,dropout조정)",
    "section": "분석 6",
    "text": "분석 6\n\ntorch.manual_seed(202250926)\nclass GCN1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(data.num_node_features, 32)\n        self.conv2 = GCNConv(32,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\nX = (data.x[data.train_mask]).numpy()\nXX = (data.x[data.test_mask]).numpy()\ny = (data.y[data.train_mask]).numpy()\nyy = (data.y[data.test_mask]).numpy()\n\nmodel = GCN1()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.05, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(800):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\n\n    model.eval()\n\npred = model(data).argmax(dim=1)\nyyhat = pred[data.test_mask]\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results6= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석6'])\n_results6\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석6\n0.917416\n0.870403\n0.982861\n0.92322"
  },
  {
    "objectID": "posts/231011 데이터(18, df50 정리3).html#분석-7",
    "href": "posts/231011 데이터(18, df50 정리3).html#분석-7",
    "title": "[FRAUD] 데이터정리 시도(GCNConv3추가,dropout조정)",
    "section": "분석 7",
    "text": "분석 7\n\ntorch.manual_seed(202250926)\nclass GCN1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(data.num_node_features, 32)\n        self.conv2 = GCNConv(32,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\nX = (data.x[data.train_mask]).numpy()\nXX = (data.x[data.test_mask]).numpy()\ny = (data.y[data.train_mask]).numpy()\nyy = (data.y[data.test_mask]).numpy()\n\nmodel = GCN1()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.05, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(800):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\n\n    model.eval()\n\npred = model(data).argmax(dim=1)\nyyhat = pred[data.test_mask]\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results7= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석7'])\n_results7\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석7\n0.899101\n0.846066\n0.978247\n0.907368"
  },
  {
    "objectID": "posts/231011 데이터(18, df50 정리3).html#분석-8",
    "href": "posts/231011 데이터(18, df50 정리3).html#분석-8",
    "title": "[FRAUD] 데이터정리 시도(GCNConv3추가,dropout조정)",
    "section": "분석 8",
    "text": "분석 8\n\ntorch.manual_seed(202250926)\nclass GCN1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(data.num_node_features, 32)\n        self.conv2 = GCNConv(32,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\nX = (data.x[data.train_mask]).numpy()\nXX = (data.x[data.test_mask]).numpy()\ny = (data.y[data.train_mask]).numpy()\nyy = (data.y[data.test_mask]).numpy()\n\nmodel = GCN1()\noptimizer = torch.optim.RMSprop(model.parameters(), lr=0.05, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(800):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\n\n    model.eval()\n\npred = model(data).argmax(dim=1)\nyyhat = pred[data.test_mask]\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results8= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석8'])\n_results8\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석8\n0.891442\n0.828461\n0.990112\n0.902102"
  },
  {
    "objectID": "posts/231011 데이터(18, df50 정리3).html#분석-9",
    "href": "posts/231011 데이터(18, df50 정리3).html#분석-9",
    "title": "[FRAUD] 데이터정리 시도(GCNConv3추가,dropout조정)",
    "section": "분석 9",
    "text": "분석 9\n\ntorch.manual_seed(202250926)\nclass GCN1(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(data.num_node_features, 32)\n        self.conv2 = GCNConv(32,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\nX = (data.x[data.train_mask]).numpy()\nXX = (data.x[data.test_mask]).numpy()\ny = (data.y[data.train_mask]).numpy()\nyy = (data.y[data.test_mask]).numpy()\n\nmodel = GCN1()\noptimizer = torch.optim.Adagrad(model.parameters(), lr=0.05, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(800):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\n\n    model.eval()\n\npred = model(data).argmax(dim=1)\nyyhat = pred[data.test_mask]\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results9= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석9'])\n_results9\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석9\n0.913087\n0.875598\n0.965063\n0.918156"
  },
  {
    "objectID": "posts/2023-11-09-graft.out.html",
    "href": "posts/2023-11-09-graft.out.html",
    "title": "[Essays] graft",
    "section": "",
    "text": "[Essays] graft\n신록예찬\n2023-11-09\n\n\nImports\n소스코드 다운로드: https://github.com/guebin/graft\n\n계속 업데이트할 예정\n\n\n!git clone https://github.com/guebin/graft\n\nfatal: destination path 'graft' already exists and is not an empty directory.\n\n\n\n!conda install -c conda-forge graph-tool -y\n\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n\n==&gt; WARNING: A newer version of conda exists. &lt;==\n  current version: 22.9.0\n  latest version: 23.10.0\n\nPlease update conda by running\n\n    $ conda update -n base -c defaults conda\n\n\n\n# All requested packages already installed.\n\nRetrieving notices: ...working... done\n\n\n\nimport numpy as np\nimport torch\nimport torch_geometric\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nimport graft\n\nImportError: /home/coco/anaconda3/envs/gt/lib/python3.11/site-packages/torch/lib/libgomp-a34b3233.so.1: version `GOMP_5.0' not found (required by /home/coco/anaconda3/envs/gt/lib/python3.11/site-packages/graph_tool/libgraph_tool_core.so)\n\n\n\nlinks = torch.tensor([[0, 1, 2, 3, 4, 3],\n                      [1, 0, 3, 2, 3, 4]], dtype=torch.long)\ng = torch_geometric.data.Data(\n    edge_index = links\n)\n\n\ngraft.graph.plot_undirected_unweighted(g)\n\nNameError: name 'graft' is not defined\n\n\n# Ex – node_names\n\ngraft.graph.plot_undirected_unweighted(\n    g, \n    node_names = ['a','b','c','d','e'], \n)\n\n\n\n\n# Ex – node_color (continuous)\n\nlinks = torch.tensor([[0, 1, 2, 3, 4, 3],\n                      [1, 0, 3, 2, 3, 4]], dtype=torch.long)\ng = torch_geometric.data.Data(\n    edge_index = links, \n    y = np.random.randn(5)\n)\n\n\ngraft.graph.plot_undirected_unweighted(\n    g,\n    node_color=g.y\n)\n\n\n\n\n# Ex – node_color (discrete)\n\nlinks = torch.tensor([[0, 1, 2, 3, 4, 3],\n                      [1, 0, 3, 2, 3, 4]], dtype=torch.long)\ng = torch_geometric.data.Data(\n    edge_index = links, \n    y = torch.tensor([0,1,0,0,1])\n)\n\n\ngraft.graph.plot_undirected_unweighted(\n    g,\n    node_color=g.y\n)\n\n\n\n\n# Ex – node_color (discrete) / node_size\n\nlinks = torch.tensor([[0, 1, 2, 3, 4, 3],\n                      [1, 0, 3, 2, 3, 4]], dtype=torch.long)\ng = torch_geometric.data.Data(\n    edge_index = links, \n    y = torch.tensor([0,1,0,0,1]),\n    x = torch.tensor([10,100,15,20,150])\n)\n\n\ngraft.graph.plot_undirected_unweighted(\n    g,\n    node_color=g.y,\n    node_size=g.x\n)\n\n\n\n\n# Ex – draw options\n\nlinks = torch.tensor([[0, 1, 2, 3, 4, 3],\n                      [1, 0, 3, 2, 3, 4]], dtype=torch.long)\ng = torch_geometric.data.Data(\n    edge_index = links\n)\n\n\ngraft.graph.plot_undirected_unweighted(\n    g,\n)\n\n\n\n\n\ndr_opts = {\n    'vertex_size':30\n}\ngraft.graph.plot_undirected_unweighted(\n    g,\n    draw_options= dr_opts\n)\n\n\n\n\n\ndr_opts = {\n    'edge_marker_size': 200, \n    'output_size': (300,300)\n}\ngraft.graph.plot_undirected_unweighted(\n    g,\n    draw_options= dr_opts\n)\n\n\n\n\n\n\nUndirected / Weighted\n# Ex\n\nlinks = torch.tensor([[0, 1, 2, 3, 4, 3],\n                      [1, 0, 3, 2, 3, 4]], dtype=torch.long)\nweights = torch.tensor([5, 5, 1.5, 1.5, 0.19, 0.19], dtype=torch.float)\n\ng = torch_geometric.data.Data(\n    edge_index=links,\n    edge_attr=weights,\n)\n\n\ngraft.graph.plot_undirected_weighted(\n    g,\n)\n\n\n\n\n# Ex\n\nlinks = torch.tensor([[0, 1, 2, 3, 4, 3],\n                      [1, 0, 3, 2, 3, 4]], dtype=torch.long)\nweights = torch.tensor([5, 5, 1.5, 1.5, 0.19, 0.19], dtype=torch.float)\n\ng = torch_geometric.data.Data(\n    edge_index=links,\n    edge_attr=weights,\n)\n\n\ngraft.graph.plot_undirected_weighted(\n    g,\n    edge_weight_text=False\n)\n\n\n\n\n\ngraft.graph.plot_undirected_weighted(\n    g,\n    edge_weight_text=False,\n    edge_weight_width=False\n)\n\n\n\n\n\ngraft.graph.plot_undirected_weighted(\n    g,\n    edge_weight_text=True,\n    edge_weight_width=True,\n    edge_weight_text_format='.1f',\n)\n\n\n\n\n\ngraft.graph.plot_undirected_weighted(\n    g,\n    edge_weight_text=True,\n    edge_weight_width=True,\n    edge_weight_text_format='.1f',\n    edge_weight_width_scale=5.0,\n)\n\n\n\n\n# Ex\n\nlinks = torch.tensor([[0, 1, 2, 3, 4, 3],\n                      [1, 0, 3, 2, 3, 4]], dtype=torch.long)\nweights = torch.tensor([5, 5, 1.5, 1.5, 0.19, 0.19], dtype=torch.float)\n\ng = torch_geometric.data.Data(\n    edge_index=links,\n    edge_attr=weights,\n    y = torch.tensor([1,1,0,0,0])\n)\n\n\ngraft.graph.plot_undirected_weighted(\n    g,\n    node_color=g.y\n)\n\n\n\n\n# Ex\n\nlinks = torch.tensor([[0, 1, 2, 3, 4, 3],\n                      [1, 0, 3, 2, 3, 4]], dtype=torch.long)\nweights = torch.tensor([5, 5, 1.5, 1.5, 0.19, 0.19], dtype=torch.float)\ng = torch_geometric.data.Data(\n    edge_index = links, \n    edge_attr = weights,\n    y = torch.tensor([0,1,0,0,1]),\n    x = torch.tensor([1,3,1,2,4])\n)\n\n\ngraft.graph.plot_undirected_weighted(\n    g,\n    node_color=g.y,\n    node_size=g.x,\n    edge_weight_text=False,\n)"
  },
  {
    "objectID": "posts/230913 데이터(11, df50 정리).html",
    "href": "posts/230913 데이터(11, df50 정리).html",
    "title": "[FRAUD] 데이터정리 시도(9.18_df50 정리중(~ing))",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\nimport xgboost as xgb\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# gnn\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric\nfrom torch_geometric.nn import GCNConv\n\n\n\ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef compute_time_difference(group):\n    n = len(group)\n    result = []\n    for i in range(n):\n        for j in range(n):\n            time_difference = abs(group.iloc[i].trans_date_trans_time.value - group.iloc[j].trans_date_trans_time.value)\n            result.append([group.iloc[i].name, group.iloc[j].name, time_difference])\n    return result\n\n\nclass GCN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 16)\n        self.conv2 = GCNConv(16,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\n\nfraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\n\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50 = df50.reset_index()\ndf50.shape\n\n(12012, 23)\n\n\n\n\n\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\ndf50_tr.shape, df50_test.shape\n\n((9009, 23), (3003, 23))\n\n\n\nN = len(df50)\ntrain_mask = [i in df50_tr.index for i in range(N)]\ntest_mask = [i in df50_test.index for i in range(N)]\ntrain_mask = np.array(train_mask)\ntest_mask = np.array(test_mask)\ntrain_mask.sum(), test_mask.sum()\n\n(9009, 3003)\n\n\n\ntrain_mask.shape, test_mask.shape\n\n((12012,), (12012,))\n\n\n\n\n\n\n\n# groups = df50.groupby('cc_num')\n# edge_index_list_plus = [compute_time_difference(group) for _, group in groups]\n# edge_index_list_plus_flat = [item for sublist in edge_index_list_plus for item in sublist]\n# edge_index_list_plus_nparr = np.array(edge_index_list_plus_flat)\n# np.save('edge_index_list_plus50.npy', edge_index_list_plus_nparr)\n\n\nedge_index = np.load('edge_index_list_plus50.npy')\nedge_index.shape\n\n(200706, 3)\n\n\n\ntheta = edge_index[:,2].mean()\nedge_index = np.load('edge_index_list_plus50.npy').astype(np.float64)\nedge_index[:,2] = (np.exp(-edge_index[:,2]/theta) != 1)*(np.exp(-edge_index[:,2]/theta))\nedge_index = edge_index.tolist()\nmean_ = np.array(edge_index)[:,2].mean()\nmean_\n\n0.5098736436405648\n\n\n\nedge_index[:5]\n\n[[1023.0, 1023.0, 0.0],\n [1023.0, 1024.0, 0.9994677478343093],\n [1023.0, 1028.0, 0.9902065900321946],\n [1023.0, 1031.0, 0.97983815585674],\n [1023.0, 1032.0, 0.97983815585674]]\n\n\n\nselected_edges = [(int(row[0]), int(row[1])) for row in edge_index if row[2] &gt; mean_]\nedge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\nedge_index_selected.shape\n\ntorch.Size([2, 93730])\n\n\n\n\n\n\n\nx = torch.tensor(df50['amt'], dtype=torch.float).reshape(-1,1)\ny = torch.tensor(df50['is_fraud'],dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index = edge_index_selected, y=y, train_mask = train_mask, test_mask = test_mask)\ndata\n\nData(x=[12012, 1], edge_index=[2, 93730], y=[12012], train_mask=[12012], test_mask=[12012])"
  },
  {
    "objectID": "posts/230913 데이터(11, df50 정리).html#데이터정리",
    "href": "posts/230913 데이터(11, df50 정리).html#데이터정리",
    "title": "[FRAUD] 데이터정리 시도(9.18_df50 정리중(~ing))",
    "section": "",
    "text": "_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50 = df50.reset_index()\ndf50.shape\n\n(12012, 23)\n\n\n\n\n\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\ndf50_tr.shape, df50_test.shape\n\n((9009, 23), (3003, 23))\n\n\n\nN = len(df50)\ntrain_mask = [i in df50_tr.index for i in range(N)]\ntest_mask = [i in df50_test.index for i in range(N)]\ntrain_mask = np.array(train_mask)\ntest_mask = np.array(test_mask)\ntrain_mask.sum(), test_mask.sum()\n\n(9009, 3003)\n\n\n\ntrain_mask.shape, test_mask.shape\n\n((12012,), (12012,))\n\n\n\n\n\n\n\n# groups = df50.groupby('cc_num')\n# edge_index_list_plus = [compute_time_difference(group) for _, group in groups]\n# edge_index_list_plus_flat = [item for sublist in edge_index_list_plus for item in sublist]\n# edge_index_list_plus_nparr = np.array(edge_index_list_plus_flat)\n# np.save('edge_index_list_plus50.npy', edge_index_list_plus_nparr)\n\n\nedge_index = np.load('edge_index_list_plus50.npy')\nedge_index.shape\n\n(200706, 3)\n\n\n\ntheta = edge_index[:,2].mean()\nedge_index = np.load('edge_index_list_plus50.npy').astype(np.float64)\nedge_index[:,2] = (np.exp(-edge_index[:,2]/theta) != 1)*(np.exp(-edge_index[:,2]/theta))\nedge_index = edge_index.tolist()\nmean_ = np.array(edge_index)[:,2].mean()\nmean_\n\n0.5098736436405648\n\n\n\nedge_index[:5]\n\n[[1023.0, 1023.0, 0.0],\n [1023.0, 1024.0, 0.9994677478343093],\n [1023.0, 1028.0, 0.9902065900321946],\n [1023.0, 1031.0, 0.97983815585674],\n [1023.0, 1032.0, 0.97983815585674]]\n\n\n\nselected_edges = [(int(row[0]), int(row[1])) for row in edge_index if row[2] &gt; mean_]\nedge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\nedge_index_selected.shape\n\ntorch.Size([2, 93730])\n\n\n\n\n\n\n\nx = torch.tensor(df50['amt'], dtype=torch.float).reshape(-1,1)\ny = torch.tensor(df50['is_fraud'],dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index = edge_index_selected, y=y, train_mask = train_mask, test_mask = test_mask)\ndata\n\nData(x=[12012, 1], edge_index=[2, 93730], y=[12012], train_mask=[12012], test_mask=[12012])"
  },
  {
    "objectID": "posts/230913 데이터(11, df50 정리).html#분석-1gnn",
    "href": "posts/230913 데이터(11, df50 정리).html#분석-1gnn",
    "title": "[FRAUD] 데이터정리 시도(9.18_df50 정리중(~ing))",
    "section": "분석 1(GNN)",
    "text": "분석 1(GNN)\n\nX = (data.x[data.train_mask]).numpy()\nXX = (data.x[data.test_mask]).numpy()\ny = (data.y[data.train_mask]).numpy()\nyy = (data.y[data.test_mask]).numpy()\n\n\nmodel = GCN()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(400):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\nmodel.eval()\n\nGCN(\n  (conv1): GCNConv(1, 16)\n  (conv2): GCNConv(16, 2)\n)\n\n\n\npred = model(data).argmax(dim=1)\nyyhat = pred[data.test_mask]\n\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n\n_results1= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석1'])\n_results1\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석1\n0.889111\n0.865884\n0.923533\n0.89378"
  },
  {
    "objectID": "posts/230913 데이터(11, df50 정리).html#분석2로지스틱-회귀",
    "href": "posts/230913 데이터(11, df50 정리).html#분석2로지스틱-회귀",
    "title": "[FRAUD] 데이터정리 시도(9.18_df50 정리중(~ing))",
    "section": "분석2(로지스틱 회귀)",
    "text": "분석2(로지스틱 회귀)\n\nX = np.array(df50_tr.loc[:,['amt']])\nXX = np.array(df50_test.loc[:,['amt']])\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n\nlrnr = sklearn.linear_model.LogisticRegression()\n\n\nlrnr.fit(X,y)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\n#thresh = y.mean()\n#yyhat = (lrnr.predict_proba(XX)&gt; thresh)[:,-1]\nyyhat = lrnr.predict(XX) \n\n\nyyhat\n\narray([0, 1, 0, ..., 0, 0, 1])\n\n\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n\n_results2= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석2'])\n_results2\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석2\n0.849484\n0.933279\n0.756098\n0.835397"
  },
  {
    "objectID": "posts/230913 데이터(11, df50 정리).html#분석3서포트-벡터-머신",
    "href": "posts/230913 데이터(11, df50 정리).html#분석3서포트-벡터-머신",
    "title": "[FRAUD] 데이터정리 시도(9.18_df50 정리중(~ing))",
    "section": "분석3(서포트 벡터 머신)",
    "text": "분석3(서포트 벡터 머신)\n\nX = np.array(df50_tr.loc[:, ['amt']])\nXX = np.array(df50_test.loc[:, ['amt']])\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n\nlrnr = SVC(kernel='linear')  \nlrnr.fit(X,y)\nyyhat = lrnr.predict(XX)\n\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n\n_results3= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석3'])\n_results3\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석3\n0.85015\n0.93551\n0.755438\n0.835886"
  },
  {
    "objectID": "posts/230913 데이터(11, df50 정리).html#분석4랜덤-포레스트",
    "href": "posts/230913 데이터(11, df50 정리).html#분석4랜덤-포레스트",
    "title": "[FRAUD] 데이터정리 시도(9.18_df50 정리중(~ing))",
    "section": "분석4(랜덤 포레스트)",
    "text": "분석4(랜덤 포레스트)\n\nX = np.array(df50_tr.loc[:, ['amt']])\nXX = np.array(df50_test.loc[:, ['amt']])\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n\nlrnr = RandomForestClassifier()  \nlrnr.fit(X, y)\nyyhat = lrnr.predict(XX)\n\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n\n_results4= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석4'])\n_results4\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석4\n0.847153\n0.850331\n0.846407\n0.848365"
  },
  {
    "objectID": "posts/230913 데이터(11, df50 정리).html#분석5부스팅",
    "href": "posts/230913 데이터(11, df50 정리).html#분석5부스팅",
    "title": "[FRAUD] 데이터정리 시도(9.18_df50 정리중(~ing))",
    "section": "분석5(부스팅)",
    "text": "분석5(부스팅)\n\nX = np.array(df50_tr.loc[:, ['amt']])\nXX = np.array(df50_test.loc[:, ['amt']])\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n\nlrnr = xgb.XGBClassifier()  \nlrnr.fit(X, y)\nyyhat = lrnr.predict(XX)\n\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n\n_results5= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석5'])\n_results5\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석5\n0.88012\n0.886957\n0.874094\n0.880478"
  },
  {
    "objectID": "posts/230913 데이터(11, df50 정리).html#분석6naive-bayes",
    "href": "posts/230913 데이터(11, df50 정리).html#분석6naive-bayes",
    "title": "[FRAUD] 데이터정리 시도(9.18_df50 정리중(~ing))",
    "section": "분석6(Naive Bayes)",
    "text": "분석6(Naive Bayes)\n\nX = np.array(df50_tr.loc[:, ['amt']])\nXX = np.array(df50_test.loc[:, ['amt']])\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n\nlrnr = GaussianNB() \nlrnr.fit(X, y)\nyyhat = lrnr.predict(XX)\n\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n\n_results6= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석6'])\n_results6\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석6\n0.857143\n0.957143\n0.750824\n0.841522"
  },
  {
    "objectID": "posts/230810 fraud(1, tr로만 96퍼 accruacy).html",
    "href": "posts/230810 fraud(1, tr로만 96퍼 accruacy).html",
    "title": "[FRAUD] 데이터정리 시도(1, tr로만 96퍼 accuracy)",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\nimport torch\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics \n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부     \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\ndef build_graph_tripartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n                                                       df[\"cc_num\"].values.tolist() +\n                                                       df[\"merchant\"].values.tolist()))}\n    df[\"in_node\"]= df[\"cc_num\"].apply(lambda x: mapping[x])\n    df[\"out_node\"]=df[\"merchant\"].apply(lambda x:mapping[x])\n    \n        \n    G=nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\\\n                        [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")     \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")   \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")  \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n\n    return G\n    \n    \ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef embedding(Graph):\n    # Graph -&gt; X (feature)\n    _edgs = list(Graph.edges)\n    subGraph = Graph.edge_subgraph([_edgs[x] for x in range(len(Graph.edges))]).copy()\n    subGraph.add_nodes_from(list(set(Graph.nodes) - set(subGraph.nodes)))    \n    embedded = AverageEmbedder(Node2Vec(subGraph, weight_key='weight').fit(window=10).wv)\n    X = [embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in range(len(Graph.edges))]\n    # Graph -&gt; y (label)\n    y = np.array(list(nx.get_edge_attributes(Graph, \"label\").values()))\n    return X,y \n\ndef anal(df):\n    Graph = build_graph_bipartite(df)\n    X,XX,y,yy = embedding(Graph)\n    lrnr = RandomForestClassifier(n_estimators=100, random_state=42) \n    lrnr.fit(X,y)\n    yyhat = lrnr.predict(XX)\n    df = pd.DataFrame({\n        'acc':[sklearn.metrics.accuracy_score(yy,yyhat)], \n        'pre':[sklearn.metrics.precision_score(yy,yyhat)], \n        'rec':[sklearn.metrics.recall_score(yy,yyhat)],\n        'f1':[sklearn.metrics.f1_score(yy,yyhat)]}\n    )    \n    return df\n\ndef our_sampling1(df):\n    cus_list = set(df.query('is_fraud==1').cc_num.tolist())\n    return df.query(\"cc_num in @ cus_list\")\n\n- 모든엣지를 고려하는 방법\n\nN = 10 \nedge_index = torch.tensor([[i,j] for i in range(N) for j in range(N)]).T\n# edge_attr = 그래프의 웨이트 \n\n\nedge_index\n\ntensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2,\n         2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4,\n         4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7,\n         7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9,\n         9, 9, 9, 9],\n        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3,\n         4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7,\n         8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1,\n         2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5,\n         6, 7, 8, 9]])\n\n\n\nfraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n- 시간 차이 계산하려면?\n\ndiff = fraudTrain.trans_date_trans_time[101]-fraudTrain.trans_date_trans_time[0]\n\n\ndiff\n\nTimedelta('0 days 01:17:00')\n\n\n\ndiff.total_seconds()\n\n4620.0\n\n\n- 적당한 theta값을 정하자.\n\ntheta = 86400*1.2\ntheta\n\n103680.0\n\n\n\ntheta = 86400*1.2\nnp.exp(-diff.total_seconds()/theta)\n\n0.9564180361647693\n\n\n\n\n\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\nN = len(fraudTrain)\nN\n\n1048575\n\n\n\ndf02을 이용해서 해보자.\n\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\n214520*214520\n\n46018830400\n\n\n\n# N = len(df02)\n# edge_index = torch.tensor([[i,j] for i in range(N) for j in range(N)]).T\n\n\ndf50\n\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\n12012*12012\n\n144288144\n\n\n\n\n\ndf50 의 shape이 12000개 이므로 9000개의 T, 3000개의 F를 train mask로 만들자.\n고객정보가 동일하면 edge를 1로, 아니면 0으로 놓고 1에대한 weight를 만들자.\ng(V,E,W)에서의 weight\n\n\ndf50 = df50.reset_index()\n\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.is_fraud.mean().round(5), df50_test.is_fraud.mean().round(5)\n\n(0.49828, 0.50516)\n\n\n\n\n\n\n현재 df50의 fraud 비율은 5:5 인데, 다른 비율을 가진 데이터로도 해보자\nGNN으로 돌려본 것과 다른 방법들과 비교를 해보자\nundersampling한 다른 데이터들과 비교해 볼 수 있을 듯(boost, logis, …)\n9000/3000 데이터를 통해 합성 데이터를 만드는데, 12000개를 그대로 만드는 방법, 고객별로(cc_num) 합성 데이터를 만드는 방법, 똑같은 cc_num로 특이한 데이터가 있다면 normal데이터와 특이 데이터를 생각해서 돌리는 방법 등을 고려하자.\n\n\ndf50_tr.shape, df50_test.shape\n\n((9009, 23), (3003, 23))\n\n\n\nN = len(df50_tr)\n#edge_index = torch.tensor([[i,j] for i in range(N) for j in range(N)]).T\n#edge_index\n\n\ndf50_tr = df50_tr.reset_index()\n\n\nedge_index_list = []\nfor i in range(N):\n    for j in range(N):\n        time_difference = (df50_tr['trans_date_trans_time'][i] - df50_tr['trans_date_trans_time'][j]).total_seconds()\n        edge_index_list.append([i, j, time_difference])\n\n\nedge_index = np.array(edge_index_list)\n\n\nedge_index.shape\n\n(81162081, 3)\n\n\n\nedge_index[:,2] = np.abs(edge_index[:,2])\n\n\ntheta = edge_index[:,2].mean()\ntheta\n\n12230796.273867842\n\n\n\nedge_index[:,2] = (np.exp(-edge_index[:,2]/theta)!=1) * np.exp(-edge_index[:,2]/theta)\n\n\nedge_index\n\narray([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 1.00000000e+00, 1.90157975e-01],\n       [0.00000000e+00, 2.00000000e+00, 9.79646259e-02],\n       ...,\n       [9.00800000e+03, 9.00600000e+03, 6.60662164e-01],\n       [9.00800000e+03, 9.00700000e+03, 1.49150646e-01],\n       [9.00800000e+03, 9.00800000e+03, 0.00000000e+00]])\n\n\n\neee = edge_index[:,:]\n\n\neee[:,1]\n\narray([0.000e+00, 1.000e+00, 2.000e+00, ..., 9.006e+03, 9.007e+03,\n       9.008e+03])\n\n\n\nedge_index_list_updated = edge_index.tolist()\n\n\nedge_index_list_updated[:5]\n\n[[0.0, 0.0, 0.0],\n [0.0, 1.0, 0.19015797528259762],\n [0.0, 2.0, 0.09796462590589798],\n [0.0, 3.0, 0.1424157407389685],\n [0.0, 4.0, 0.11107338192969567]]\n\n\n\ndf50_tr\n\n\n\n\n\n\n\n\nlevel_0\nindex\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n476\n51331\n2019-01-31 00:44:00\n3.543590e+15\nfraud_Medhurst PLC\nshopping_net\n921.24\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nc8928ba53be26fdd997b26f7130c757e\n1327970678\n40.064488\n-78.210499\n1\n\n\n1\n3671\n625691\n2019-09-23 00:09:00\n2.610530e+15\nfraud_Torphy-Goyette\nshopping_pos\n698.28\nTanya\nDickerson\nF\n...\n36.2416\n-86.6117\n22191\nPrison officer\n1994-07-27\n90453290b765904ed1c3426882a6788b\n1348358993\n35.884288\n-87.513318\n1\n\n\n2\n6641\n896244\n2019-12-25 21:30:00\n6.011330e+15\nfraud_Monahan-Morar\npersonal_care\n220.56\nLauren\nButler\nF\n...\n36.0557\n-96.0602\n413574\nTeacher, special educational needs\n1971-09-01\n4072a3effcf51cf7cf88f69d00642cd9\n1356471044\n35.789798\n-95.859736\n0\n\n\n3\n4288\n717690\n2019-11-02 22:22:00\n6.011380e+15\nfraud_Daugherty, Pouros and Beahan\nshopping_pos\n905.43\nMartin\nDuarte\nM\n...\n44.6001\n-84.2931\n864\nGeneral practice doctor\n1942-05-04\nf2fa1b25eef2f43fa5c09e3e1bfe7f77\n1351894926\n44.652759\n-84.500359\n1\n\n\n4\n4770\n815813\n2019-12-08 02:50:00\n4.430880e+15\nfraud_Hudson-Ratke\ngrocery_pos\n307.98\nAlicia\nMorales\nF\n...\n39.3199\n-106.6596\n61\nPublic relations account executive\n1939-11-04\nf06eff8da349e36e623cff026de8e970\n1354935056\n38.389399\n-106.111026\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9004\n11964\n177703\n2019-04-02 21:48:00\n3.572980e+15\nfraud_Ziemann-Waters\nhealth_fitness\n63.89\nWilliam\nLopez\nM\n...\n41.1832\n-96.9882\n614\nAssociate Professor\n1967-06-20\n5b19aad28d65a6b0a912fa7b9d1896de\n1333403300\n42.067169\n-96.876892\n0\n\n\n9005\n5191\n921796\n2019-12-30 23:29:00\n6.762920e+11\nfraud_Wiza, Schaden and Stark\nmisc_pos\n51.41\nLisa\nFitzpatrick\nF\n...\n41.2336\n-75.2389\n104\nFinancial trader\n1927-08-25\nb2a9e44026fc57e54b4e45ade6017668\n1356910178\n40.502189\n-74.814956\n1\n\n\n9006\n5390\n950365\n2020-01-16 03:15:00\n4.807550e+12\nfraud_Murray-Smitham\ngrocery_pos\n357.62\nKimberly\nCastro\nF\n...\n40.2158\n-83.9579\n133\nProfessor Emeritus\n1954-01-29\n4bfa37c329f327074e7220ea6e5d8f8d\n1358306148\n40.620284\n-84.274495\n1\n\n\n9007\n860\n88685\n2019-02-22 02:19:00\n5.738600e+11\nfraud_McDermott-Weimann\ngrocery_pos\n304.75\nCristian\nJones\nM\n...\n42.0765\n-87.7246\n27020\nTrade mark attorney\n1986-07-23\na1c3025ddb615ab2ef890bf82fc3d66a\n1329877195\n42.722479\n-88.362364\n1\n\n\n9008\n7270\n753787\n2019-11-18 10:58:00\n6.042293e+10\nfraud_Terry, Johns and Bins\nmisc_pos\n1.64\nJeffrey\nPowers\nM\n...\n33.6028\n-81.9748\n46944\nSecondary school teacher\n1942-04-02\nee10d61782bde2b5cabc2ad649e977cc\n1353236287\n34.243599\n-82.971344\n0\n\n\n\n\n9009 rows × 24 columns\n\n\n\n\ncc_num로 그룹별로 묶자.\n\n\ndf50_tr[df50_tr['cc_num']==3.543590e+15]\n\n\n\n\n\n\n\n\nlevel_0\nindex\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n476\n51331\n2019-01-31 00:44:00\n3.543590e+15\nfraud_Medhurst PLC\nshopping_net\n921.24\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nc8928ba53be26fdd997b26f7130c757e\n1327970678\n40.064488\n-78.210499\n1\n\n\n344\n462\n50905\n2019-01-30 16:53:00\n3.543590e+15\nfraud_Lesch Ltd\nshopping_pos\n881.11\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n9f7b7675c4decefd03cce56df045ed1c\n1327942400\n39.591484\n-79.575246\n1\n\n\n1377\n6607\n814736\n2019-12-07 22:17:00\n3.543590e+15\nfraud_Botsford and Sons\nhome\n10.41\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\naa9b533e84970309a4ad60a914a8cd77\n1354918668\n41.287791\n-79.980592\n0\n\n\n1447\n485\n51816\n2019-01-31 12:38:00\n3.543590e+15\nfraud_Ruecker-Mayert\nkids_pets\n21.93\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\ncec656f154e0978b0f26702c29ddeeca\n1328013517\n39.946187\n-78.078864\n1\n\n\n1639\n11176\n12947\n2019-01-08 11:08:00\n3.543590e+15\nfraud_Stroman, Hudson and Erdman\ngas_transport\n76.03\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nc10451edc4e21b865d049312acf18ecd\n1326020892\n39.503960\n-78.471680\n0\n\n\n2046\n8124\n627045\n2019-09-23 12:53:00\n3.543590e+15\nfraud_Botsford Ltd\nshopping_pos\n3.20\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n003d591d208f7ee52277b5cc4fa4a37f\n1348404838\n40.066686\n-79.326630\n0\n\n\n2093\n477\n51367\n2019-01-31 01:36:00\n3.543590e+15\nfraud_Watsica, Haag and Considine\nshopping_pos\n1090.67\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nb42bc0820a78de54845c5138b9c39dd5\n1327973774\n40.923284\n-78.882504\n1\n\n\n2415\n491\n52402\n2019-01-31 22:17:00\n3.543590e+15\nfraud_Metz, Russel and Metz\nkids_pets\n22.35\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n51f9352216e99bbe9e8b03b082305971\n1328048275\n39.979547\n-78.851379\n1\n\n\n2625\n463\n51047\n2019-01-30 19:35:00\n3.543590e+15\nfraud_Ruecker-Mayert\nkids_pets\n22.95\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n8e804422b761537e3a49a237afd1ea9a\n1327952100\n40.051981\n-79.021769\n1\n\n\n2769\n478\n51374\n2019-01-31 01:42:00\n3.543590e+15\nfraud_Schmidt and Sons\nshopping_net\n1043.59\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nbbe4e9e431cba66e6531199ffaf79657\n1327974178\n40.192896\n-79.366393\n1\n\n\n3192\n505\n52522\n2019-01-31 23:57:00\n3.543590e+15\nfraud_Kutch, Steuber and Gerhold\nfood_dining\n116.45\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nfcf46ca0264437bbb938c29eca2c92ad\n1328054256\n40.288401\n-78.286914\n1\n\n\n3670\n11714\n1010269\n2020-02-20 06:02:00\n3.543590e+15\nfraud_Huels-Hahn\ngas_transport\n51.80\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nb72c0124f4c5662db13e1bea2f04784b\n1361340164\n39.672719\n-79.642589\n0\n\n\n3945\n6087\n243892\n2019-05-02 13:38:00\n3.543590e+15\nfraud_Cruickshank-Mills\nentertainment\n5.72\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n990da059d387e5fa7481d76ff5c29199\n1335965925\n40.577553\n-79.315460\n0\n\n\n5017\n484\n51431\n2019-01-31 03:28:00\n3.543590e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n741.98\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n41312d7d5fc76be3782b5e9cef04726f\n1327980509\n41.290570\n-79.682069\n1\n\n\n5505\n8148\n181398\n2019-04-04 23:32:00\n3.543590e+15\nfraud_Feil, Hilpert and Koss\nfood_dining\n89.23\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\ne304fd4ebc897fce190925dadcd2b524\n1333582347\n39.736380\n-79.481667\n0\n\n\n5729\n11116\n329202\n2019-06-06 03:26:00\n3.543590e+15\nfraud_Connelly, Reichert and Fritsch\ngas_transport\n69.36\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nbc0832ac8bac6d26548ab6ab553d5d5e\n1338953171\n40.780469\n-79.668417\n0\n\n\n7605\n481\n51392\n2019-01-31 02:16:00\n3.543590e+15\nfraud_Huels-Hahn\ngas_transport\n12.41\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n5f4379c2fc20457f0f99a126cadda1af\n1327976216\n39.884234\n-79.374966\n1\n\n\n7800\n8609\n55920\n2019-02-03 06:51:00\n3.543590e+15\nfraud_Corwin-Gorczany\nmisc_net\n6.70\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n90f33381b6b6644c6d03c8cdb51d05dc\n1328251865\n40.064532\n-78.920283\n0\n\n\n8100\n10488\n509733\n2019-08-09 11:47:00\n3.543590e+15\nfraud_Kutch and Sons\ngrocery_pos\n108.74\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n40a620cc7c5ba396b1fe112f5361e4a9\n1344512838\n40.057443\n-78.569798\n0\n\n\n8313\n504\n52514\n2019-01-31 23:52:00\n3.543590e+15\nfraud_Douglas, Schneider and Turner\nshopping_pos\n1129.56\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nec208107f178422e0953560343d0cf8b\n1328053975\n40.840340\n-78.027854\n1\n\n\n\n\n20 rows × 24 columns\n\n\n\n\ndf50_grouped=df50_tr.groupby(by='cc_num')\n\n\nedge_index_list = []\nfor i in range(N):\n    for j in range(N):\n        if df50_tr['cc_num'][i] != df50_tr['cc_num'][j]:  # cc_num 값이 같다면\n            time_difference = 0\n        else:\n            time_difference = (df50_tr['trans_date_trans_time'][i] - df50_tr['trans_date_trans_time'][j]).total_seconds()\n        edge_index_list.append([i, j, time_difference])\n\n\nedge_index = np.array(edge_index_list)\n\n\nedge_index.shape\n\n(81162081, 3)\n\n\n\nedge_index\n\narray([[0.000e+00, 0.000e+00, 0.000e+00],\n       [0.000e+00, 1.000e+00, 0.000e+00],\n       [0.000e+00, 2.000e+00, 0.000e+00],\n       ...,\n       [9.008e+03, 9.006e+03, 0.000e+00],\n       [9.008e+03, 9.007e+03, 0.000e+00],\n       [9.008e+03, 9.008e+03, 0.000e+00]])\n\n\n\nedge_index[:,2] = np.abs(edge_index[:,2])\n\n\ntheta = edge_index[:,2].mean()\ntheta\n\n10988.585252761077\n\n\n\nedge_index\n\narray([[0.000e+00, 0.000e+00, 0.000e+00],\n       [0.000e+00, 1.000e+00, 0.000e+00],\n       [0.000e+00, 2.000e+00, 0.000e+00],\n       ...,\n       [9.008e+03, 9.006e+03, 0.000e+00],\n       [9.008e+03, 9.007e+03, 0.000e+00],\n       [9.008e+03, 9.008e+03, 0.000e+00]])\n\n\n\nedge_index[:,2] = (np.exp(-edge_index[:,2]/theta)!=1) * np.exp(-edge_index[:,2]/theta)\n\n\nedge_index\n\narray([[0.000e+00, 0.000e+00, 0.000e+00],\n       [0.000e+00, 1.000e+00, 0.000e+00],\n       [0.000e+00, 2.000e+00, 0.000e+00],\n       ...,\n       [9.008e+03, 9.006e+03, 0.000e+00],\n       [9.008e+03, 9.007e+03, 0.000e+00],\n       [9.008e+03, 9.008e+03, 0.000e+00]])\n\n\n\nedge_index_list_updated = edge_index.tolist()\n\n\nnp.array(edge_index_list_updated)[:,2].mean()\n\n8.344409093328692e-05\n\n\n\nmm = np.array(edge_index_list_updated)[:,2].mean()\n\nedge_index_list_updated가 w\n\nselected_edges = [(int(row[0]), int(row[1])) for row in edge_index_list_updated if row[2] &gt; mm]\n\n\nedge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\n\n\nedge_index_selected.shape\n\ntorch.Size([2, 28472])\n\n\n\nedge_index\n\narray([[0.000e+00, 0.000e+00, 0.000e+00],\n       [0.000e+00, 1.000e+00, 0.000e+00],\n       [0.000e+00, 2.000e+00, 0.000e+00],\n       ...,\n       [9.008e+03, 9.006e+03, 0.000e+00],\n       [9.008e+03, 9.007e+03, 0.000e+00],\n       [9.008e+03, 9.008e+03, 0.000e+00]])\n\n\n\n\n\n\nedge_index 돌아가는 게 너무 오래걸려서 이렇게 저장해놓으면 빠르게 실행할 수 있다.\n\n\n#import numpy as np\n\n#data = np.array([1, 2, 3, 4, 5])\nnp.save('edge_index.npy', edge_index)\n\nloaded_data = np.load('edge_index.npy')\n\n\nnpy로 끝나는 건 위에처럼 저장하기 아님 피클로!ㅡ, torch방법\n\n\nx = df50_tr['amt']\n\n\nx\n\n0       921.24\n1       698.28\n2       220.56\n3       905.43\n4       307.98\n         ...  \n9004     63.89\n9005     51.41\n9006    357.62\n9007    304.75\n9008      1.64\nName: amt, Length: 9009, dtype: float64\n\n\n\na = torch.tensor(x, dtype=torch.float)\n\n\na = a.reshape(-1,1)\na\n\ntensor([[921.2400],\n        [698.2800],\n        [220.5600],\n        ...,\n        [357.6200],\n        [304.7500],\n        [  1.6400]])\n\n\n\ny = df50_tr['is_fraud']\n\n\nb = torch.tensor(y,dtype=torch.int64)\n\n\nb\n\ntensor([1, 1, 0,  ..., 1, 1, 0])\n\n\n\nimport torch_geometric\n\n\ndata = torch_geometric.data.Data(x=a, edge_index = edge_index_selected, y=b)\n\n\ndata\n\nData(x=[9009, 1], edge_index=[2, 28472], y=[9009])\n\n\n- pyg lesson6\n\ngconv = torch_geometric.nn.GCNConv(1,4)\ngconv\n\nGCNConv(1, 4)\n\n\n\ngconv(data.x, data.edge_index)\n\ntensor([[ 4.0225e+02,  2.5312e+02, -2.9747e+02, -1.6831e+02],\n        [ 3.7246e+02,  2.3437e+02, -2.7543e+02, -1.5584e+02],\n        [ 1.5695e+02,  9.8760e+01, -1.1606e+02, -6.5670e+01],\n        ...,\n        [ 2.5448e+02,  1.6013e+02, -1.8818e+02, -1.0648e+02],\n        [ 5.4738e+02,  3.4444e+02, -4.0478e+02, -2.2903e+02],\n        [ 1.1670e+00,  7.3434e-01, -8.6299e-01, -4.8830e-01]],\n       grad_fn=&lt;AddBackward0&gt;)\n\n\n\nlist(gconv.parameters())\n\n[Parameter containing:\n tensor([0., 0., 0., 0.], requires_grad=True),\n Parameter containing:\n tensor([[ 0.7116],\n         [ 0.4478],\n         [-0.5262],\n         [-0.2977]], requires_grad=True)]\n\n\n\n_,W = list(gconv.parameters())\nW\n\nParameter containing:\ntensor([[-0.6724],\n        [ 0.7172],\n        [-0.3185],\n        [ 0.5363]], requires_grad=True)\n\n\n- pyg lesson5\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\n\nclass GCN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 16)\n        self.conv2 = GCNConv(16,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\n\nmodel = GCN()\n\n\nmodel\n\nGCN(\n  (conv1): GCNConv(1, 16)\n  (conv2): GCNConv(16, 2)\n)\n\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\n\nGCN(\n  (conv1): GCNConv(1, 16)\n  (conv2): GCNConv(16, 2)\n)\n\n\n\nout\n\ntensor([[-1.8963e+02,  0.0000e+00],\n        [-1.5192e+02,  0.0000e+00],\n        [-5.3630e+01,  0.0000e+00],\n        ...,\n        [-3.0590e+02,  0.0000e+00],\n        [-3.0298e+02,  0.0000e+00],\n        [-1.3924e+00, -2.8567e-01]], grad_fn=&lt;LogSoftmaxBackward0&gt;)\n\n\n\ndata.y\n\ntensor([1, 1, 0,  ..., 1, 1, 0])\n\n\n\nfor epoch in range(200):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out, data.y)\n    loss.backward()\n    optimizer.step()\n\n\nmodel.eval()\npred = model(data).argmax(dim=1)\ncorrect = (pred == data.y).sum() # 애큐러시는 test\nacc = int(correct) / 9009\nprint(f'Accuracy: {acc:.4f}')\n\nAccuracy: 0.9633\n\n\n\nfraud_mask = (data.y == 1)\n\n\nmodel.eval()\npred = model(data).argmax(dim=1)\ncorrect = (pred[fraud_mask] == data.y[fraud_mask]).sum() # 애큐러시는 test\nacc = int(correct) / int(fraud_mask.sum())\nprint(f'recall: {acc:.4f}')\n\nrecall: 0.9619\n\n\n\n위의 recall은 test가 없어서 train으로만 했던 거..!"
  },
  {
    "objectID": "posts/230810 fraud(1, tr로만 96퍼 accruacy).html#시도",
    "href": "posts/230810 fraud(1, tr로만 96퍼 accruacy).html#시도",
    "title": "[FRAUD] 데이터정리 시도(1, tr로만 96퍼 accuracy)",
    "section": "",
    "text": "fraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\nN = len(fraudTrain)\nN\n\n1048575\n\n\n\ndf02을 이용해서 해보자.\n\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\n214520*214520\n\n46018830400\n\n\n\n# N = len(df02)\n# edge_index = torch.tensor([[i,j] for i in range(N) for j in range(N)]).T\n\n\ndf50\n\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\n12012*12012\n\n144288144\n\n\n\n\n\ndf50 의 shape이 12000개 이므로 9000개의 T, 3000개의 F를 train mask로 만들자.\n고객정보가 동일하면 edge를 1로, 아니면 0으로 놓고 1에대한 weight를 만들자.\ng(V,E,W)에서의 weight\n\n\ndf50 = df50.reset_index()\n\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.is_fraud.mean().round(5), df50_test.is_fraud.mean().round(5)\n\n(0.49828, 0.50516)\n\n\n\n\n\n\n현재 df50의 fraud 비율은 5:5 인데, 다른 비율을 가진 데이터로도 해보자\nGNN으로 돌려본 것과 다른 방법들과 비교를 해보자\nundersampling한 다른 데이터들과 비교해 볼 수 있을 듯(boost, logis, …)\n9000/3000 데이터를 통해 합성 데이터를 만드는데, 12000개를 그대로 만드는 방법, 고객별로(cc_num) 합성 데이터를 만드는 방법, 똑같은 cc_num로 특이한 데이터가 있다면 normal데이터와 특이 데이터를 생각해서 돌리는 방법 등을 고려하자.\n\n\ndf50_tr.shape, df50_test.shape\n\n((9009, 23), (3003, 23))\n\n\n\nN = len(df50_tr)\n#edge_index = torch.tensor([[i,j] for i in range(N) for j in range(N)]).T\n#edge_index\n\n\ndf50_tr = df50_tr.reset_index()\n\n\nedge_index_list = []\nfor i in range(N):\n    for j in range(N):\n        time_difference = (df50_tr['trans_date_trans_time'][i] - df50_tr['trans_date_trans_time'][j]).total_seconds()\n        edge_index_list.append([i, j, time_difference])\n\n\nedge_index = np.array(edge_index_list)\n\n\nedge_index.shape\n\n(81162081, 3)\n\n\n\nedge_index[:,2] = np.abs(edge_index[:,2])\n\n\ntheta = edge_index[:,2].mean()\ntheta\n\n12230796.273867842\n\n\n\nedge_index[:,2] = (np.exp(-edge_index[:,2]/theta)!=1) * np.exp(-edge_index[:,2]/theta)\n\n\nedge_index\n\narray([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 1.00000000e+00, 1.90157975e-01],\n       [0.00000000e+00, 2.00000000e+00, 9.79646259e-02],\n       ...,\n       [9.00800000e+03, 9.00600000e+03, 6.60662164e-01],\n       [9.00800000e+03, 9.00700000e+03, 1.49150646e-01],\n       [9.00800000e+03, 9.00800000e+03, 0.00000000e+00]])\n\n\n\neee = edge_index[:,:]\n\n\neee[:,1]\n\narray([0.000e+00, 1.000e+00, 2.000e+00, ..., 9.006e+03, 9.007e+03,\n       9.008e+03])\n\n\n\nedge_index_list_updated = edge_index.tolist()\n\n\nedge_index_list_updated[:5]\n\n[[0.0, 0.0, 0.0],\n [0.0, 1.0, 0.19015797528259762],\n [0.0, 2.0, 0.09796462590589798],\n [0.0, 3.0, 0.1424157407389685],\n [0.0, 4.0, 0.11107338192969567]]\n\n\n\ndf50_tr\n\n\n\n\n\n\n\n\nlevel_0\nindex\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n476\n51331\n2019-01-31 00:44:00\n3.543590e+15\nfraud_Medhurst PLC\nshopping_net\n921.24\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nc8928ba53be26fdd997b26f7130c757e\n1327970678\n40.064488\n-78.210499\n1\n\n\n1\n3671\n625691\n2019-09-23 00:09:00\n2.610530e+15\nfraud_Torphy-Goyette\nshopping_pos\n698.28\nTanya\nDickerson\nF\n...\n36.2416\n-86.6117\n22191\nPrison officer\n1994-07-27\n90453290b765904ed1c3426882a6788b\n1348358993\n35.884288\n-87.513318\n1\n\n\n2\n6641\n896244\n2019-12-25 21:30:00\n6.011330e+15\nfraud_Monahan-Morar\npersonal_care\n220.56\nLauren\nButler\nF\n...\n36.0557\n-96.0602\n413574\nTeacher, special educational needs\n1971-09-01\n4072a3effcf51cf7cf88f69d00642cd9\n1356471044\n35.789798\n-95.859736\n0\n\n\n3\n4288\n717690\n2019-11-02 22:22:00\n6.011380e+15\nfraud_Daugherty, Pouros and Beahan\nshopping_pos\n905.43\nMartin\nDuarte\nM\n...\n44.6001\n-84.2931\n864\nGeneral practice doctor\n1942-05-04\nf2fa1b25eef2f43fa5c09e3e1bfe7f77\n1351894926\n44.652759\n-84.500359\n1\n\n\n4\n4770\n815813\n2019-12-08 02:50:00\n4.430880e+15\nfraud_Hudson-Ratke\ngrocery_pos\n307.98\nAlicia\nMorales\nF\n...\n39.3199\n-106.6596\n61\nPublic relations account executive\n1939-11-04\nf06eff8da349e36e623cff026de8e970\n1354935056\n38.389399\n-106.111026\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9004\n11964\n177703\n2019-04-02 21:48:00\n3.572980e+15\nfraud_Ziemann-Waters\nhealth_fitness\n63.89\nWilliam\nLopez\nM\n...\n41.1832\n-96.9882\n614\nAssociate Professor\n1967-06-20\n5b19aad28d65a6b0a912fa7b9d1896de\n1333403300\n42.067169\n-96.876892\n0\n\n\n9005\n5191\n921796\n2019-12-30 23:29:00\n6.762920e+11\nfraud_Wiza, Schaden and Stark\nmisc_pos\n51.41\nLisa\nFitzpatrick\nF\n...\n41.2336\n-75.2389\n104\nFinancial trader\n1927-08-25\nb2a9e44026fc57e54b4e45ade6017668\n1356910178\n40.502189\n-74.814956\n1\n\n\n9006\n5390\n950365\n2020-01-16 03:15:00\n4.807550e+12\nfraud_Murray-Smitham\ngrocery_pos\n357.62\nKimberly\nCastro\nF\n...\n40.2158\n-83.9579\n133\nProfessor Emeritus\n1954-01-29\n4bfa37c329f327074e7220ea6e5d8f8d\n1358306148\n40.620284\n-84.274495\n1\n\n\n9007\n860\n88685\n2019-02-22 02:19:00\n5.738600e+11\nfraud_McDermott-Weimann\ngrocery_pos\n304.75\nCristian\nJones\nM\n...\n42.0765\n-87.7246\n27020\nTrade mark attorney\n1986-07-23\na1c3025ddb615ab2ef890bf82fc3d66a\n1329877195\n42.722479\n-88.362364\n1\n\n\n9008\n7270\n753787\n2019-11-18 10:58:00\n6.042293e+10\nfraud_Terry, Johns and Bins\nmisc_pos\n1.64\nJeffrey\nPowers\nM\n...\n33.6028\n-81.9748\n46944\nSecondary school teacher\n1942-04-02\nee10d61782bde2b5cabc2ad649e977cc\n1353236287\n34.243599\n-82.971344\n0\n\n\n\n\n9009 rows × 24 columns\n\n\n\n\ncc_num로 그룹별로 묶자.\n\n\ndf50_tr[df50_tr['cc_num']==3.543590e+15]\n\n\n\n\n\n\n\n\nlevel_0\nindex\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n476\n51331\n2019-01-31 00:44:00\n3.543590e+15\nfraud_Medhurst PLC\nshopping_net\n921.24\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nc8928ba53be26fdd997b26f7130c757e\n1327970678\n40.064488\n-78.210499\n1\n\n\n344\n462\n50905\n2019-01-30 16:53:00\n3.543590e+15\nfraud_Lesch Ltd\nshopping_pos\n881.11\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n9f7b7675c4decefd03cce56df045ed1c\n1327942400\n39.591484\n-79.575246\n1\n\n\n1377\n6607\n814736\n2019-12-07 22:17:00\n3.543590e+15\nfraud_Botsford and Sons\nhome\n10.41\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\naa9b533e84970309a4ad60a914a8cd77\n1354918668\n41.287791\n-79.980592\n0\n\n\n1447\n485\n51816\n2019-01-31 12:38:00\n3.543590e+15\nfraud_Ruecker-Mayert\nkids_pets\n21.93\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\ncec656f154e0978b0f26702c29ddeeca\n1328013517\n39.946187\n-78.078864\n1\n\n\n1639\n11176\n12947\n2019-01-08 11:08:00\n3.543590e+15\nfraud_Stroman, Hudson and Erdman\ngas_transport\n76.03\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nc10451edc4e21b865d049312acf18ecd\n1326020892\n39.503960\n-78.471680\n0\n\n\n2046\n8124\n627045\n2019-09-23 12:53:00\n3.543590e+15\nfraud_Botsford Ltd\nshopping_pos\n3.20\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n003d591d208f7ee52277b5cc4fa4a37f\n1348404838\n40.066686\n-79.326630\n0\n\n\n2093\n477\n51367\n2019-01-31 01:36:00\n3.543590e+15\nfraud_Watsica, Haag and Considine\nshopping_pos\n1090.67\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nb42bc0820a78de54845c5138b9c39dd5\n1327973774\n40.923284\n-78.882504\n1\n\n\n2415\n491\n52402\n2019-01-31 22:17:00\n3.543590e+15\nfraud_Metz, Russel and Metz\nkids_pets\n22.35\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n51f9352216e99bbe9e8b03b082305971\n1328048275\n39.979547\n-78.851379\n1\n\n\n2625\n463\n51047\n2019-01-30 19:35:00\n3.543590e+15\nfraud_Ruecker-Mayert\nkids_pets\n22.95\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n8e804422b761537e3a49a237afd1ea9a\n1327952100\n40.051981\n-79.021769\n1\n\n\n2769\n478\n51374\n2019-01-31 01:42:00\n3.543590e+15\nfraud_Schmidt and Sons\nshopping_net\n1043.59\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nbbe4e9e431cba66e6531199ffaf79657\n1327974178\n40.192896\n-79.366393\n1\n\n\n3192\n505\n52522\n2019-01-31 23:57:00\n3.543590e+15\nfraud_Kutch, Steuber and Gerhold\nfood_dining\n116.45\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nfcf46ca0264437bbb938c29eca2c92ad\n1328054256\n40.288401\n-78.286914\n1\n\n\n3670\n11714\n1010269\n2020-02-20 06:02:00\n3.543590e+15\nfraud_Huels-Hahn\ngas_transport\n51.80\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nb72c0124f4c5662db13e1bea2f04784b\n1361340164\n39.672719\n-79.642589\n0\n\n\n3945\n6087\n243892\n2019-05-02 13:38:00\n3.543590e+15\nfraud_Cruickshank-Mills\nentertainment\n5.72\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n990da059d387e5fa7481d76ff5c29199\n1335965925\n40.577553\n-79.315460\n0\n\n\n5017\n484\n51431\n2019-01-31 03:28:00\n3.543590e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n741.98\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n41312d7d5fc76be3782b5e9cef04726f\n1327980509\n41.290570\n-79.682069\n1\n\n\n5505\n8148\n181398\n2019-04-04 23:32:00\n3.543590e+15\nfraud_Feil, Hilpert and Koss\nfood_dining\n89.23\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\ne304fd4ebc897fce190925dadcd2b524\n1333582347\n39.736380\n-79.481667\n0\n\n\n5729\n11116\n329202\n2019-06-06 03:26:00\n3.543590e+15\nfraud_Connelly, Reichert and Fritsch\ngas_transport\n69.36\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nbc0832ac8bac6d26548ab6ab553d5d5e\n1338953171\n40.780469\n-79.668417\n0\n\n\n7605\n481\n51392\n2019-01-31 02:16:00\n3.543590e+15\nfraud_Huels-Hahn\ngas_transport\n12.41\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n5f4379c2fc20457f0f99a126cadda1af\n1327976216\n39.884234\n-79.374966\n1\n\n\n7800\n8609\n55920\n2019-02-03 06:51:00\n3.543590e+15\nfraud_Corwin-Gorczany\nmisc_net\n6.70\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n90f33381b6b6644c6d03c8cdb51d05dc\n1328251865\n40.064532\n-78.920283\n0\n\n\n8100\n10488\n509733\n2019-08-09 11:47:00\n3.543590e+15\nfraud_Kutch and Sons\ngrocery_pos\n108.74\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n40a620cc7c5ba396b1fe112f5361e4a9\n1344512838\n40.057443\n-78.569798\n0\n\n\n8313\n504\n52514\n2019-01-31 23:52:00\n3.543590e+15\nfraud_Douglas, Schneider and Turner\nshopping_pos\n1129.56\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nec208107f178422e0953560343d0cf8b\n1328053975\n40.840340\n-78.027854\n1\n\n\n\n\n20 rows × 24 columns\n\n\n\n\ndf50_grouped=df50_tr.groupby(by='cc_num')\n\n\nedge_index_list = []\nfor i in range(N):\n    for j in range(N):\n        if df50_tr['cc_num'][i] != df50_tr['cc_num'][j]:  # cc_num 값이 같다면\n            time_difference = 0\n        else:\n            time_difference = (df50_tr['trans_date_trans_time'][i] - df50_tr['trans_date_trans_time'][j]).total_seconds()\n        edge_index_list.append([i, j, time_difference])\n\n\nedge_index = np.array(edge_index_list)\n\n\nedge_index.shape\n\n(81162081, 3)\n\n\n\nedge_index\n\narray([[0.000e+00, 0.000e+00, 0.000e+00],\n       [0.000e+00, 1.000e+00, 0.000e+00],\n       [0.000e+00, 2.000e+00, 0.000e+00],\n       ...,\n       [9.008e+03, 9.006e+03, 0.000e+00],\n       [9.008e+03, 9.007e+03, 0.000e+00],\n       [9.008e+03, 9.008e+03, 0.000e+00]])\n\n\n\nedge_index[:,2] = np.abs(edge_index[:,2])\n\n\ntheta = edge_index[:,2].mean()\ntheta\n\n10988.585252761077\n\n\n\nedge_index\n\narray([[0.000e+00, 0.000e+00, 0.000e+00],\n       [0.000e+00, 1.000e+00, 0.000e+00],\n       [0.000e+00, 2.000e+00, 0.000e+00],\n       ...,\n       [9.008e+03, 9.006e+03, 0.000e+00],\n       [9.008e+03, 9.007e+03, 0.000e+00],\n       [9.008e+03, 9.008e+03, 0.000e+00]])\n\n\n\nedge_index[:,2] = (np.exp(-edge_index[:,2]/theta)!=1) * np.exp(-edge_index[:,2]/theta)\n\n\nedge_index\n\narray([[0.000e+00, 0.000e+00, 0.000e+00],\n       [0.000e+00, 1.000e+00, 0.000e+00],\n       [0.000e+00, 2.000e+00, 0.000e+00],\n       ...,\n       [9.008e+03, 9.006e+03, 0.000e+00],\n       [9.008e+03, 9.007e+03, 0.000e+00],\n       [9.008e+03, 9.008e+03, 0.000e+00]])\n\n\n\nedge_index_list_updated = edge_index.tolist()\n\n\nnp.array(edge_index_list_updated)[:,2].mean()\n\n8.344409093328692e-05\n\n\n\nmm = np.array(edge_index_list_updated)[:,2].mean()\n\nedge_index_list_updated가 w\n\nselected_edges = [(int(row[0]), int(row[1])) for row in edge_index_list_updated if row[2] &gt; mm]\n\n\nedge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\n\n\nedge_index_selected.shape\n\ntorch.Size([2, 28472])\n\n\n\nedge_index\n\narray([[0.000e+00, 0.000e+00, 0.000e+00],\n       [0.000e+00, 1.000e+00, 0.000e+00],\n       [0.000e+00, 2.000e+00, 0.000e+00],\n       ...,\n       [9.008e+03, 9.006e+03, 0.000e+00],\n       [9.008e+03, 9.007e+03, 0.000e+00],\n       [9.008e+03, 9.008e+03, 0.000e+00]])\n\n\n\n\n\n\nedge_index 돌아가는 게 너무 오래걸려서 이렇게 저장해놓으면 빠르게 실행할 수 있다.\n\n\n#import numpy as np\n\n#data = np.array([1, 2, 3, 4, 5])\nnp.save('edge_index.npy', edge_index)\n\nloaded_data = np.load('edge_index.npy')\n\n\nnpy로 끝나는 건 위에처럼 저장하기 아님 피클로!ㅡ, torch방법\n\n\nx = df50_tr['amt']\n\n\nx\n\n0       921.24\n1       698.28\n2       220.56\n3       905.43\n4       307.98\n         ...  \n9004     63.89\n9005     51.41\n9006    357.62\n9007    304.75\n9008      1.64\nName: amt, Length: 9009, dtype: float64\n\n\n\na = torch.tensor(x, dtype=torch.float)\n\n\na = a.reshape(-1,1)\na\n\ntensor([[921.2400],\n        [698.2800],\n        [220.5600],\n        ...,\n        [357.6200],\n        [304.7500],\n        [  1.6400]])\n\n\n\ny = df50_tr['is_fraud']\n\n\nb = torch.tensor(y,dtype=torch.int64)\n\n\nb\n\ntensor([1, 1, 0,  ..., 1, 1, 0])\n\n\n\nimport torch_geometric\n\n\ndata = torch_geometric.data.Data(x=a, edge_index = edge_index_selected, y=b)\n\n\ndata\n\nData(x=[9009, 1], edge_index=[2, 28472], y=[9009])\n\n\n- pyg lesson6\n\ngconv = torch_geometric.nn.GCNConv(1,4)\ngconv\n\nGCNConv(1, 4)\n\n\n\ngconv(data.x, data.edge_index)\n\ntensor([[ 4.0225e+02,  2.5312e+02, -2.9747e+02, -1.6831e+02],\n        [ 3.7246e+02,  2.3437e+02, -2.7543e+02, -1.5584e+02],\n        [ 1.5695e+02,  9.8760e+01, -1.1606e+02, -6.5670e+01],\n        ...,\n        [ 2.5448e+02,  1.6013e+02, -1.8818e+02, -1.0648e+02],\n        [ 5.4738e+02,  3.4444e+02, -4.0478e+02, -2.2903e+02],\n        [ 1.1670e+00,  7.3434e-01, -8.6299e-01, -4.8830e-01]],\n       grad_fn=&lt;AddBackward0&gt;)\n\n\n\nlist(gconv.parameters())\n\n[Parameter containing:\n tensor([0., 0., 0., 0.], requires_grad=True),\n Parameter containing:\n tensor([[ 0.7116],\n         [ 0.4478],\n         [-0.5262],\n         [-0.2977]], requires_grad=True)]\n\n\n\n_,W = list(gconv.parameters())\nW\n\nParameter containing:\ntensor([[-0.6724],\n        [ 0.7172],\n        [-0.3185],\n        [ 0.5363]], requires_grad=True)\n\n\n- pyg lesson5\n\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\n\nclass GCN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 16)\n        self.conv2 = GCNConv(16,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\n\nmodel = GCN()\n\n\nmodel\n\nGCN(\n  (conv1): GCNConv(1, 16)\n  (conv2): GCNConv(16, 2)\n)\n\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\n\nGCN(\n  (conv1): GCNConv(1, 16)\n  (conv2): GCNConv(16, 2)\n)\n\n\n\nout\n\ntensor([[-1.8963e+02,  0.0000e+00],\n        [-1.5192e+02,  0.0000e+00],\n        [-5.3630e+01,  0.0000e+00],\n        ...,\n        [-3.0590e+02,  0.0000e+00],\n        [-3.0298e+02,  0.0000e+00],\n        [-1.3924e+00, -2.8567e-01]], grad_fn=&lt;LogSoftmaxBackward0&gt;)\n\n\n\ndata.y\n\ntensor([1, 1, 0,  ..., 1, 1, 0])\n\n\n\nfor epoch in range(200):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out, data.y)\n    loss.backward()\n    optimizer.step()\n\n\nmodel.eval()\npred = model(data).argmax(dim=1)\ncorrect = (pred == data.y).sum() # 애큐러시는 test\nacc = int(correct) / 9009\nprint(f'Accuracy: {acc:.4f}')\n\nAccuracy: 0.9633\n\n\n\nfraud_mask = (data.y == 1)\n\n\nmodel.eval()\npred = model(data).argmax(dim=1)\ncorrect = (pred[fraud_mask] == data.y[fraud_mask]).sum() # 애큐러시는 test\nacc = int(correct) / int(fraud_mask.sum())\nprint(f'recall: {acc:.4f}')\n\nrecall: 0.9619\n\n\n\n위의 recall은 test가 없어서 train으로만 했던 거..!"
  },
  {
    "objectID": "posts/230905 데이터(10, df02교수님).html",
    "href": "posts/230905 데이터(10, df02교수님).html",
    "title": "[FRAUD] df02 accuracy 0.9707 f1은 망함",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\nimport xgboost as xgb\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n# gnn\nimport torch\nimport torch_geometric\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\n- fraudTrain\nfraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n- df02\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02 = df02.reset_index()\n- df_toy\ndf_toy=df02[:5].copy()\ndf_toy.cc_num = pd.Series([1,1,1,2,2])\ndf_toy\n\n\n\n\n\n\n\n\nindex\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n669418\n2019-10-12 18:21:00\n1\nfraud_Haley, Jewess and Bechtelar\nshopping_pos\n7.53\nDebra\nStark\nF\n686 Linda Rest\n...\n32.3836\n-94.8653\n24536\nMultimedia programmer\n1983-10-14\nd313353fa30233e5fab5468e852d22fc\n1350066071\n32.202008\n-94.371865\n0\n\n\n1\n32567\n2019-01-20 13:06:00\n1\nfraud_Turner LLC\ntravel\n3.79\nJudith\nMoss\nF\n46297 Benjamin Plains Suite 703\n...\n39.5370\n-83.4550\n22305\nTelevision floor manager\n1939-03-09\n88c65b4e1585934d578511e627fe3589\n1327064760\n39.156673\n-82.930503\n0\n\n\n2\n156587\n2019-03-24 18:09:00\n1\nfraud_Klein Group\nentertainment\n59.07\nDebbie\nPayne\nF\n204 Ashley Neck Apt. 169\n...\n41.5224\n-71.9934\n4720\nBroadcast presenter\n1977-05-18\n3bd9ede04b5c093143d5e5292940b670\n1332612553\n41.657152\n-72.595751\n0\n\n\n3\n1020243\n2020-02-25 15:12:00\n2\nfraud_Monahan-Morar\npersonal_care\n25.58\nAlan\nParsons\nM\n0547 Russell Ford Suite 574\n...\n39.6171\n-102.4776\n207\nNetwork engineer\n1955-12-04\n19e16ee7a01d229e750359098365e321\n1361805120\n39.080346\n-103.213452\n0\n\n\n4\n116272\n2019-03-06 23:19:00\n2\nfraud_Kozey-Kuhlman\npersonal_care\n84.96\nJill\nFlores\nF\n639 Cruz Islands\n...\n41.9488\n-86.4913\n3104\nHorticulturist, commercial\n1981-03-29\na0c8641ca1f5d6e243ed5a2246e66176\n1331075954\n42.502065\n-86.732664\n0\n\n\n\n\n5 rows × 23 columns\n- df_toy 에서 time_difference 구함\n고객1\ndf_toy.iloc[0].trans_date_trans_time.value - df_toy.iloc[1].trans_date_trans_time.value\n\n22914900000000000\ndf_toy.iloc[0].trans_date_trans_time.value - df_toy.iloc[2].trans_date_trans_time.value\n\n17453520000000000\ndf_toy.iloc[1].trans_date_trans_time.value - df_toy.iloc[2].trans_date_trans_time.value\n\n-5461380000000000\n고객2\ndf_toy.iloc[3].trans_date_trans_time.value - df_toy.iloc[4].trans_date_trans_time.value\n\n30729180000000000\n고객1,2\ndef compute_time_difference(group):\n    n = len(group)\n    result = []\n    for i in range(n):\n        for j in range(n):\n            time_difference = abs(group.iloc[i].trans_date_trans_time.value - group.iloc[j].trans_date_trans_time.value)\n            result.append([group.iloc[i].name, group.iloc[j].name, time_difference])\n    return result\ngroups = df_toy.groupby('cc_num')\nedge_index_list_plus = [compute_time_difference(group) for _, group in groups]\nedge_index_list_plus_flat = [item for sublist in edge_index_list_plus for item in sublist]\nedge_index_list_plus_nparr = np.array(edge_index_list_plus_flat)\nedge_index_list_plus_nparr\n\narray([[                0,                 0,                 0],\n       [                0,                 1, 22914900000000000],\n       [                0,                 2, 17453520000000000],\n       [                1,                 0, 22914900000000000],\n       [                1,                 1,                 0],\n       [                1,                 2,  5461380000000000],\n       [                2,                 0, 17453520000000000],\n       [                2,                 1,  5461380000000000],\n       [                2,                 2,                 0],\n       [                3,                 3,                 0],\n       [                3,                 4, 30729180000000000],\n       [                4,                 3, 30729180000000000],\n       [                4,                 4,                 0]])\n- df02에서 time_difference 구함\n# t1 = time.time()\n# groups = df02.groupby('cc_num')\n# edge_index_list_plus = [compute_time_difference(group) for _, group in groups]\n# edge_index_list_plus_flat = [item for sublist in edge_index_list_plus for item in sublist]\n# edge_index_list_plus_nparr = np.array(edge_index_list_plus_flat)\n# np.save('edge_index_list_plus02.npy', edge_index_list_plus_nparr)\n# t2 = time.time()\n# t2-t1\ngroups = df02.groupby(\"cc_num\")\nedge_index_list_plus02[:,2] = (np.exp(-edge_index_list_plus02[:,2]/theta) != 1)*(np.exp(-edge_index_list_plus02[:,2]/theta))\nedge_index_list_plus02\n\narray([[  2881,   2881,      0],\n       [  2881,   3061,      0],\n       [  2881,   4867,      0],\n       ...,\n       [212771, 212765,      0],\n       [212771, 212769,      0],\n       [212771, 212771,      0]])\ntheta = edge_index_list_plus02[:,2].mean()\nedge_index_list_plus02 = np.load('edge_index_list_plus02.npy').astype(np.float64)\nedge_index_list_plus02[:,2] = (np.exp(-edge_index_list_plus02[:,2]/theta) != 1)*(np.exp(-edge_index_list_plus02[:,2]/theta))\nedge_index_list_plus02\n\narray([[2.88100000e+03, 2.88100000e+03, 0.00000000e+00],\n       [2.88100000e+03, 3.06100000e+03, 1.96061280e-01],\n       [2.88100000e+03, 4.86700000e+03, 8.12918172e-01],\n       ...,\n       [2.12771000e+05, 2.12765000e+05, 9.97708695e-01],\n       [2.12771000e+05, 2.12769000e+05, 9.99923197e-01],\n       [2.12771000e+05, 2.12771000e+05, 0.00000000e+00]])\n같은 cc_num별로.. 시간 차이를 계산했어.\nweight = (np.exp(-edge_index_list_plus02[:,2]/theta) != 1)*(np.exp(-edge_index_list_plus02[:,2]/theta))\nweight\n\narray([0.        , 0.19606128, 0.81291817, ..., 0.9977087 , 0.9999232 ,\n       0.        ])\n자꾸 아래처럼 하면 3열의 값이 다 0나와가지고;; 이상하게 해봄\nedge_index_list_plus02 = np.column_stack((edge_index_list_plus02, weight))\nedge_index_list_plus02\n\narray([[2.88100000e+03, 2.88100000e+03, 0.00000000e+00, 0.00000000e+00],\n       [2.88100000e+03, 3.06100000e+03, 1.90922400e+16, 1.96061280e-01],\n       [2.88100000e+03, 4.86700000e+03, 2.42706000e+15, 8.12918172e-01],\n       ...,\n       [2.12771000e+05, 2.12765000e+05, 2.68800000e+13, 9.97708695e-01],\n       [2.12771000e+05, 2.12769000e+05, 9.00000000e+11, 9.99923197e-01],\n       [2.12771000e+05, 2.12771000e+05, 0.00000000e+00, 0.00000000e+00]])\nedge_index_list_plus02 = np.delete(edge_index_list_plus02, 2, axis=1)\nedge_index_list_plus02\n\narray([[2.88100000e+03, 2.88100000e+03, 0.00000000e+00],\n       [2.88100000e+03, 3.06100000e+03, 1.96061280e-01],\n       [2.88100000e+03, 4.86700000e+03, 8.12918172e-01],\n       ...,\n       [2.12771000e+05, 2.12765000e+05, 9.97708695e-01],\n       [2.12771000e+05, 2.12769000e+05, 9.99923197e-01],\n       [2.12771000e+05, 2.12771000e+05, 0.00000000e+00]])\nedge_index_list_plus02.shape\n\n(65831594, 3)\nedge_index_list_updated = edge_index_list_plus02.tolist()\nnp.array(edge_index_list_updated)[:,2].mean()\n\n0.4536043999922591\nmm = np.array(edge_index_list_updated)[:,2].mean()\nselected_edges = [(int(row[0]), int(row[1])) for row in edge_index_list_updated if row[2] &gt; mm]\nedge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\nedge_index_selected.shape\n\ntorch.Size([2, 29970380])"
  },
  {
    "objectID": "posts/230905 데이터(10, df02교수님).html#trtest",
    "href": "posts/230905 데이터(10, df02교수님).html#trtest",
    "title": "[FRAUD] df02 accuracy 0.9707 f1은 망함",
    "section": "tr/test",
    "text": "tr/test\n\ndf02_tr,df02_test = sklearn.model_selection.train_test_split(df02, random_state=42)\n\n\ndf02_tr.shape, df02_test.shape\n\n((160890, 23), (53630, 23))\n\n\n\nN = len(df02)\ntrain_mask = [i in df02_tr.index for i in range(N)]\ntest_mask = [i in df02_test.index for i in range(N)]\n\n\ntrain_mask = np.array(train_mask)\ntest_mask = np.array(test_mask)\n\n\ntrain_mask.shape, test_mask.shape\n\n((214520,), (214520,))"
  },
  {
    "objectID": "posts/230905 데이터(10, df02교수님).html#data",
    "href": "posts/230905 데이터(10, df02교수님).html#data",
    "title": "[FRAUD] df02 accuracy 0.9707 f1은 망함",
    "section": "data",
    "text": "data\n\nx = torch.tensor(df02['amt'], dtype=torch.float).reshape(-1,1)\ny = torch.tensor(df02['is_fraud'],dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index = edge_index_selected, y=y, train_mask = train_mask, test_mask = test_mask)\n\n\ndata\n\nData(x=[214520, 1], edge_index=[2, 29970380], y=[214520], train_mask=[214520], test_mask=[214520])\n\n\n\n\nclass GCN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 16)\n        self.conv2 = GCNConv(16,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\n\nmodel = GCN()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(400):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\n\n\nmodel.eval()\npred = model(data).argmax(dim=1)\ncorrect = (pred[data.test_mask] == data.y[data.test_mask]).sum()\nacc = int(correct) / int(np.array(data.test_mask).sum())\nprint(f'Accuracy: {acc:.4f}')\n\nAccuracy: 0.9707\n\n\n\npredicted_labels = pred[data.test_mask]\ntrue_labels = data.y[data.test_mask]\n\n\nprecision = precision_score(true_labels, predicted_labels, average='macro')\nrecall = recall_score(true_labels, predicted_labels, average='macro')\nf1 = f1_score(true_labels, predicted_labels, average='macro')\nprint(f'Precision: {precision:.4f}')\nprint(f'Recall: {recall:.4f}')\nprint(f'F1 Score: {f1:.4f}')\n\nPrecision: 0.4854\nRecall: 0.5000\nF1 Score: 0.4926\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))"
  },
  {
    "objectID": "posts/231110.html",
    "href": "posts/231110.html",
    "title": "[FRAUD] 그래프 그림 그리기(graft)",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# gnn\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric\nfrom torch_geometric.nn import GCNConv\n\n\n\nimport graft\n\n\ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef compute_time_difference(group):\n    n = len(group)\n    result = []\n    for i in range(n):\n        for j in range(n):\n            time_difference = abs(group.iloc[i].trans_date_trans_time.value - group.iloc[j].trans_date_trans_time.value)\n            result.append([group.iloc[i].name, group.iloc[j].name, time_difference])\n    return result\n\ndef mask(df):\n    df_tr,df_test = sklearn.model_selection.train_test_split(df, random_state=42)\n    N = len(df)\n    train_mask = [i in df_tr.index for i in range(N)]\n    test_mask = [i in df_test.index for i in range(N)]\n    train_mask = np.array(train_mask)\n    test_mask = np.array(test_mask)\n    return train_mask, test_mask\n\ndef edge_index_selected(edge_index):\n    theta = edge_index[:,2].mean()\n    edge_index[:,2] = (np.exp(-edge_index[:,2]/theta) != 1)*(np.exp(-edge_index[:,2]/theta))\n    edge_index = edge_index.tolist()\n    mean_ = np.array(edge_index)[:,2].mean()\n    selected_edges = [(int(row[0]), int(row[1])) for row in edge_index if row[2] &gt; mean_]\n    edge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\n    return edge_index_selected\n\n\n\n\nfraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\n\n\nlen(set(fraudTrain['cc_num']))\n\n943\n\n\n\nlen(set(fraudTrain['merchant']))\n\n693\n\n\n\n# # fraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\n# fraudTrain = fraudTrain.assign(\n#     trans_date_trans_time= fraudTrain.trans_date_trans_time.apply(pd.to_datetime)\n# )\nfraudTrain = pd.read_pickle('temp.pkl')\n\n\n\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50 = df50.reset_index()\ndf50.shape\n\n(12012, 23)\n\n\n\n\n\n\nmask(df50)\n\ntrain_mask, test_mask = mask(df50)\n\n\n\n\n\n\n\n\n# x = torch.tensor(df50['amt'], dtype=torch.float).reshape(-1,1)\n# y = torch.tensor(df50['is_fraud'],dtype=torch.int64)\n# data = torch_geometric.data.Data(x=x, edge_index = edge_index_selected, y=y, train_mask = train_mask, test_mask = test_mask)\n# data\n\n\n\n# torch.manual_seed(202250926)\n# class GCN2(torch.nn.Module):\n#     def __init__(self):\n#         super().__init__()\n#         self.conv1 = GCNConv(1, 32)\n#         self.conv2 = GCNConv(32,2)\n\n#     def forward(self, data):\n#         x, edge_index = data.x, data.edge_index\n\n#         x = self.conv1(x, edge_index)\n#         x = F.relu(x)\n#         x = F.dropout(x, training=self.training)\n#         x = self.conv2(x, edge_index)\n\n#         return F.log_softmax(x, dim=1)\n\n# X = (data.x[data.train_mask]).numpy()\n# XX = (data.x[data.test_mask]).numpy()\n# y = (data.y[data.train_mask]).numpy()\n# yy = (data.y[data.test_mask]).numpy()\n\n# model = GCN2()\n# optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n# model.train()\n# for epoch in range(400):\n#     optimizer.zero_grad()\n#     out = model(data)\n#     loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n#     loss.backward()\n#     optimizer.step()\n# model.eval()\n\n# pred = model(data).argmax(dim=1)\n# yyhat = pred[data.test_mask]\n\n# metrics = [sklearn.metrics.accuracy_score,\n#            sklearn.metrics.precision_score,\n#            sklearn.metrics.recall_score,\n#            sklearn.metrics.f1_score]\n\n# _results1= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석1'])\n# _results1\n\n\n# x = torch.tensor(df50['amt'], dtype=torch.float).reshape(-1,1)\n# y = torch.tensor(df50['is_fraud'],dtype=torch.int64)\n# data = torch_geometric.data.Data(x=x, edge_index = edge_index_selected, y=y, train_mask = train_mask, test_mask = test_mask)\n# data"
  },
  {
    "objectID": "posts/231110.html#데이터정리",
    "href": "posts/231110.html#데이터정리",
    "title": "[FRAUD] 그래프 그림 그리기(graft)",
    "section": "",
    "text": "_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf50 = down_sample_textbook(df02)\ndf50 = df50.reset_index()\ndf50.shape\n\n(12012, 23)\n\n\n\n\n\n\nmask(df50)\n\ntrain_mask, test_mask = mask(df50)"
  },
  {
    "objectID": "posts/231110.html#분석-1gcn-amt",
    "href": "posts/231110.html#분석-1gcn-amt",
    "title": "[FRAUD] 그래프 그림 그리기(graft)",
    "section": "",
    "text": "# x = torch.tensor(df50['amt'], dtype=torch.float).reshape(-1,1)\n# y = torch.tensor(df50['is_fraud'],dtype=torch.int64)\n# data = torch_geometric.data.Data(x=x, edge_index = edge_index_selected, y=y, train_mask = train_mask, test_mask = test_mask)\n# data\n\n\n\n# torch.manual_seed(202250926)\n# class GCN2(torch.nn.Module):\n#     def __init__(self):\n#         super().__init__()\n#         self.conv1 = GCNConv(1, 32)\n#         self.conv2 = GCNConv(32,2)\n\n#     def forward(self, data):\n#         x, edge_index = data.x, data.edge_index\n\n#         x = self.conv1(x, edge_index)\n#         x = F.relu(x)\n#         x = F.dropout(x, training=self.training)\n#         x = self.conv2(x, edge_index)\n\n#         return F.log_softmax(x, dim=1)\n\n# X = (data.x[data.train_mask]).numpy()\n# XX = (data.x[data.test_mask]).numpy()\n# y = (data.y[data.train_mask]).numpy()\n# yy = (data.y[data.test_mask]).numpy()\n\n# model = GCN2()\n# optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n# model.train()\n# for epoch in range(400):\n#     optimizer.zero_grad()\n#     out = model(data)\n#     loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n#     loss.backward()\n#     optimizer.step()\n# model.eval()\n\n# pred = model(data).argmax(dim=1)\n# yyhat = pred[data.test_mask]\n\n# metrics = [sklearn.metrics.accuracy_score,\n#            sklearn.metrics.precision_score,\n#            sklearn.metrics.recall_score,\n#            sklearn.metrics.f1_score]\n\n# _results1= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석1'])\n# _results1\n\n\n# x = torch.tensor(df50['amt'], dtype=torch.float).reshape(-1,1)\n# y = torch.tensor(df50['is_fraud'],dtype=torch.int64)\n# data = torch_geometric.data.Data(x=x, edge_index = edge_index_selected, y=y, train_mask = train_mask, test_mask = test_mask)\n# data"
  },
  {
    "objectID": "posts/230905 데이터(10, df02교수님)-Copy1.html",
    "href": "posts/230905 데이터(10, df02교수님)-Copy1.html",
    "title": "[FRAUD] df02 accuracy 0.9707 f1은 망함",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\nimport xgboost as xgb\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n# gnn\nimport torch\nimport torch_geometric\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_geometric/typing.py:18: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/libpyg.so: undefined symbol: _ZN2at4_ops12split_Tensor4callERKNS_6TensorEN3c106SymIntEl\n  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_geometric/typing.py:31: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_scatter/_scatter_cuda.so: undefined symbol: _ZNK3c107SymBool10guard_boolEPKcl\n  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_geometric/typing.py:42: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/torch_sparse/_diag_cuda.so: undefined symbol: _ZN3c106detail19maybe_wrap_dim_slowIlEET_S2_S2_b\n  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n- fraudTrain\nfraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n- df02\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02 = df02.reset_index()\n- df_toy\ndf_toy=df02[:5].copy()\ndf_toy.cc_num = pd.Series([1,1,1,2,2])\ndf_toy\n\n\n\n\n\n\n\n\nindex\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n669418\n2019-10-12 18:21:00\n1\nfraud_Haley, Jewess and Bechtelar\nshopping_pos\n7.53\nDebra\nStark\nF\n686 Linda Rest\n...\n32.3836\n-94.8653\n24536\nMultimedia programmer\n1983-10-14\nd313353fa30233e5fab5468e852d22fc\n1350066071\n32.202008\n-94.371865\n0\n\n\n1\n32567\n2019-01-20 13:06:00\n1\nfraud_Turner LLC\ntravel\n3.79\nJudith\nMoss\nF\n46297 Benjamin Plains Suite 703\n...\n39.5370\n-83.4550\n22305\nTelevision floor manager\n1939-03-09\n88c65b4e1585934d578511e627fe3589\n1327064760\n39.156673\n-82.930503\n0\n\n\n2\n156587\n2019-03-24 18:09:00\n1\nfraud_Klein Group\nentertainment\n59.07\nDebbie\nPayne\nF\n204 Ashley Neck Apt. 169\n...\n41.5224\n-71.9934\n4720\nBroadcast presenter\n1977-05-18\n3bd9ede04b5c093143d5e5292940b670\n1332612553\n41.657152\n-72.595751\n0\n\n\n3\n1020243\n2020-02-25 15:12:00\n2\nfraud_Monahan-Morar\npersonal_care\n25.58\nAlan\nParsons\nM\n0547 Russell Ford Suite 574\n...\n39.6171\n-102.4776\n207\nNetwork engineer\n1955-12-04\n19e16ee7a01d229e750359098365e321\n1361805120\n39.080346\n-103.213452\n0\n\n\n4\n116272\n2019-03-06 23:19:00\n2\nfraud_Kozey-Kuhlman\npersonal_care\n84.96\nJill\nFlores\nF\n639 Cruz Islands\n...\n41.9488\n-86.4913\n3104\nHorticulturist, commercial\n1981-03-29\na0c8641ca1f5d6e243ed5a2246e66176\n1331075954\n42.502065\n-86.732664\n0\n\n\n\n\n5 rows × 23 columns\n- df_toy 에서 time_difference 구함\n고객1\ndf_toy.iloc[0].trans_date_trans_time.value - df_toy.iloc[1].trans_date_trans_time.value\n\n22914900000000000\ndf_toy.iloc[0].trans_date_trans_time.value - df_toy.iloc[2].trans_date_trans_time.value\n\n17453520000000000\ndf_toy.iloc[1].trans_date_trans_time.value - df_toy.iloc[2].trans_date_trans_time.value\n\n-5461380000000000\n고객2\ndf_toy.iloc[3].trans_date_trans_time.value - df_toy.iloc[4].trans_date_trans_time.value\n\n30729180000000000\n고객1,2\ndef compute_time_difference(group):\n    n = len(group)\n    result = []\n    for i in range(n):\n        for j in range(n):\n            time_difference = abs(group.iloc[i].trans_date_trans_time.value - group.iloc[j].trans_date_trans_time.value)\n            result.append([group.iloc[i].name, group.iloc[j].name, time_difference])\n    return result\ngroups = df_toy.groupby('cc_num')\nedge_index_list_plus = [compute_time_difference(group) for _, group in groups]\nedge_index_list_plus_flat = [item for sublist in edge_index_list_plus for item in sublist]\nedge_index_list_plus_nparr = np.array(edge_index_list_plus_flat)\nedge_index_list_plus_nparr\n\narray([[                0,                 0,                 0],\n       [                0,                 1, 22914900000000000],\n       [                0,                 2, 17453520000000000],\n       [                1,                 0, 22914900000000000],\n       [                1,                 1,                 0],\n       [                1,                 2,  5461380000000000],\n       [                2,                 0, 17453520000000000],\n       [                2,                 1,  5461380000000000],\n       [                2,                 2,                 0],\n       [                3,                 3,                 0],\n       [                3,                 4, 30729180000000000],\n       [                4,                 3, 30729180000000000],\n       [                4,                 4,                 0]])\n- df02에서 time_difference 구함\n# t1 = time.time()\n# groups = df02.groupby('cc_num')\n# edge_index_list_plus = [compute_time_difference(group) for _, group in groups]\n# edge_index_list_plus_flat = [item for sublist in edge_index_list_plus for item in sublist]\n# edge_index_list_plus_nparr = np.array(edge_index_list_plus_flat)\n# np.save('edge_index_list_plus02.npy', edge_index_list_plus_nparr)\n# t2 = time.time()\n# t2-t1\ngroups = df02.groupby(\"cc_num\")\nedge_index_list_plus02[:,2] = (np.exp(-edge_index_list_plus02[:,2]/theta) != 1)*(np.exp(-edge_index_list_plus02[:,2]/theta))\nedge_index_list_plus02\n\narray([[  2881,   2881,      0],\n       [  2881,   3061,      0],\n       [  2881,   4867,      0],\n       ...,\n       [212771, 212765,      0],\n       [212771, 212769,      0],\n       [212771, 212771,      0]])\nedge_index_list_plus02 = np.load('edge_index_list_plus02.npy').astype(np.float64)\ntheta = edge_index_list_plus02[:,2].mean()\nedge_index_list_plus02[:,2] = (np.exp(-edge_index_list_plus02[:,2]/theta) != 1)*(np.exp(-edge_index_list_plus02[:,2]/theta))\nedge_index_list_plus02\n\narray([[2.88100000e+03, 2.88100000e+03, 0.00000000e+00],\n       [2.88100000e+03, 3.06100000e+03, 1.96061280e-01],\n       [2.88100000e+03, 4.86700000e+03, 8.12918172e-01],\n       ...,\n       [2.12771000e+05, 2.12765000e+05, 9.97708695e-01],\n       [2.12771000e+05, 2.12769000e+05, 9.99923197e-01],\n       [2.12771000e+05, 2.12771000e+05, 0.00000000e+00]])\nedge_index_list_plus02.shape\n\n(65831594, 3)\nedge_index_list_updated = edge_index_list_plus02.tolist()\nnp.array(edge_index_list_updated)[:,2].mean()\n\n0.4536043999922591\nmm = np.array(edge_index_list_updated)[:,2].mean()\nselected_edges = [(int(row[0]), int(row[1])) for row in edge_index_list_updated if row[2] &gt; mm]\nedge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\nedge_index_selected.shape\n\ntorch.Size([2, 29970380])"
  },
  {
    "objectID": "posts/230905 데이터(10, df02교수님)-Copy1.html#trtest",
    "href": "posts/230905 데이터(10, df02교수님)-Copy1.html#trtest",
    "title": "[FRAUD] df02 accuracy 0.9707 f1은 망함",
    "section": "tr/test",
    "text": "tr/test\n\ndf02_tr,df02_test = sklearn.model_selection.train_test_split(df02, random_state=42)\n\n\ndf02_tr.shape, df02_test.shape\n\n((160890, 23), (53630, 23))\n\n\n\nN = len(df02)\ntrain_mask = [i in df02_tr.index for i in range(N)]\ntest_mask = [i in df02_test.index for i in range(N)]\n\n\ntrain_mask = np.array(train_mask)\ntest_mask = np.array(test_mask)\n\n\ntrain_mask.shape, test_mask.shape\n\n((214520,), (214520,))"
  },
  {
    "objectID": "posts/230905 데이터(10, df02교수님)-Copy1.html#data",
    "href": "posts/230905 데이터(10, df02교수님)-Copy1.html#data",
    "title": "[FRAUD] df02 accuracy 0.9707 f1은 망함",
    "section": "data",
    "text": "data\n\nx = torch.tensor(df02['amt'], dtype=torch.float).reshape(-1,1)\ny = torch.tensor(df02['is_fraud'],dtype=torch.int64)\ndata = torch_geometric.data.Data(x=x, edge_index = edge_index_selected, y=y, train_mask = train_mask, test_mask = test_mask)\n\n\ndata\n\nData(x=[214520, 1], edge_index=[2, 29970380], y=[214520], train_mask=[214520], test_mask=[214520])\n\n\n\ntorch.manual_seed(202250926)\nclass GCN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = GCNConv(1, 32)\n        self.conv2 = GCNConv(32,2)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n    \n\nX = (data.x[data.train_mask]).numpy()\nXX = (data.x[data.test_mask]).numpy()\ny = (data.y[data.train_mask]).numpy()\nyy = (data.y[data.test_mask]).numpy()\n\n\nmodel = GCN()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nmodel.train()\nfor epoch in range(400):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\n    \n    \n    model.eval()\n\npred = model(data).argmax(dim=1)\nyyhat = pred[data.test_mask]\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n_results2= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석2'])\n_results2\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석2\n0.973112\n0.72695\n0.130573\n0.221382\n\n\n\n\n\n\n\n\nmodel.eval()\npred = model(data).argmax(dim=1)\ncorrect = (pred[data.test_mask] == data.y[data.test_mask]).sum()\nacc = int(correct) / int(np.array(data.test_mask).sum())\nprint(f'Accuracy: {acc:.4f}')\n\nAccuracy: 0.9707\n\n\n\npredicted_labels = pred[data.test_mask]\ntrue_labels = data.y[data.test_mask]\n\n\nprecision = precision_score(true_labels, predicted_labels, average='macro')\nrecall = recall_score(true_labels, predicted_labels, average='macro')\nf1 = f1_score(true_labels, predicted_labels, average='macro')\nprint(f'Precision: {precision:.4f}')\nprint(f'Recall: {recall:.4f}')\nprint(f'F1 Score: {f1:.4f}')\n\nPrecision: 0.4854\nRecall: 0.5000\nF1 Score: 0.4926\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Boram-coco",
    "section": "",
    "text": "Everyday with Coco"
  }
]