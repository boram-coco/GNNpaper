{
 "cells": [
  {
   "cell_type": "raw",
   "id": "c8593b37-a6e6-4ccb-b9b2-349f55cb2b69",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"[FRAUD] 책_코드 함수 만들기\"\n",
    "author: \"김보람\"\n",
    "date: \"04/04/2024\"\n",
    "categories:\n",
    "  - graph\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eedf6e-cd2a-40f0-aac9-e8899d5b1acc",
   "metadata": {},
   "source": [
    "# FAURD코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8812aa9-c507-4520-9dfc-38a88301f9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import networkx as nx\n",
    "import sklearn\n",
    "import xgboost as xgb\n",
    "import pickle \n",
    "import time \n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# sklearn\n",
    "from sklearn import model_selection # split함수이용\n",
    "from sklearn import ensemble # RF,GBM\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# gnn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d01fbd14-ccf1-4fca-b40d-b465036e21cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_sample_textbook(df):\n",
    "    df_majority = df[df.is_fraud==0].copy()\n",
    "    df_minority = df[df.is_fraud==1].copy()\n",
    "    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n",
    "    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n",
    "    return df_downsampled\n",
    "\n",
    "def compute_time_difference(group):\n",
    "    n = len(group)\n",
    "    result = []\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            time_difference = abs(group.iloc[i].trans_date_trans_time.value - group.iloc[j].trans_date_trans_time.value)\n",
    "            result.append([group.iloc[i].name, group.iloc[j].name, time_difference])\n",
    "    return result\n",
    "\n",
    "def mask(df):\n",
    "    df_tr,df_test = sklearn.model_selection.train_test_split(df, random_state=42)\n",
    "    N = len(df)\n",
    "    train_mask = [i in df_tr.index for i in range(N)]\n",
    "    test_mask = [i in df_test.index for i in range(N)]\n",
    "    train_mask = np.array(train_mask)\n",
    "    test_mask = np.array(test_mask)\n",
    "    return train_mask, test_mask\n",
    "\n",
    "def edge_index_selected(edge_index):\n",
    "    theta = edge_index[:,2].mean()\n",
    "    edge_index[:,2] = (np.exp(-edge_index[:,2]/theta) != 1)*(np.exp(-edge_index[:,2]/theta))\n",
    "    edge_index = edge_index.tolist()\n",
    "    mean_ = np.array(edge_index)[:,2].mean()\n",
    "    selected_edges = [(int(row[0]), int(row[1])) for row in edge_index if row[2] > mean_]\n",
    "    edge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\n",
    "    return edge_index_selected\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00abe65e-e827-4a77-98d1-a1541c76b292",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../fraudTrain.pkl', 'rb') as file:\n",
    "    fraudTrain = pickle.load(file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6dd5e82-f16f-479c-8280-d5d0dcf3f075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trans_date_trans_time</th>\n",
       "      <th>cc_num</th>\n",
       "      <th>merchant</th>\n",
       "      <th>category</th>\n",
       "      <th>amt</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>gender</th>\n",
       "      <th>street</th>\n",
       "      <th>city</th>\n",
       "      <th>...</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>job</th>\n",
       "      <th>dob</th>\n",
       "      <th>trans_num</th>\n",
       "      <th>unix_time</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01 00:00:00</td>\n",
       "      <td>2.703190e+15</td>\n",
       "      <td>fraud_Rippin, Kub and Mann</td>\n",
       "      <td>misc_net</td>\n",
       "      <td>4.97</td>\n",
       "      <td>Jennifer</td>\n",
       "      <td>Banks</td>\n",
       "      <td>F</td>\n",
       "      <td>561 Perry Cove</td>\n",
       "      <td>Moravian Falls</td>\n",
       "      <td>...</td>\n",
       "      <td>36.0788</td>\n",
       "      <td>-81.1781</td>\n",
       "      <td>3495</td>\n",
       "      <td>Psychologist, counselling</td>\n",
       "      <td>1988-03-09</td>\n",
       "      <td>0b242abb623afc578575680df30655b9</td>\n",
       "      <td>1325376018</td>\n",
       "      <td>36.011293</td>\n",
       "      <td>-82.048315</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01 00:00:00</td>\n",
       "      <td>6.304230e+11</td>\n",
       "      <td>fraud_Heller, Gutmann and Zieme</td>\n",
       "      <td>grocery_pos</td>\n",
       "      <td>107.23</td>\n",
       "      <td>Stephanie</td>\n",
       "      <td>Gill</td>\n",
       "      <td>F</td>\n",
       "      <td>43039 Riley Greens Suite 393</td>\n",
       "      <td>Orient</td>\n",
       "      <td>...</td>\n",
       "      <td>48.8878</td>\n",
       "      <td>-118.2105</td>\n",
       "      <td>149</td>\n",
       "      <td>Special educational needs teacher</td>\n",
       "      <td>1978-06-21</td>\n",
       "      <td>1f76529f8574734946361c461b024d99</td>\n",
       "      <td>1325376044</td>\n",
       "      <td>49.159047</td>\n",
       "      <td>-118.186462</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01 00:00:00</td>\n",
       "      <td>3.885950e+13</td>\n",
       "      <td>fraud_Lind-Buckridge</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>220.11</td>\n",
       "      <td>Edward</td>\n",
       "      <td>Sanchez</td>\n",
       "      <td>M</td>\n",
       "      <td>594 White Dale Suite 530</td>\n",
       "      <td>Malad City</td>\n",
       "      <td>...</td>\n",
       "      <td>42.1808</td>\n",
       "      <td>-112.2620</td>\n",
       "      <td>4154</td>\n",
       "      <td>Nature conservation officer</td>\n",
       "      <td>1962-01-19</td>\n",
       "      <td>a1a22d70485983eac12b5b88dad1cf95</td>\n",
       "      <td>1325376051</td>\n",
       "      <td>43.150704</td>\n",
       "      <td>-112.154481</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01 00:01:00</td>\n",
       "      <td>3.534090e+15</td>\n",
       "      <td>fraud_Kutch, Hermiston and Farrell</td>\n",
       "      <td>gas_transport</td>\n",
       "      <td>45.00</td>\n",
       "      <td>Jeremy</td>\n",
       "      <td>White</td>\n",
       "      <td>M</td>\n",
       "      <td>9443 Cynthia Court Apt. 038</td>\n",
       "      <td>Boulder</td>\n",
       "      <td>...</td>\n",
       "      <td>46.2306</td>\n",
       "      <td>-112.1138</td>\n",
       "      <td>1939</td>\n",
       "      <td>Patent attorney</td>\n",
       "      <td>1967-01-12</td>\n",
       "      <td>6b849c168bdad6f867558c3793159a81</td>\n",
       "      <td>1325376076</td>\n",
       "      <td>47.034331</td>\n",
       "      <td>-112.561071</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01 00:03:00</td>\n",
       "      <td>3.755340e+14</td>\n",
       "      <td>fraud_Keeling-Crist</td>\n",
       "      <td>misc_pos</td>\n",
       "      <td>41.96</td>\n",
       "      <td>Tyler</td>\n",
       "      <td>Garcia</td>\n",
       "      <td>M</td>\n",
       "      <td>408 Bradley Rest</td>\n",
       "      <td>Doe Hill</td>\n",
       "      <td>...</td>\n",
       "      <td>38.4207</td>\n",
       "      <td>-79.4629</td>\n",
       "      <td>99</td>\n",
       "      <td>Dance movement psychotherapist</td>\n",
       "      <td>1986-03-28</td>\n",
       "      <td>a41d7549acf90789359a9aa5346dcb46</td>\n",
       "      <td>1325376186</td>\n",
       "      <td>38.674999</td>\n",
       "      <td>-78.632459</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048570</th>\n",
       "      <td>2020-03-10 16:07:00</td>\n",
       "      <td>6.011980e+15</td>\n",
       "      <td>fraud_Fadel Inc</td>\n",
       "      <td>health_fitness</td>\n",
       "      <td>77.00</td>\n",
       "      <td>Haley</td>\n",
       "      <td>Wagner</td>\n",
       "      <td>F</td>\n",
       "      <td>05561 Farrell Crescent</td>\n",
       "      <td>Annapolis</td>\n",
       "      <td>...</td>\n",
       "      <td>39.0305</td>\n",
       "      <td>-76.5515</td>\n",
       "      <td>92106</td>\n",
       "      <td>Accountant, chartered certified</td>\n",
       "      <td>1943-05-28</td>\n",
       "      <td>45ecd198c65e81e597db22e8d2ef7361</td>\n",
       "      <td>1362931649</td>\n",
       "      <td>38.779464</td>\n",
       "      <td>-76.317042</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048571</th>\n",
       "      <td>2020-03-10 16:07:00</td>\n",
       "      <td>4.839040e+15</td>\n",
       "      <td>fraud_Cremin, Hamill and Reichel</td>\n",
       "      <td>misc_pos</td>\n",
       "      <td>116.94</td>\n",
       "      <td>Meredith</td>\n",
       "      <td>Campbell</td>\n",
       "      <td>F</td>\n",
       "      <td>043 Hanson Turnpike</td>\n",
       "      <td>Hedrick</td>\n",
       "      <td>...</td>\n",
       "      <td>41.1826</td>\n",
       "      <td>-92.3097</td>\n",
       "      <td>1583</td>\n",
       "      <td>Geochemist</td>\n",
       "      <td>1999-06-28</td>\n",
       "      <td>c00ce51c6ebb7657474a77b9e0b51f34</td>\n",
       "      <td>1362931670</td>\n",
       "      <td>41.400318</td>\n",
       "      <td>-92.726724</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048572</th>\n",
       "      <td>2020-03-10 16:08:00</td>\n",
       "      <td>5.718440e+11</td>\n",
       "      <td>fraud_O'Connell, Botsford and Hand</td>\n",
       "      <td>home</td>\n",
       "      <td>21.27</td>\n",
       "      <td>Susan</td>\n",
       "      <td>Mills</td>\n",
       "      <td>F</td>\n",
       "      <td>005 Cody Estates</td>\n",
       "      <td>Louisville</td>\n",
       "      <td>...</td>\n",
       "      <td>38.2507</td>\n",
       "      <td>-85.7476</td>\n",
       "      <td>736284</td>\n",
       "      <td>Engineering geologist</td>\n",
       "      <td>1952-04-02</td>\n",
       "      <td>17c9dc8b2a6449ca2473726346e58e6c</td>\n",
       "      <td>1362931711</td>\n",
       "      <td>37.293339</td>\n",
       "      <td>-84.798122</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048573</th>\n",
       "      <td>2020-03-10 16:08:00</td>\n",
       "      <td>4.646850e+18</td>\n",
       "      <td>fraud_Thompson-Gleason</td>\n",
       "      <td>health_fitness</td>\n",
       "      <td>9.52</td>\n",
       "      <td>Julia</td>\n",
       "      <td>Bell</td>\n",
       "      <td>F</td>\n",
       "      <td>576 House Crossroad</td>\n",
       "      <td>West Sayville</td>\n",
       "      <td>...</td>\n",
       "      <td>40.7320</td>\n",
       "      <td>-73.1000</td>\n",
       "      <td>4056</td>\n",
       "      <td>Film/video editor</td>\n",
       "      <td>1990-06-25</td>\n",
       "      <td>5ca650881b48a6a38754f841c23b77ab</td>\n",
       "      <td>1362931718</td>\n",
       "      <td>39.773077</td>\n",
       "      <td>-72.213209</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048574</th>\n",
       "      <td>2020-03-10 16:08:00</td>\n",
       "      <td>2.283740e+15</td>\n",
       "      <td>fraud_Buckridge PLC</td>\n",
       "      <td>misc_pos</td>\n",
       "      <td>6.81</td>\n",
       "      <td>Shannon</td>\n",
       "      <td>Williams</td>\n",
       "      <td>F</td>\n",
       "      <td>9345 Spencer Junctions Suite 183</td>\n",
       "      <td>Alpharetta</td>\n",
       "      <td>...</td>\n",
       "      <td>34.0770</td>\n",
       "      <td>-84.3033</td>\n",
       "      <td>165556</td>\n",
       "      <td>Prison officer</td>\n",
       "      <td>1997-12-27</td>\n",
       "      <td>8d0a575fe635bbde12f1a2bffc126731</td>\n",
       "      <td>1362931730</td>\n",
       "      <td>33.601468</td>\n",
       "      <td>-83.891921</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1048575 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        trans_date_trans_time        cc_num  \\\n",
       "0         2019-01-01 00:00:00  2.703190e+15   \n",
       "1         2019-01-01 00:00:00  6.304230e+11   \n",
       "2         2019-01-01 00:00:00  3.885950e+13   \n",
       "3         2019-01-01 00:01:00  3.534090e+15   \n",
       "4         2019-01-01 00:03:00  3.755340e+14   \n",
       "...                       ...           ...   \n",
       "1048570   2020-03-10 16:07:00  6.011980e+15   \n",
       "1048571   2020-03-10 16:07:00  4.839040e+15   \n",
       "1048572   2020-03-10 16:08:00  5.718440e+11   \n",
       "1048573   2020-03-10 16:08:00  4.646850e+18   \n",
       "1048574   2020-03-10 16:08:00  2.283740e+15   \n",
       "\n",
       "                                   merchant        category     amt  \\\n",
       "0                fraud_Rippin, Kub and Mann        misc_net    4.97   \n",
       "1           fraud_Heller, Gutmann and Zieme     grocery_pos  107.23   \n",
       "2                      fraud_Lind-Buckridge   entertainment  220.11   \n",
       "3        fraud_Kutch, Hermiston and Farrell   gas_transport   45.00   \n",
       "4                       fraud_Keeling-Crist        misc_pos   41.96   \n",
       "...                                     ...             ...     ...   \n",
       "1048570                     fraud_Fadel Inc  health_fitness   77.00   \n",
       "1048571    fraud_Cremin, Hamill and Reichel        misc_pos  116.94   \n",
       "1048572  fraud_O'Connell, Botsford and Hand            home   21.27   \n",
       "1048573              fraud_Thompson-Gleason  health_fitness    9.52   \n",
       "1048574                 fraud_Buckridge PLC        misc_pos    6.81   \n",
       "\n",
       "             first      last gender                            street  \\\n",
       "0         Jennifer     Banks      F                    561 Perry Cove   \n",
       "1        Stephanie      Gill      F      43039 Riley Greens Suite 393   \n",
       "2           Edward   Sanchez      M          594 White Dale Suite 530   \n",
       "3           Jeremy     White      M       9443 Cynthia Court Apt. 038   \n",
       "4            Tyler    Garcia      M                  408 Bradley Rest   \n",
       "...            ...       ...    ...                               ...   \n",
       "1048570      Haley    Wagner      F            05561 Farrell Crescent   \n",
       "1048571   Meredith  Campbell      F               043 Hanson Turnpike   \n",
       "1048572      Susan     Mills      F                  005 Cody Estates   \n",
       "1048573      Julia      Bell      F               576 House Crossroad   \n",
       "1048574    Shannon  Williams      F  9345 Spencer Junctions Suite 183   \n",
       "\n",
       "                   city  ...      lat      long  city_pop  \\\n",
       "0        Moravian Falls  ...  36.0788  -81.1781      3495   \n",
       "1                Orient  ...  48.8878 -118.2105       149   \n",
       "2            Malad City  ...  42.1808 -112.2620      4154   \n",
       "3               Boulder  ...  46.2306 -112.1138      1939   \n",
       "4              Doe Hill  ...  38.4207  -79.4629        99   \n",
       "...                 ...  ...      ...       ...       ...   \n",
       "1048570       Annapolis  ...  39.0305  -76.5515     92106   \n",
       "1048571         Hedrick  ...  41.1826  -92.3097      1583   \n",
       "1048572      Louisville  ...  38.2507  -85.7476    736284   \n",
       "1048573   West Sayville  ...  40.7320  -73.1000      4056   \n",
       "1048574      Alpharetta  ...  34.0770  -84.3033    165556   \n",
       "\n",
       "                                       job         dob  \\\n",
       "0                Psychologist, counselling  1988-03-09   \n",
       "1        Special educational needs teacher  1978-06-21   \n",
       "2              Nature conservation officer  1962-01-19   \n",
       "3                          Patent attorney  1967-01-12   \n",
       "4           Dance movement psychotherapist  1986-03-28   \n",
       "...                                    ...         ...   \n",
       "1048570    Accountant, chartered certified  1943-05-28   \n",
       "1048571                         Geochemist  1999-06-28   \n",
       "1048572              Engineering geologist  1952-04-02   \n",
       "1048573                  Film/video editor  1990-06-25   \n",
       "1048574                     Prison officer  1997-12-27   \n",
       "\n",
       "                                trans_num   unix_time  merch_lat  merch_long  \\\n",
       "0        0b242abb623afc578575680df30655b9  1325376018  36.011293  -82.048315   \n",
       "1        1f76529f8574734946361c461b024d99  1325376044  49.159047 -118.186462   \n",
       "2        a1a22d70485983eac12b5b88dad1cf95  1325376051  43.150704 -112.154481   \n",
       "3        6b849c168bdad6f867558c3793159a81  1325376076  47.034331 -112.561071   \n",
       "4        a41d7549acf90789359a9aa5346dcb46  1325376186  38.674999  -78.632459   \n",
       "...                                   ...         ...        ...         ...   \n",
       "1048570  45ecd198c65e81e597db22e8d2ef7361  1362931649  38.779464  -76.317042   \n",
       "1048571  c00ce51c6ebb7657474a77b9e0b51f34  1362931670  41.400318  -92.726724   \n",
       "1048572  17c9dc8b2a6449ca2473726346e58e6c  1362931711  37.293339  -84.798122   \n",
       "1048573  5ca650881b48a6a38754f841c23b77ab  1362931718  39.773077  -72.213209   \n",
       "1048574  8d0a575fe635bbde12f1a2bffc126731  1362931730  33.601468  -83.891921   \n",
       "\n",
       "         is_fraud  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  \n",
       "...           ...  \n",
       "1048570         0  \n",
       "1048571         0  \n",
       "1048572         0  \n",
       "1048573         0  \n",
       "1048574         0  \n",
       "\n",
       "[1048575 rows x 22 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "fraudTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06ec0330-c92d-494f-b47b-2cb0dcde110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../function_proposed_gcn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2843d90-bbe5-4e3c-98a4-a63f8f099439",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../functions-book.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3164c1db-7c0d-4ed6-8dee-7c48f7ff1198",
   "metadata": {},
   "source": [
    "## 데이터정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dd7e2cc5-dd94-4462-a43b-3e2c19f65fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df50 = throw(fraudTrain, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0e987e34-1467-4608-918b-a4988910abe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask, test_mask = mask(df50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede10437-7a8b-4b83-936f-41ff40cf2e6d",
   "metadata": {},
   "source": [
    "# 책(신용카드 거래에 대한 그래프 분석)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9472937e-ac8d-45ab-8d26-454bbbf76c3d",
   "metadata": {},
   "source": [
    "`-` 이분그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "44f78797-91db-4943-880f-625f41ff87ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_bipartite(df_input, graph_type=nx.Graph()):\n",
    "    df=df_input.copy()\n",
    "    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n",
    "                                                      df[\"merchant\"].values.tolist()))}\n",
    "    \n",
    "    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n",
    "    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n",
    "    \n",
    "    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n",
    "    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x>0 else 0)\n",
    "    \n",
    "    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n",
    "    \n",
    "    nx.set_edge_attributes(G, {(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부 \n",
    "    \n",
    "    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5d7d24d3-ddc9-4573-b411-b787f47239af",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_bu = build_graph_bipartite(df50, nx.Graph(name=\"Bipartite Undirect\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a5f718-5b3e-4c30-8d67-f30a737143bb",
   "metadata": {},
   "source": [
    "`-` 삼분그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de4f50b6-213c-4b04-b3bd-4ff0a8221ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_tripartite(df_input, graph_type=nx.Graph()):\n",
    "    df=df_input.copy()\n",
    "    mapping={x:node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n",
    "                                                       df[\"cc_num\"].values.tolist() +\n",
    "                                                       df[\"merchant\"].values.tolist()))}\n",
    "    df[\"in_node\"]= df[\"cc_num\"].apply(lambda x: mapping[x])\n",
    "    df[\"out_node\"]=df[\"merchant\"].apply(lambda x:mapping[x])\n",
    "    \n",
    "        \n",
    "    G=nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\\\n",
    "                        [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)\n",
    "    \n",
    "    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")\n",
    "     \n",
    "    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")\n",
    "    \n",
    "    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n",
    "    \n",
    "    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n",
    "    \n",
    "    \n",
    "    return G\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6027823d-ca18-439f-9bea-54460f7d7e70",
   "metadata": {},
   "source": [
    "- 판매자, 고객, 거래에 노드 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2be1fed8-bc99-46cd-8946-78e5edffb031",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_tu = build_graph_tripartite(df50, nx.Graph())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fb33a5-23a8-4378-91ac-aedcca26ef9b",
   "metadata": {},
   "source": [
    "## 사기 탐지를 위한 지도 및 비지도 임베딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59d2633-61e6-4a0e-aee5-5d676f3bf563",
   "metadata": {},
   "source": [
    "### 지도학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "41785fd7-4d03-4c31-b75d-37ab2c196c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from node2vec import Node2Vec\n",
    "from node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from node2vec import Node2Vec\n",
    "from node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n",
    "\n",
    "\n",
    "def build_graph_bipartite(df_input, graph_type=nx.Graph()):\n",
    "    \"\"\"\n",
    "    Build a bipartite graph from the input dataframe.\n",
    "\n",
    "    Parameters:\n",
    "        df_input (DataFrame): Input dataframe containing transaction information.\n",
    "        graph_type (networkx graph type, optional): Type of graph to create. Defaults to nx.Graph().\n",
    "\n",
    "    Returns:\n",
    "        networkx.Graph: Bipartite graph.\n",
    "    \"\"\"\n",
    "    df = df_input.copy()\n",
    "    mapping = {x: node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist() + df[\"merchant\"].values.tolist()))}\n",
    "    \n",
    "    df[\"from\"] = df[\"cc_num\"].apply(lambda x: mapping[x])  # 엣지의 출발점\n",
    "    df[\"to\"] = df[\"merchant\"].apply(lambda x: mapping[x])  # 엣지의 도착점\n",
    "    \n",
    "    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from', 'to']).agg({\"is_fraud\":\"sum\", \"amt\":\"sum\"}).reset_index()\n",
    "    df[\"is_fraud\"] = df[\"is_fraud\"].apply(lambda x: 1 if x > 0 else 0)\n",
    "    \n",
    "    G = nx.from_edgelist(df[[\"from\", \"to\"]].values, create_using=graph_type)\n",
    "    \n",
    "    nx.set_edge_attributes(G, {(int(x[\"from\"]), int(x[\"to\"])): x[\"is_fraud\"] for idx, x in df[[\"from\", \"to\", \"is_fraud\"]].iterrows()}, \"label\")  # 엣지 속성 설정, 각 속성의 사기 여부\n",
    "    \n",
    "    nx.set_edge_attributes(G, {(int(x[\"from\"]), int(x[\"to\"])): x[\"amt\"] for idx, x in df[[\"from\", \"to\", \"amt\"]].iterrows()}, \"weight\")  # 엣지 속성 설정, 각 엣지의 거래 금액\n",
    "\n",
    "    return G\n",
    "\n",
    "def train_and_evaluate_node2vec(df, embedding_dimension=128, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Train and evaluate node2vec embeddings with a Random Forest classifier.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): Input dataframe containing transaction information.\n",
    "        embedding_dimension (int, optional): Dimension of node embeddings. Defaults to 128.\n",
    "        test_size (float, optional): Proportion of the dataset to include in the test split. Defaults to 0.2.\n",
    "        random_state (int, optional): Seed used by the random number generator. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing evaluation metrics.\n",
    "    \"\"\"\n",
    "    G = build_graph_bipartite(df)\n",
    "    \n",
    "    train_edges, test_edges, train_labels, y = train_test_split(list(range(len(G.edges))), \n",
    "                                                                 list(nx.get_edge_attributes(G, \"label\").values()))\n",
    "    \n",
    "    edgs = list(G.edges)\n",
    "    train_graph = G.edge_subgraph([edgs[x] for x in train_edges]).copy()\n",
    "    train_graph.add_nodes_from(list(set(G.nodes) - set(train_graph.nodes)))\n",
    "\n",
    "    node2vec_train = Node2Vec(train_graph, dimensions=embedding_dimension, weight_key='weight')\n",
    "    model_train = node2vec_train.fit(window=10)\n",
    "    \n",
    "    classes = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\n",
    "    evaluation_results = {}\n",
    "    \n",
    "    for cl in classes:\n",
    "        embeddings_train = cl(keyed_vectors=model_train.wv)\n",
    "\n",
    "        train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n",
    "        test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n",
    "\n",
    "        rf = RandomForestClassifier(n_estimators=1000, random_state=random_state)\n",
    "        rf.fit(train_embeddings, train_labels)\n",
    "\n",
    "        yhat = rf.predict(test_embeddings)\n",
    "        acc = metrics.accuracy_score(y, yhat)\n",
    "        pre = metrics.precision_score(y, yhat)\n",
    "        rec = metrics.recall_score(y, yhat)\n",
    "        f1 = metrics.f1_score(y, yhat)\n",
    "        auc = metrics.roc_auc_score(y, yhat)\n",
    "        \n",
    "        evaluation_results[cl.__name__] = {\"accuracy\": acc, \"precision\": pre, \"recall\": rec, \"f1-score\": f1, \"auc\": auc}\n",
    "\n",
    "    return evaluation_results\n",
    "\n",
    "# Example usage:\n",
    "# evaluation_results = train_and_evaluate_node2vec(df50)\n",
    "# print(evaluation_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d895bfd6-7f14-4ccd-80e9-6aaf2b503e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f1ffe10ba514a79a32462a65e8cc813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/1629 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|██████████| 10/10 [00:04<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'HadamardEmbedder': {'accuracy': 0.541913632514818, 'precision': 0.7526315789473684, 'recall': 0.12139219015280135, 'f1-score': 0.2090643274853801, 'auc': 0.5408481221034277}, 'AverageEmbedder': {'accuracy': 0.7159187129551228, 'precision': 0.6975837879968823, 'recall': 0.7597623089983022, 'f1-score': 0.7273466070702965, 'auc': 0.7160298031477998}, 'WeightedL1Embedder': {'accuracy': 0.5055038103302286, 'precision': 0.6190476190476191, 'recall': 0.022071307300509338, 'f1-score': 0.042622950819672135, 'auc': 0.504278896893498}, 'WeightedL2Embedder': {'accuracy': 0.506350550381033, 'precision': 0.625, 'recall': 0.025466893039049237, 'f1-score': 0.048939641109298535, 'auc': 0.5051320951681733}}\n"
     ]
    }
   ],
   "source": [
    "evaluation_results = train_and_evaluate_node2vec(df50)\n",
    "print(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bb96ddff-8792-4024-b287-2377d78667fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_edges, test_edges, train_labels, y = train_test_split(list(range(len(G_bu.edges))), \n",
    "                                                             list(nx.get_edge_attributes(G_bu, \"label\").values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fc8cba97-6f77-4f04-9865-5054bf52f0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8854,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "13085652-cc1f-457e-ab48-bf963b9eb8b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2952,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "feb457b9-c205-46f0-886b-3710e497fa1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2a9cd27b-c225-4009-bb8e-bfe4162d02e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_book(fraudTrain, fraudrate, n, prev_results=None):\n",
    "    if prev_results is None:\n",
    "        df_results = pd.DataFrame(columns=[\n",
    "            'model', 'time', 'acc', 'pre', 'rec', 'f1', 'auc', 'graph_based', \n",
    "            'method', 'throw_rate', 'train_size', 'train_cols', 'train_frate', \n",
    "            'test_size', 'test_frate', 'hyper_params'\n",
    "        ])\n",
    "    else:\n",
    "        df_results = prev_results\n",
    "    \n",
    "    dfrate = throw(fraudTrain, fraudrate)\n",
    "    df_tr, df_tst = sklearn.model_selection.train_test_split(dfrate)\n",
    "        \n",
    "    dfn = fraudTrain[::n]\n",
    "    dfnn = dfn[~dfn.index.isin(df_tr.index)]\n",
    "    dfnn = dfnn.reset_index(drop=True)\n",
    "    df_trn, df_tstn = sklearn.model_selection.train_test_split(dfnn)\n",
    "   \n",
    "    df2, mask = concat(df_tr, df_tstn)\n",
    "    df2['index'] = df2.index\n",
    "    df = df2.reset_index()\n",
    "\n",
    "    G_df = build_graph_tripartite(df, nx.Graph())\n",
    "\n",
    "    train_edges, test_edges, train_labels, y = train_test_split(list(range(len(G_df.edges))), \n",
    "                                                                 list(nx.get_edge_attributes(G_df, \"label\").values()), \n",
    "                                                                 test_size=0.20, \n",
    "                                                                 random_state=42)\n",
    "\n",
    "    edgs = list(G_df.edges)\n",
    "    train_graph = G_df.edge_subgraph([edgs[x] for x in train_edges]).copy()\n",
    "    train_graph.add_nodes_from(list(set(G_df.nodes) - set(train_graph.nodes)))\n",
    "\n",
    "    node2vec_train = Node2Vec(train_graph, weight_key='weight')\n",
    "    model_train = node2vec_train.fit(window=10)\n",
    "\n",
    "    \n",
    "    #classes = [HadamardEmbedder]#, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\n",
    "    #evaluation_results = {}\n",
    "    \n",
    "    \n",
    "    embeddings_train = HadamardEmbedder(keyed_vectors=model_train.wv)\n",
    "\n",
    "    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n",
    "    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "    rf.fit(train_embeddings, train_labels)\n",
    "\n",
    "    yhat = rf.predict(test_embeddings)\n",
    "    acc = metrics.accuracy_score(y, yhat)\n",
    "    pre = metrics.precision_score(y, yhat)\n",
    "    rec = metrics.recall_score(y, yhat)\n",
    "    f1 = metrics.f1_score(y, yhat)\n",
    "    auc = metrics.roc_auc_score(y, yhat)\n",
    "\n",
    "    \n",
    "    result = {\n",
    "        'model': 'bipartite',\n",
    "        'time': None,\n",
    "        'acc': acc,\n",
    "        'pre': pre,\n",
    "        'rec': rec,\n",
    "        'f1': f1,\n",
    "        'auc': auc,\n",
    "        'graph_based': True,\n",
    "        'method': \"Hadaembedder\",\n",
    "        'throw_rate': df.is_fraud.mean(),\n",
    "        'train_size': len(train_labels),\n",
    "        'train_cols': 'amt',\n",
    "        'train_frate': np.array(train_labels).mean(),\n",
    "        'test_size': len(y),\n",
    "        'test_frate': np.array(y).mean(),\n",
    "        'hyper_params': None,\n",
    "        'theta': None,\n",
    "        'gamma': None\n",
    "    }\n",
    "\n",
    "    ymdhms = datetime.datetime.fromtimestamp(time.time()).strftime('%Y%m%d-%H%M%S') \n",
    "    df_results.to_csv(f'./results/{ymdhms}-pyod.csv',index=False)\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5eb44df5-b9bd-409d-b0da-07bec644c2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18fbdc39e5d540d3885349c470ec2e7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/36627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|██████████| 10/10 [01:37<00:00,  9.72s/it]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'results'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtry_book\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfraudTrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[49], line 80\u001b[0m, in \u001b[0;36mtry_book\u001b[0;34m(fraudTrain, fraudrate, n, prev_results)\u001b[0m\n\u001b[1;32m     58\u001b[0m result \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbipartite\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     77\u001b[0m }\n\u001b[1;32m     79\u001b[0m ymdhms \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mfromtimestamp(time\u001b[38;5;241m.\u001b[39mtime())\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[0;32m---> 80\u001b[0m \u001b[43mdf_results\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./results/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mymdhms\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-pyod.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df_results\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/pandas/core/generic.py:3720\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3709\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3711\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3712\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3713\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3717\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3718\u001b[0m )\n\u001b[0;32m-> 3720\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3723\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3725\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3737\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/pandas/io/formats/format.py:1189\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1168\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m   1171\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m   1172\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1187\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1188\u001b[0m )\n\u001b[0;32m-> 1189\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1192\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/pandas/io/formats/csvs.py:241\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    249\u001b[0m \n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    252\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    253\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/pandas/io/common.py:734\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[0;32m--> 734\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    738\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/pandas/io/common.py:597\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    595\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'results'"
     ]
    }
   ],
   "source": [
    "try_book(fraudTrain, 0.5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb66951d-125e-485c-bbde-bb0d3dbe3b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "try_book(fraudTrain, 0.7, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002fee1a-f928-4ea0-8c85-ed9aeb18ae3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3752be-6c8d-4c51-9630-8eea98f49856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1a56a5-c842-43ef-b501-f9f8a904826a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9b14cd5c-bd74-48d6-b255-a217df8f680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_book_A(fraudTrain, fraudrate, n, prev_results=None):\n",
    "    if prev_results is None:\n",
    "        df_results = pd.DataFrame(columns=[\n",
    "            'model', 'time', 'acc', 'pre', 'rec', 'f1', 'auc', 'graph_based', \n",
    "            'method', 'throw_rate', 'train_size', 'train_cols', 'train_frate', \n",
    "            'test_size', 'test_frate', 'hyper_params'\n",
    "        ])\n",
    "    else:\n",
    "        df_results = prev_results\n",
    "    \n",
    "    dfrate = throw(fraudTrain, fraudrate)\n",
    "    df_tr, df_tst = sklearn.model_selection.train_test_split(dfrate)\n",
    "        \n",
    "    dfn = fraudTrain[::n]\n",
    "    dfnn = dfn[~dfn.index.isin(df_tr.index)]\n",
    "    dfnn = dfnn.reset_index(drop=True)\n",
    "    df_trn, df_tstn = sklearn.model_selection.train_test_split(dfnn)\n",
    "   \n",
    "    df2, mask = concat(df_tr, df_tstn)\n",
    "    df2['index'] = df2.index\n",
    "    df = df2.reset_index()\n",
    "\n",
    "    G_df = build_graph_tripartite(df, nx.Graph())\n",
    "\n",
    "    train_edges, test_edges, train_labels, y = train_test_split(list(range(len(G_df.edges))), \n",
    "                                                                 list(nx.get_edge_attributes(G_df, \"label\").values()), \n",
    "                                                                 test_size=0.20, \n",
    "                                                                 random_state=42)\n",
    "\n",
    "    edgs = list(G_df.edges)\n",
    "    train_graph = G_df.edge_subgraph([edgs[x] for x in train_edges]).copy()\n",
    "    train_graph.add_nodes_from(list(set(G_df.nodes) - set(train_graph.nodes)))\n",
    "\n",
    "    node2vec_train = Node2Vec(train_graph, weight_key='weight')\n",
    "    model_train = node2vec_train.fit(window=10)\n",
    "\n",
    "    \n",
    "    #classes = [HadamardEmbedder]#, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\n",
    "    #evaluation_results = {}\n",
    "    \n",
    "    \n",
    "    embeddings_train = AverageEmbedder(keyed_vectors=model_train.wv)\n",
    "\n",
    "    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n",
    "    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "    rf.fit(train_embeddings, train_labels)\n",
    "\n",
    "    yhat = rf.predict(test_embeddings)\n",
    "    acc = metrics.accuracy_score(y, yhat)\n",
    "    pre = metrics.precision_score(y, yhat)\n",
    "    rec = metrics.recall_score(y, yhat)\n",
    "    f1 = metrics.f1_score(y, yhat)\n",
    "    auc = metrics.roc_auc_score(y, yhat)\n",
    "\n",
    "    \n",
    "    result = {\n",
    "        'model': 'bipartite',\n",
    "        'time': None,\n",
    "        'acc': acc,\n",
    "        'pre': pre,\n",
    "        'rec': rec,\n",
    "        'f1': f1,\n",
    "        'auc': auc,\n",
    "        'graph_based': True,\n",
    "        'method': 'AverageEmbedder',\n",
    "        'throw_rate': df.is_fraud.mean(),\n",
    "        'train_size': len(train_labels),\n",
    "        'train_cols': 'amt',\n",
    "        'train_frate': np.array(train_labels).mean(),\n",
    "        'test_size': len(y),\n",
    "        'test_frate': np.array(y).mean(),\n",
    "        'hyper_params': None,\n",
    "        'theta': None,\n",
    "        'gamma': None\n",
    "    }\n",
    "\n",
    "    \n",
    "    df_results = df_results.append(evaluation_results, ignore_index=True)\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0589a004-6045-471e-9576-38a30b6aec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_book_W1(fraudTrain, fraudrate, n, prev_results=None):\n",
    "    if prev_results is None:\n",
    "        df_results = pd.DataFrame(columns=[\n",
    "            'model', 'time', 'acc', 'pre', 'rec', 'f1', 'auc', 'graph_based', \n",
    "            'method', 'throw_rate', 'train_size', 'train_cols', 'train_frate', \n",
    "            'test_size', 'test_frate', 'hyper_params'\n",
    "        ])\n",
    "    else:\n",
    "        df_results = prev_results\n",
    "    \n",
    "    dfrate = throw(fraudTrain, fraudrate)\n",
    "    df_tr, df_tst = sklearn.model_selection.train_test_split(dfrate)\n",
    "        \n",
    "    dfn = fraudTrain[::n]\n",
    "    dfnn = dfn[~dfn.index.isin(df_tr.index)]\n",
    "    dfnn = dfnn.reset_index(drop=True)\n",
    "    df_trn, df_tstn = sklearn.model_selection.train_test_split(dfnn)\n",
    "   \n",
    "    df2, mask = concat(df_tr, df_tstn)\n",
    "    df2['index'] = df2.index\n",
    "    df = df2.reset_index()\n",
    "\n",
    "    G_df = build_graph_tripartite(df, nx.Graph())\n",
    "\n",
    "    train_edges, test_edges, train_labels, y = train_test_split(list(range(len(G_df.edges))), \n",
    "                                                                 list(nx.get_edge_attributes(G_df, \"label\").values()), \n",
    "                                                                 test_size=0.20, \n",
    "                                                                 random_state=42)\n",
    "\n",
    "    edgs = list(G_df.edges)\n",
    "    train_graph = G_df.edge_subgraph([edgs[x] for x in train_edges]).copy()\n",
    "    train_graph.add_nodes_from(list(set(G_df.nodes) - set(train_graph.nodes)))\n",
    "\n",
    "    node2vec_train = Node2Vec(train_graph, weight_key='weight')\n",
    "    model_train = node2vec_train.fit(window=10)\n",
    "\n",
    "    \n",
    "    #classes = [HadamardEmbedder]#, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\n",
    "    #evaluation_results = {}\n",
    "    \n",
    "    \n",
    "    embeddings_train = WeightedL1Embedder(keyed_vectors=model_train.wv)\n",
    "\n",
    "    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n",
    "    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "    rf.fit(train_embeddings, train_labels)\n",
    "\n",
    "    yhat = rf.predict(test_embeddings)\n",
    "    acc = metrics.accuracy_score(y, yhat)\n",
    "    pre = metrics.precision_score(y, yhat)\n",
    "    rec = metrics.recall_score(y, yhat)\n",
    "    f1 = metrics.f1_score(y, yhat)\n",
    "    auc = metrics.roc_auc_score(y, yhat)\n",
    "\n",
    "    \n",
    "    result = {\n",
    "        'model': 'bipartite',\n",
    "        'time': None,\n",
    "        'acc': acc,\n",
    "        'pre': pre,\n",
    "        'rec': rec,\n",
    "        'f1': f1,\n",
    "        'auc': auc,\n",
    "        'graph_based': True,\n",
    "        'method': 'WeightedL1Embedder',\n",
    "        'throw_rate': df.is_fraud.mean(),\n",
    "        'train_size': len(train_labels),\n",
    "        'train_cols': 'amt',\n",
    "        'train_frate': np.array(train_labels).mean(),\n",
    "        'test_size': len(y),\n",
    "        'test_frate': np.array(y).mean(),\n",
    "        'hyper_params': None,\n",
    "        'theta': None,\n",
    "        'gamma': None\n",
    "    }\n",
    "\n",
    "    \n",
    "    df_results = df_results.append(evaluation_results, ignore_index=True)\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "27dc4420-2aa5-49bb-9fa8-f8b70508702c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_book_W2(fraudTrain, fraudrate, n, prev_results=None):\n",
    "    if prev_results is None:\n",
    "        df_results = pd.DataFrame(columns=[\n",
    "            'model', 'time', 'acc', 'pre', 'rec', 'f1', 'auc', 'graph_based', \n",
    "            'method', 'throw_rate', 'train_size', 'train_cols', 'train_frate', \n",
    "            'test_size', 'test_frate', 'hyper_params'\n",
    "        ])\n",
    "    else:\n",
    "        df_results = prev_results\n",
    "    \n",
    "    dfrate = throw(fraudTrain, fraudrate)\n",
    "    df_tr, df_tst = sklearn.model_selection.train_test_split(dfrate)\n",
    "        \n",
    "    dfn = fraudTrain[::n]\n",
    "    dfnn = dfn[~dfn.index.isin(df_tr.index)]\n",
    "    dfnn = dfnn.reset_index(drop=True)\n",
    "    df_trn, df_tstn = sklearn.model_selection.train_test_split(dfnn)\n",
    "   \n",
    "    df2, mask = concat(df_tr, df_tstn)\n",
    "    df2['index'] = df2.index\n",
    "    df = df2.reset_index()\n",
    "\n",
    "    G_df = build_graph_tripartite(df, nx.Graph())\n",
    "\n",
    "    train_edges, test_edges, train_labels, y = train_test_split(list(range(len(G_df.edges))), \n",
    "                                                                 list(nx.get_edge_attributes(G_df, \"label\").values()), \n",
    "                                                                 test_size=0.20, \n",
    "                                                                 random_state=42)\n",
    "\n",
    "    edgs = list(G_df.edges)\n",
    "    train_graph = G_df.edge_subgraph([edgs[x] for x in train_edges]).copy()\n",
    "    train_graph.add_nodes_from(list(set(G_df.nodes) - set(train_graph.nodes)))\n",
    "\n",
    "    node2vec_train = Node2Vec(train_graph, weight_key='weight')\n",
    "    model_train = node2vec_train.fit(window=10)\n",
    "\n",
    "    \n",
    "    #classes = [HadamardEmbedder]#, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\n",
    "    #evaluation_results = {}\n",
    "    \n",
    "    \n",
    "    embeddings_train = WeightedL2Embedder(keyed_vectors=model_train.wv)\n",
    "\n",
    "    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n",
    "    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "    rf.fit(train_embeddings, train_labels)\n",
    "\n",
    "    yhat = rf.predict(test_embeddings)\n",
    "    acc = metrics.accuracy_score(y, yhat)\n",
    "    pre = metrics.precision_score(y, yhat)\n",
    "    rec = metrics.recall_score(y, yhat)\n",
    "    f1 = metrics.f1_score(y, yhat)\n",
    "    auc = metrics.roc_auc_score(y, yhat)\n",
    "\n",
    "    \n",
    "    result = {\n",
    "        'model': 'bipartite',\n",
    "        'time': None,\n",
    "        'acc': acc,\n",
    "        'pre': pre,\n",
    "        'rec': rec,\n",
    "        'f1': f1,\n",
    "        'auc': auc,\n",
    "        'graph_based': True,\n",
    "        'method': 'WeightedL2Embedder',\n",
    "        'throw_rate': df.is_fraud.mean(),\n",
    "        'train_size': len(train_labels),\n",
    "        'train_cols': 'amt',\n",
    "        'train_frate': np.array(train_labels).mean(),\n",
    "        'test_size': len(y),\n",
    "        'test_frate': np.array(y).mean(),\n",
    "        'hyper_params': None,\n",
    "        'theta': None,\n",
    "        'gamma': None\n",
    "    }\n",
    "\n",
    "    \n",
    "    df_results = df_results.append(evaluation_results, ignore_index=True)\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce200ec-07c7-4b9b-b526-e37d1bd3fcdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
