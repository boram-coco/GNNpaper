{
 "cells": [
  {
   "cell_type": "raw",
   "id": "c8593b37-a6e6-4ccb-b9b2-349f55cb2b69",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"[FRAUD] 책_코드 함수 만들기\"\n",
    "author: \"김보람\"\n",
    "date: \"04/04/2024\"\n",
    "categories:\n",
    "  - graph\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eedf6e-cd2a-40f0-aac9-e8899d5b1acc",
   "metadata": {},
   "source": [
    "# FAURD코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8812aa9-c507-4520-9dfc-38a88301f9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import networkx as nx\n",
    "import sklearn\n",
    "import xgboost as xgb\n",
    "import pickle \n",
    "import time \n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# sklearn\n",
    "from sklearn import model_selection # split함수이용\n",
    "from sklearn import ensemble # RF,GBM\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# gnn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d01fbd14-ccf1-4fca-b40d-b465036e21cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_sample_textbook(df):\n",
    "    df_majority = df[df.is_fraud==0].copy()\n",
    "    df_minority = df[df.is_fraud==1].copy()\n",
    "    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n",
    "    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n",
    "    return df_downsampled\n",
    "\n",
    "def compute_time_difference(group):\n",
    "    n = len(group)\n",
    "    result = []\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            time_difference = abs(group.iloc[i].trans_date_trans_time.value - group.iloc[j].trans_date_trans_time.value)\n",
    "            result.append([group.iloc[i].name, group.iloc[j].name, time_difference])\n",
    "    return result\n",
    "\n",
    "def mask(df):\n",
    "    df_tr,df_test = sklearn.model_selection.train_test_split(df, random_state=42)\n",
    "    N = len(df)\n",
    "    train_mask = [i in df_tr.index for i in range(N)]\n",
    "    test_mask = [i in df_test.index for i in range(N)]\n",
    "    train_mask = np.array(train_mask)\n",
    "    test_mask = np.array(test_mask)\n",
    "    return train_mask, test_mask\n",
    "\n",
    "def edge_index_selected(edge_index):\n",
    "    theta = edge_index[:,2].mean()\n",
    "    edge_index[:,2] = (np.exp(-edge_index[:,2]/theta) != 1)*(np.exp(-edge_index[:,2]/theta))\n",
    "    edge_index = edge_index.tolist()\n",
    "    mean_ = np.array(edge_index)[:,2].mean()\n",
    "    selected_edges = [(int(row[0]), int(row[1])) for row in edge_index if row[2] > mean_]\n",
    "    edge_index_selected = torch.tensor(selected_edges, dtype=torch.long).t()\n",
    "    return edge_index_selected\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00abe65e-e827-4a77-98d1-a1541c76b292",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../fraudTrain.pkl', 'rb') as file:\n",
    "    fraudTrain = pickle.load(file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6dd5e82-f16f-479c-8280-d5d0dcf3f075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trans_date_trans_time</th>\n",
       "      <th>cc_num</th>\n",
       "      <th>merchant</th>\n",
       "      <th>category</th>\n",
       "      <th>amt</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>gender</th>\n",
       "      <th>street</th>\n",
       "      <th>city</th>\n",
       "      <th>...</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>job</th>\n",
       "      <th>dob</th>\n",
       "      <th>trans_num</th>\n",
       "      <th>unix_time</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01 00:00:00</td>\n",
       "      <td>2.703190e+15</td>\n",
       "      <td>fraud_Rippin, Kub and Mann</td>\n",
       "      <td>misc_net</td>\n",
       "      <td>4.97</td>\n",
       "      <td>Jennifer</td>\n",
       "      <td>Banks</td>\n",
       "      <td>F</td>\n",
       "      <td>561 Perry Cove</td>\n",
       "      <td>Moravian Falls</td>\n",
       "      <td>...</td>\n",
       "      <td>36.0788</td>\n",
       "      <td>-81.1781</td>\n",
       "      <td>3495</td>\n",
       "      <td>Psychologist, counselling</td>\n",
       "      <td>1988-03-09</td>\n",
       "      <td>0b242abb623afc578575680df30655b9</td>\n",
       "      <td>1325376018</td>\n",
       "      <td>36.011293</td>\n",
       "      <td>-82.048315</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01 00:00:00</td>\n",
       "      <td>6.304230e+11</td>\n",
       "      <td>fraud_Heller, Gutmann and Zieme</td>\n",
       "      <td>grocery_pos</td>\n",
       "      <td>107.23</td>\n",
       "      <td>Stephanie</td>\n",
       "      <td>Gill</td>\n",
       "      <td>F</td>\n",
       "      <td>43039 Riley Greens Suite 393</td>\n",
       "      <td>Orient</td>\n",
       "      <td>...</td>\n",
       "      <td>48.8878</td>\n",
       "      <td>-118.2105</td>\n",
       "      <td>149</td>\n",
       "      <td>Special educational needs teacher</td>\n",
       "      <td>1978-06-21</td>\n",
       "      <td>1f76529f8574734946361c461b024d99</td>\n",
       "      <td>1325376044</td>\n",
       "      <td>49.159047</td>\n",
       "      <td>-118.186462</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01 00:00:00</td>\n",
       "      <td>3.885950e+13</td>\n",
       "      <td>fraud_Lind-Buckridge</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>220.11</td>\n",
       "      <td>Edward</td>\n",
       "      <td>Sanchez</td>\n",
       "      <td>M</td>\n",
       "      <td>594 White Dale Suite 530</td>\n",
       "      <td>Malad City</td>\n",
       "      <td>...</td>\n",
       "      <td>42.1808</td>\n",
       "      <td>-112.2620</td>\n",
       "      <td>4154</td>\n",
       "      <td>Nature conservation officer</td>\n",
       "      <td>1962-01-19</td>\n",
       "      <td>a1a22d70485983eac12b5b88dad1cf95</td>\n",
       "      <td>1325376051</td>\n",
       "      <td>43.150704</td>\n",
       "      <td>-112.154481</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01 00:01:00</td>\n",
       "      <td>3.534090e+15</td>\n",
       "      <td>fraud_Kutch, Hermiston and Farrell</td>\n",
       "      <td>gas_transport</td>\n",
       "      <td>45.00</td>\n",
       "      <td>Jeremy</td>\n",
       "      <td>White</td>\n",
       "      <td>M</td>\n",
       "      <td>9443 Cynthia Court Apt. 038</td>\n",
       "      <td>Boulder</td>\n",
       "      <td>...</td>\n",
       "      <td>46.2306</td>\n",
       "      <td>-112.1138</td>\n",
       "      <td>1939</td>\n",
       "      <td>Patent attorney</td>\n",
       "      <td>1967-01-12</td>\n",
       "      <td>6b849c168bdad6f867558c3793159a81</td>\n",
       "      <td>1325376076</td>\n",
       "      <td>47.034331</td>\n",
       "      <td>-112.561071</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01 00:03:00</td>\n",
       "      <td>3.755340e+14</td>\n",
       "      <td>fraud_Keeling-Crist</td>\n",
       "      <td>misc_pos</td>\n",
       "      <td>41.96</td>\n",
       "      <td>Tyler</td>\n",
       "      <td>Garcia</td>\n",
       "      <td>M</td>\n",
       "      <td>408 Bradley Rest</td>\n",
       "      <td>Doe Hill</td>\n",
       "      <td>...</td>\n",
       "      <td>38.4207</td>\n",
       "      <td>-79.4629</td>\n",
       "      <td>99</td>\n",
       "      <td>Dance movement psychotherapist</td>\n",
       "      <td>1986-03-28</td>\n",
       "      <td>a41d7549acf90789359a9aa5346dcb46</td>\n",
       "      <td>1325376186</td>\n",
       "      <td>38.674999</td>\n",
       "      <td>-78.632459</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048570</th>\n",
       "      <td>2020-03-10 16:07:00</td>\n",
       "      <td>6.011980e+15</td>\n",
       "      <td>fraud_Fadel Inc</td>\n",
       "      <td>health_fitness</td>\n",
       "      <td>77.00</td>\n",
       "      <td>Haley</td>\n",
       "      <td>Wagner</td>\n",
       "      <td>F</td>\n",
       "      <td>05561 Farrell Crescent</td>\n",
       "      <td>Annapolis</td>\n",
       "      <td>...</td>\n",
       "      <td>39.0305</td>\n",
       "      <td>-76.5515</td>\n",
       "      <td>92106</td>\n",
       "      <td>Accountant, chartered certified</td>\n",
       "      <td>1943-05-28</td>\n",
       "      <td>45ecd198c65e81e597db22e8d2ef7361</td>\n",
       "      <td>1362931649</td>\n",
       "      <td>38.779464</td>\n",
       "      <td>-76.317042</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048571</th>\n",
       "      <td>2020-03-10 16:07:00</td>\n",
       "      <td>4.839040e+15</td>\n",
       "      <td>fraud_Cremin, Hamill and Reichel</td>\n",
       "      <td>misc_pos</td>\n",
       "      <td>116.94</td>\n",
       "      <td>Meredith</td>\n",
       "      <td>Campbell</td>\n",
       "      <td>F</td>\n",
       "      <td>043 Hanson Turnpike</td>\n",
       "      <td>Hedrick</td>\n",
       "      <td>...</td>\n",
       "      <td>41.1826</td>\n",
       "      <td>-92.3097</td>\n",
       "      <td>1583</td>\n",
       "      <td>Geochemist</td>\n",
       "      <td>1999-06-28</td>\n",
       "      <td>c00ce51c6ebb7657474a77b9e0b51f34</td>\n",
       "      <td>1362931670</td>\n",
       "      <td>41.400318</td>\n",
       "      <td>-92.726724</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048572</th>\n",
       "      <td>2020-03-10 16:08:00</td>\n",
       "      <td>5.718440e+11</td>\n",
       "      <td>fraud_O'Connell, Botsford and Hand</td>\n",
       "      <td>home</td>\n",
       "      <td>21.27</td>\n",
       "      <td>Susan</td>\n",
       "      <td>Mills</td>\n",
       "      <td>F</td>\n",
       "      <td>005 Cody Estates</td>\n",
       "      <td>Louisville</td>\n",
       "      <td>...</td>\n",
       "      <td>38.2507</td>\n",
       "      <td>-85.7476</td>\n",
       "      <td>736284</td>\n",
       "      <td>Engineering geologist</td>\n",
       "      <td>1952-04-02</td>\n",
       "      <td>17c9dc8b2a6449ca2473726346e58e6c</td>\n",
       "      <td>1362931711</td>\n",
       "      <td>37.293339</td>\n",
       "      <td>-84.798122</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048573</th>\n",
       "      <td>2020-03-10 16:08:00</td>\n",
       "      <td>4.646850e+18</td>\n",
       "      <td>fraud_Thompson-Gleason</td>\n",
       "      <td>health_fitness</td>\n",
       "      <td>9.52</td>\n",
       "      <td>Julia</td>\n",
       "      <td>Bell</td>\n",
       "      <td>F</td>\n",
       "      <td>576 House Crossroad</td>\n",
       "      <td>West Sayville</td>\n",
       "      <td>...</td>\n",
       "      <td>40.7320</td>\n",
       "      <td>-73.1000</td>\n",
       "      <td>4056</td>\n",
       "      <td>Film/video editor</td>\n",
       "      <td>1990-06-25</td>\n",
       "      <td>5ca650881b48a6a38754f841c23b77ab</td>\n",
       "      <td>1362931718</td>\n",
       "      <td>39.773077</td>\n",
       "      <td>-72.213209</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048574</th>\n",
       "      <td>2020-03-10 16:08:00</td>\n",
       "      <td>2.283740e+15</td>\n",
       "      <td>fraud_Buckridge PLC</td>\n",
       "      <td>misc_pos</td>\n",
       "      <td>6.81</td>\n",
       "      <td>Shannon</td>\n",
       "      <td>Williams</td>\n",
       "      <td>F</td>\n",
       "      <td>9345 Spencer Junctions Suite 183</td>\n",
       "      <td>Alpharetta</td>\n",
       "      <td>...</td>\n",
       "      <td>34.0770</td>\n",
       "      <td>-84.3033</td>\n",
       "      <td>165556</td>\n",
       "      <td>Prison officer</td>\n",
       "      <td>1997-12-27</td>\n",
       "      <td>8d0a575fe635bbde12f1a2bffc126731</td>\n",
       "      <td>1362931730</td>\n",
       "      <td>33.601468</td>\n",
       "      <td>-83.891921</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1048575 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        trans_date_trans_time        cc_num  \\\n",
       "0         2019-01-01 00:00:00  2.703190e+15   \n",
       "1         2019-01-01 00:00:00  6.304230e+11   \n",
       "2         2019-01-01 00:00:00  3.885950e+13   \n",
       "3         2019-01-01 00:01:00  3.534090e+15   \n",
       "4         2019-01-01 00:03:00  3.755340e+14   \n",
       "...                       ...           ...   \n",
       "1048570   2020-03-10 16:07:00  6.011980e+15   \n",
       "1048571   2020-03-10 16:07:00  4.839040e+15   \n",
       "1048572   2020-03-10 16:08:00  5.718440e+11   \n",
       "1048573   2020-03-10 16:08:00  4.646850e+18   \n",
       "1048574   2020-03-10 16:08:00  2.283740e+15   \n",
       "\n",
       "                                   merchant        category     amt  \\\n",
       "0                fraud_Rippin, Kub and Mann        misc_net    4.97   \n",
       "1           fraud_Heller, Gutmann and Zieme     grocery_pos  107.23   \n",
       "2                      fraud_Lind-Buckridge   entertainment  220.11   \n",
       "3        fraud_Kutch, Hermiston and Farrell   gas_transport   45.00   \n",
       "4                       fraud_Keeling-Crist        misc_pos   41.96   \n",
       "...                                     ...             ...     ...   \n",
       "1048570                     fraud_Fadel Inc  health_fitness   77.00   \n",
       "1048571    fraud_Cremin, Hamill and Reichel        misc_pos  116.94   \n",
       "1048572  fraud_O'Connell, Botsford and Hand            home   21.27   \n",
       "1048573              fraud_Thompson-Gleason  health_fitness    9.52   \n",
       "1048574                 fraud_Buckridge PLC        misc_pos    6.81   \n",
       "\n",
       "             first      last gender                            street  \\\n",
       "0         Jennifer     Banks      F                    561 Perry Cove   \n",
       "1        Stephanie      Gill      F      43039 Riley Greens Suite 393   \n",
       "2           Edward   Sanchez      M          594 White Dale Suite 530   \n",
       "3           Jeremy     White      M       9443 Cynthia Court Apt. 038   \n",
       "4            Tyler    Garcia      M                  408 Bradley Rest   \n",
       "...            ...       ...    ...                               ...   \n",
       "1048570      Haley    Wagner      F            05561 Farrell Crescent   \n",
       "1048571   Meredith  Campbell      F               043 Hanson Turnpike   \n",
       "1048572      Susan     Mills      F                  005 Cody Estates   \n",
       "1048573      Julia      Bell      F               576 House Crossroad   \n",
       "1048574    Shannon  Williams      F  9345 Spencer Junctions Suite 183   \n",
       "\n",
       "                   city  ...      lat      long  city_pop  \\\n",
       "0        Moravian Falls  ...  36.0788  -81.1781      3495   \n",
       "1                Orient  ...  48.8878 -118.2105       149   \n",
       "2            Malad City  ...  42.1808 -112.2620      4154   \n",
       "3               Boulder  ...  46.2306 -112.1138      1939   \n",
       "4              Doe Hill  ...  38.4207  -79.4629        99   \n",
       "...                 ...  ...      ...       ...       ...   \n",
       "1048570       Annapolis  ...  39.0305  -76.5515     92106   \n",
       "1048571         Hedrick  ...  41.1826  -92.3097      1583   \n",
       "1048572      Louisville  ...  38.2507  -85.7476    736284   \n",
       "1048573   West Sayville  ...  40.7320  -73.1000      4056   \n",
       "1048574      Alpharetta  ...  34.0770  -84.3033    165556   \n",
       "\n",
       "                                       job         dob  \\\n",
       "0                Psychologist, counselling  1988-03-09   \n",
       "1        Special educational needs teacher  1978-06-21   \n",
       "2              Nature conservation officer  1962-01-19   \n",
       "3                          Patent attorney  1967-01-12   \n",
       "4           Dance movement psychotherapist  1986-03-28   \n",
       "...                                    ...         ...   \n",
       "1048570    Accountant, chartered certified  1943-05-28   \n",
       "1048571                         Geochemist  1999-06-28   \n",
       "1048572              Engineering geologist  1952-04-02   \n",
       "1048573                  Film/video editor  1990-06-25   \n",
       "1048574                     Prison officer  1997-12-27   \n",
       "\n",
       "                                trans_num   unix_time  merch_lat  merch_long  \\\n",
       "0        0b242abb623afc578575680df30655b9  1325376018  36.011293  -82.048315   \n",
       "1        1f76529f8574734946361c461b024d99  1325376044  49.159047 -118.186462   \n",
       "2        a1a22d70485983eac12b5b88dad1cf95  1325376051  43.150704 -112.154481   \n",
       "3        6b849c168bdad6f867558c3793159a81  1325376076  47.034331 -112.561071   \n",
       "4        a41d7549acf90789359a9aa5346dcb46  1325376186  38.674999  -78.632459   \n",
       "...                                   ...         ...        ...         ...   \n",
       "1048570  45ecd198c65e81e597db22e8d2ef7361  1362931649  38.779464  -76.317042   \n",
       "1048571  c00ce51c6ebb7657474a77b9e0b51f34  1362931670  41.400318  -92.726724   \n",
       "1048572  17c9dc8b2a6449ca2473726346e58e6c  1362931711  37.293339  -84.798122   \n",
       "1048573  5ca650881b48a6a38754f841c23b77ab  1362931718  39.773077  -72.213209   \n",
       "1048574  8d0a575fe635bbde12f1a2bffc126731  1362931730  33.601468  -83.891921   \n",
       "\n",
       "         is_fraud  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  \n",
       "...           ...  \n",
       "1048570         0  \n",
       "1048571         0  \n",
       "1048572         0  \n",
       "1048573         0  \n",
       "1048574         0  \n",
       "\n",
       "[1048575 rows x 22 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "fraudTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06ec0330-c92d-494f-b47b-2cb0dcde110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../function_proposed_gcn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2843d90-bbe5-4e3c-98a4-a63f8f099439",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../functions-book.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3164c1db-7c0d-4ed6-8dee-7c48f7ff1198",
   "metadata": {},
   "source": [
    "## 데이터정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd7e2cc5-dd94-4462-a43b-3e2c19f65fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df50 = throw(fraudTrain, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e987e34-1467-4608-918b-a4988910abe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask, test_mask = mask(df50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede10437-7a8b-4b83-936f-41ff40cf2e6d",
   "metadata": {},
   "source": [
    "# 책(신용카드 거래에 대한 그래프 분석)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9472937e-ac8d-45ab-8d26-454bbbf76c3d",
   "metadata": {},
   "source": [
    "`-` 이분그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44f78797-91db-4943-880f-625f41ff87ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_bipartite(df_input, graph_type=nx.Graph()):\n",
    "    df=df_input.copy()\n",
    "    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n",
    "                                                      df[\"merchant\"].values.tolist()))}\n",
    "    \n",
    "    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n",
    "    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n",
    "    \n",
    "    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n",
    "    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x>0 else 0)\n",
    "    \n",
    "    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n",
    "    \n",
    "    nx.set_edge_attributes(G, {(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부 \n",
    "    \n",
    "    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d7d24d3-ddc9-4573-b411-b787f47239af",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_bu = build_graph_bipartite(df50, nx.Graph(name=\"Bipartite Undirect\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a5f718-5b3e-4c30-8d67-f30a737143bb",
   "metadata": {},
   "source": [
    "`-` 삼분그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de4f50b6-213c-4b04-b3bd-4ff0a8221ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_tripartite(df_input, graph_type=nx.Graph()):\n",
    "    df=df_input.copy()\n",
    "    mapping={x:node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n",
    "                                                       df[\"cc_num\"].values.tolist() +\n",
    "                                                       df[\"merchant\"].values.tolist()))}\n",
    "    df[\"in_node\"]= df[\"cc_num\"].apply(lambda x: mapping[x])\n",
    "    df[\"out_node\"]=df[\"merchant\"].apply(lambda x:mapping[x])\n",
    "    \n",
    "        \n",
    "    G=nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\\\n",
    "                        [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)\n",
    "    \n",
    "    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")\n",
    "     \n",
    "    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")\n",
    "    \n",
    "    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n",
    "    \n",
    "    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n",
    "    \n",
    "    \n",
    "    return G\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6027823d-ca18-439f-9bea-54460f7d7e70",
   "metadata": {},
   "source": [
    "- 판매자, 고객, 거래에 노드 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2be1fed8-bc99-46cd-8946-78e5edffb031",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_tu = build_graph_tripartite(df50, nx.Graph())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fb33a5-23a8-4378-91ac-aedcca26ef9b",
   "metadata": {},
   "source": [
    "## 사기 탐지를 위한 지도 및 비지도 임베딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59d2633-61e6-4a0e-aee5-5d676f3bf563",
   "metadata": {},
   "source": [
    "### 지도학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "41785fd7-4d03-4c31-b75d-37ab2c196c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from node2vec import Node2Vec\n",
    "from node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from node2vec import Node2Vec\n",
    "from node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n",
    "\n",
    "\n",
    "def build_graph_bipartite(df_input, graph_type=nx.Graph()):\n",
    "    \"\"\"\n",
    "    Build a bipartite graph from the input dataframe.\n",
    "\n",
    "    Parameters:\n",
    "        df_input (DataFrame): Input dataframe containing transaction information.\n",
    "        graph_type (networkx graph type, optional): Type of graph to create. Defaults to nx.Graph().\n",
    "\n",
    "    Returns:\n",
    "        networkx.Graph: Bipartite graph.\n",
    "    \"\"\"\n",
    "    df = df_input.copy()\n",
    "    mapping = {x: node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist() + df[\"merchant\"].values.tolist()))}\n",
    "    \n",
    "    df[\"from\"] = df[\"cc_num\"].apply(lambda x: mapping[x])  # 엣지의 출발점\n",
    "    df[\"to\"] = df[\"merchant\"].apply(lambda x: mapping[x])  # 엣지의 도착점\n",
    "    \n",
    "    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from', 'to']).agg({\"is_fraud\":\"sum\", \"amt\":\"sum\"}).reset_index()\n",
    "    df[\"is_fraud\"] = df[\"is_fraud\"].apply(lambda x: 1 if x > 0 else 0)\n",
    "    \n",
    "    G = nx.from_edgelist(df[[\"from\", \"to\"]].values, create_using=graph_type)\n",
    "    \n",
    "    nx.set_edge_attributes(G, {(int(x[\"from\"]), int(x[\"to\"])): x[\"is_fraud\"] for idx, x in df[[\"from\", \"to\", \"is_fraud\"]].iterrows()}, \"label\")  # 엣지 속성 설정, 각 속성의 사기 여부\n",
    "    \n",
    "    nx.set_edge_attributes(G, {(int(x[\"from\"]), int(x[\"to\"])): x[\"amt\"] for idx, x in df[[\"from\", \"to\", \"amt\"]].iterrows()}, \"weight\")  # 엣지 속성 설정, 각 엣지의 거래 금액\n",
    "\n",
    "    return G\n",
    "\n",
    "def train_and_evaluate_node2vec(df, embedding_dimension=128, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Train and evaluate node2vec embeddings with a Random Forest classifier.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): Input dataframe containing transaction information.\n",
    "        embedding_dimension (int, optional): Dimension of node embeddings. Defaults to 128.\n",
    "        test_size (float, optional): Proportion of the dataset to include in the test split. Defaults to 0.2.\n",
    "        random_state (int, optional): Seed used by the random number generator. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing evaluation metrics.\n",
    "    \"\"\"\n",
    "    G = build_graph_bipartite(df)\n",
    "    \n",
    "    train_edges, test_edges, train_labels, y = train_test_split(list(range(len(G.edges))), \n",
    "                                                                 list(nx.get_edge_attributes(G, \"label\").values()))\n",
    "    \n",
    "    edgs = list(G.edges)\n",
    "    train_graph = G.edge_subgraph([edgs[x] for x in train_edges]).copy()\n",
    "    train_graph.add_nodes_from(list(set(G.nodes) - set(train_graph.nodes)))\n",
    "\n",
    "    node2vec_train = Node2Vec(train_graph, dimensions=embedding_dimension, weight_key='weight')\n",
    "    model_train = node2vec_train.fit(window=10)\n",
    "    \n",
    "    classes = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\n",
    "    evaluation_results = {}\n",
    "    \n",
    "    for cl in classes:\n",
    "        embeddings_train = cl(keyed_vectors=model_train.wv)\n",
    "\n",
    "        train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n",
    "        test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n",
    "\n",
    "        rf = RandomForestClassifier(n_estimators=1000, random_state=random_state)\n",
    "        rf.fit(train_embeddings, train_labels)\n",
    "\n",
    "        yhat = rf.predict(test_embeddings)\n",
    "        acc = metrics.accuracy_score(y, yhat)\n",
    "        pre = metrics.precision_score(y, yhat)\n",
    "        rec = metrics.recall_score(y, yhat)\n",
    "        f1 = metrics.f1_score(y, yhat)\n",
    "        auc = metrics.roc_auc_score(y, yhat)\n",
    "        \n",
    "        evaluation_results[cl.__name__] = {\"accuracy\": acc, \"precision\": pre, \"recall\": rec, \"f1-score\": f1, \"auc\": auc}\n",
    "\n",
    "    return evaluation_results\n",
    "\n",
    "# Example usage:\n",
    "# evaluation_results = train_and_evaluate_node2vec(df50)\n",
    "# print(evaluation_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d895bfd6-7f14-4ccd-80e9-6aaf2b503e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f1ffe10ba514a79a32462a65e8cc813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/1629 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|██████████| 10/10 [00:04<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'HadamardEmbedder': {'accuracy': 0.541913632514818, 'precision': 0.7526315789473684, 'recall': 0.12139219015280135, 'f1-score': 0.2090643274853801, 'auc': 0.5408481221034277}, 'AverageEmbedder': {'accuracy': 0.7159187129551228, 'precision': 0.6975837879968823, 'recall': 0.7597623089983022, 'f1-score': 0.7273466070702965, 'auc': 0.7160298031477998}, 'WeightedL1Embedder': {'accuracy': 0.5055038103302286, 'precision': 0.6190476190476191, 'recall': 0.022071307300509338, 'f1-score': 0.042622950819672135, 'auc': 0.504278896893498}, 'WeightedL2Embedder': {'accuracy': 0.506350550381033, 'precision': 0.625, 'recall': 0.025466893039049237, 'f1-score': 0.048939641109298535, 'auc': 0.5051320951681733}}\n"
     ]
    }
   ],
   "source": [
    "evaluation_results = train_and_evaluate_node2vec(df50)\n",
    "print(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bb96ddff-8792-4024-b287-2377d78667fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_edges, test_edges, train_labels, y = train_test_split(list(range(len(G_bu.edges))), \n",
    "                                                             list(nx.get_edge_attributes(G_bu, \"label\").values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fc8cba97-6f77-4f04-9865-5054bf52f0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8854,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "13085652-cc1f-457e-ab48-bf963b9eb8b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2952,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "feb457b9-c205-46f0-886b-3710e497fa1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a9cd27b-c225-4009-bb8e-bfe4162d02e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_book(fraudTrain, fraudrate, n, prev_results=None):\n",
    "    if prev_results is None:\n",
    "        df_results = pd.DataFrame(columns=[\n",
    "            'model', 'time', 'acc', 'pre', 'rec', 'f1', 'auc', 'graph_based', \n",
    "            'method', 'throw_rate', 'train_size', 'train_cols', 'train_frate', \n",
    "            'test_size', 'test_frate', 'hyper_params'\n",
    "        ])\n",
    "    else:\n",
    "        df_results = prev_results\n",
    "    \n",
    "    dfrate = throw(fraudTrain, fraudrate)\n",
    "    df_tr, df_tst = sklearn.model_selection.train_test_split(dfrate)\n",
    "        \n",
    "    dfn = fraudTrain[::n]\n",
    "    dfnn = dfn[~dfn.index.isin(df_tr.index)]\n",
    "    dfnn = dfnn.reset_index(drop=True)\n",
    "    df_trn, df_tstn = sklearn.model_selection.train_test_split(dfnn)\n",
    "   \n",
    "    df2, mask = concat(df_tr, df_tstn)\n",
    "    df2['index'] = df2.index\n",
    "    df = df2.reset_index()\n",
    "\n",
    "    G_df = build_graph_tripartite(df, nx.Graph())\n",
    "\n",
    "    train_edges, test_edges, train_labels, y = train_test_split(list(range(len(G_df.edges))), \n",
    "                                                                 list(nx.get_edge_attributes(G_df, \"label\").values()), \n",
    "                                                                 test_size=0.20, \n",
    "                                                                 random_state=42)\n",
    "\n",
    "    edgs = list(G_df.edges)\n",
    "    train_graph = G_df.edge_subgraph([edgs[x] for x in train_edges]).copy()\n",
    "    train_graph.add_nodes_from(list(set(G_df.nodes) - set(train_graph.nodes)))\n",
    "\n",
    "    node2vec_train = Node2Vec(train_graph, weight_key='weight')\n",
    "    model_train = node2vec_train.fit(window=10)\n",
    "\n",
    "    \n",
    "    #classes = [HadamardEmbedder]#, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\n",
    "    #evaluation_results = {}\n",
    "    \n",
    "    \n",
    "    embeddings_train = HadamardEmbedder(keyed_vectors=model_train.wv)\n",
    "\n",
    "    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n",
    "    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "    rf.fit(train_embeddings, train_labels)\n",
    "\n",
    "    yhat = rf.predict(test_embeddings)\n",
    "    acc = metrics.accuracy_score(y, yhat)\n",
    "    pre = metrics.precision_score(y, yhat)\n",
    "    rec = metrics.recall_score(y, yhat)\n",
    "    f1 = metrics.f1_score(y, yhat)\n",
    "    auc = metrics.roc_auc_score(y, yhat)\n",
    "\n",
    "    \n",
    "    result = {\n",
    "        'model': 'bipartite',\n",
    "        'time': None,\n",
    "        'acc': acc,\n",
    "        'pre': pre,\n",
    "        'rec': rec,\n",
    "        'f1': f1,\n",
    "        'auc': auc,\n",
    "        'graph_based': True,\n",
    "        'method': \"Hadaembedder\",\n",
    "        'throw_rate': df.is_fraud.mean(),\n",
    "        'train_size': len(train_labels),\n",
    "        'train_cols': 'amt',\n",
    "        'train_frate': np.array(train_labels).mean(),\n",
    "        'test_size': len(y),\n",
    "        'test_frate': np.array(y).mean(),\n",
    "        'hyper_params': None,\n",
    "        'theta': None,\n",
    "        'gamma': None\n",
    "    }\n",
    "\n",
    "    ymdhms = datetime.datetime.fromtimestamp(time.time()).strftime('%Y%m%d-%H%M%S') \n",
    "    df_results.to_csv(f'../results/{ymdhms}-pyod.csv',index=False)\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5eb44df5-b9bd-409d-b0da-07bec644c2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "187d58e3f27b4046840c9e37b0bd32ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/36621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|██████████| 10/10 [01:29<00:00,  8.93s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>time</th>\n",
       "      <th>acc</th>\n",
       "      <th>pre</th>\n",
       "      <th>rec</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc</th>\n",
       "      <th>graph_based</th>\n",
       "      <th>method</th>\n",
       "      <th>throw_rate</th>\n",
       "      <th>train_size</th>\n",
       "      <th>train_cols</th>\n",
       "      <th>train_frate</th>\n",
       "      <th>test_size</th>\n",
       "      <th>test_frate</th>\n",
       "      <th>hyper_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [model, time, acc, pre, rec, f1, auc, graph_based, method, throw_rate, train_size, train_cols, train_frate, test_size, test_frate, hyper_params]\n",
       "Index: []"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_book(fraudTrain, 0.5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e71a8cc0-6a8d-413f-bc5b-0be9d0ddaa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfrate = throw(fraudTrain, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1122b17f-36c8-42b4-bc79-23c294411f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr, df_tst = sklearn.model_selection.train_test_split(dfrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a4069b1-7d6a-4da9-9271-35ec70137669",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn = fraudTrain[::10]\n",
    "dfnn = dfn[~dfn.index.isin(df_tr.index)]\n",
    "dfnn = dfnn.reset_index(drop=True)\n",
    "df_trn, df_tstn = sklearn.model_selection.train_test_split(dfnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5948ac7-91d5-4d26-9e6c-87b280a0a166",
   "metadata": {},
   "outputs": [],
   "source": [
    "    df2, mask = concat(df_tr, df_tstn)\n",
    "    df2['index'] = df2.index\n",
    "    df = df2.reset_index()\n",
    "\n",
    "    G_df = build_graph_tripartite(df, nx.Graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24aedf80-8e6b-40a0-86d3-7ad3bd844391",
   "metadata": {},
   "outputs": [],
   "source": [
    " train_edges, test_edges, train_labels, y = train_test_split(list(range(len(G_df.edges))), \n",
    "                                                                 list(nx.get_edge_attributes(G_df, \"label\").values()), \n",
    "                                                                 test_size=0.20, \n",
    "                                                                 random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1b0a89d-90a3-463c-ae9d-5fb0525271cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4090d5511b7d48a8a57a4cc1b5e57e76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/36626 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|██████████| 10/10 [01:30<00:00,  9.05s/it]\n"
     ]
    }
   ],
   "source": [
    "edgs = list(G_df.edges)\n",
    "train_graph = G_df.edge_subgraph([edgs[x] for x in train_edges]).copy()\n",
    "train_graph.add_nodes_from(list(set(G_df.nodes) - set(train_graph.nodes)))\n",
    "\n",
    "node2vec_train = Node2Vec(train_graph, weight_key='weight')\n",
    "model_train = node2vec_train.fit(window=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5fd9318d-9a2b-472c-aa08-3dab9e72fba3",
   "metadata": {},
   "outputs": [],
   "source": [
    " embeddings_train = HadamardEmbedder(keyed_vectors=model_train.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4fd2583-e422-4779-b3dc-07d0212b74a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_estimators=1000, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_estimators=1000, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(n_estimators=1000, random_state=42)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n",
    "test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "rf.fit(train_embeddings, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc12a64c-326a-4708-9f24-5340e0370c82",
   "metadata": {},
   "outputs": [],
   "source": [
    ".0 h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ed5146-b20f-48b2-bfc6-95f4dbbf8c20",
   "metadata": {},
   "outputs": [],
   "source": [
    " result = {\n",
    "        'model': 'bipartite',\n",
    "        'time': None,\n",
    "        'acc': acc,\n",
    "        'pre': pre,\n",
    "        'rec': rec,\n",
    "        'f1': f1,\n",
    "        'auc': auc,\n",
    "        'graph_based': True,\n",
    "        'method': \"Hadaembedder\",\n",
    "        'throw_rate': df.is_fraud.mean(),\n",
    "        'train_size': len(train_labels),\n",
    "        'train_cols': 'amt',\n",
    "        'train_frate': np.array(train_labels).mean(),\n",
    "        'test_size': len(y),\n",
    "        'test_frate': np.array(y).mean(),\n",
    "        'hyper_params': None,\n",
    "        'theta': None,\n",
    "        'gamma': None\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbd7401-bb5d-472e-82db-2eb392a2b29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "   \n",
    "   \n",
    "\n",
    "    \n",
    "    #classes = [HadamardEmbedder]#, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\n",
    "    #evaluation_results = {}\n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7aa962-d380-4d6f-9b3d-edbeb8a3bbf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b61517d-9695-49bb-a5fa-20c64c27c14a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6740c3-be2c-444e-a146-532ed14b85f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654537f7-13a1-4de7-9c6d-4eec6a35a162",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf845291-dc72-48f4-989c-e9388769d424",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791addf9-8d81-4717-bdbd-b83956e8f178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99e36ab-2ea9-4909-8402-b5b602a6927c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47966dc-dded-4c8a-8884-badaec5aabd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd77fe5-0123-48b8-9fed-a5d542e86ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de2761b-be01-4d02-bfcf-b2099c5f10b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f2998f-2caa-43a1-ba30-cbbb2111d02a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2346a48d-f80c-4f31-af69-ae618bf6f499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e3411b-7f9f-4650-b07f-a64bbf4cd119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429c0082-ac42-4525-947a-f858020d3079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb66951d-125e-485c-bbde-bb0d3dbe3b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "try_book(fraudTrain, 0.7, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002fee1a-f928-4ea0-8c85-ed9aeb18ae3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3752be-6c8d-4c51-9630-8eea98f49856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1a56a5-c842-43ef-b501-f9f8a904826a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9b14cd5c-bd74-48d6-b255-a217df8f680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_book_A(fraudTrain, fraudrate, n, prev_results=None):\n",
    "    if prev_results is None:\n",
    "        df_results = pd.DataFrame(columns=[\n",
    "            'model', 'time', 'acc', 'pre', 'rec', 'f1', 'auc', 'graph_based', \n",
    "            'method', 'throw_rate', 'train_size', 'train_cols', 'train_frate', \n",
    "            'test_size', 'test_frate', 'hyper_params'\n",
    "        ])\n",
    "    else:\n",
    "        df_results = prev_results\n",
    "    \n",
    "    dfrate = throw(fraudTrain, fraudrate)\n",
    "    df_tr, df_tst = sklearn.model_selection.train_test_split(dfrate)\n",
    "        \n",
    "    dfn = fraudTrain[::n]\n",
    "    dfnn = dfn[~dfn.index.isin(df_tr.index)]\n",
    "    dfnn = dfnn.reset_index(drop=True)\n",
    "    df_trn, df_tstn = sklearn.model_selection.train_test_split(dfnn)\n",
    "   \n",
    "    df2, mask = concat(df_tr, df_tstn)\n",
    "    df2['index'] = df2.index\n",
    "    df = df2.reset_index()\n",
    "\n",
    "    G_df = build_graph_tripartite(df, nx.Graph())\n",
    "\n",
    "    train_edges, test_edges, train_labels, y = train_test_split(list(range(len(G_df.edges))), \n",
    "                                                                 list(nx.get_edge_attributes(G_df, \"label\").values()), \n",
    "                                                                 test_size=0.20, \n",
    "                                                                 random_state=42)\n",
    "\n",
    "    edgs = list(G_df.edges)\n",
    "    train_graph = G_df.edge_subgraph([edgs[x] for x in train_edges]).copy()\n",
    "    train_graph.add_nodes_from(list(set(G_df.nodes) - set(train_graph.nodes)))\n",
    "\n",
    "    node2vec_train = Node2Vec(train_graph, weight_key='weight')\n",
    "    model_train = node2vec_train.fit(window=10)\n",
    "\n",
    "    \n",
    "    #classes = [HadamardEmbedder]#, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\n",
    "    #evaluation_results = {}\n",
    "    \n",
    "    \n",
    "    embeddings_train = AverageEmbedder(keyed_vectors=model_train.wv)\n",
    "\n",
    "    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n",
    "    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "    rf.fit(train_embeddings, train_labels)\n",
    "\n",
    "    yhat = rf.predict(test_embeddings)\n",
    "    acc = metrics.accuracy_score(y, yhat)\n",
    "    pre = metrics.precision_score(y, yhat)\n",
    "    rec = metrics.recall_score(y, yhat)\n",
    "    f1 = metrics.f1_score(y, yhat)\n",
    "    auc = metrics.roc_auc_score(y, yhat)\n",
    "\n",
    "    \n",
    "    result = {\n",
    "        'model': 'bipartite',\n",
    "        'time': None,\n",
    "        'acc': acc,\n",
    "        'pre': pre,\n",
    "        'rec': rec,\n",
    "        'f1': f1,\n",
    "        'auc': auc,\n",
    "        'graph_based': True,\n",
    "        'method': 'AverageEmbedder',\n",
    "        'throw_rate': df.is_fraud.mean(),\n",
    "        'train_size': len(train_labels),\n",
    "        'train_cols': 'amt',\n",
    "        'train_frate': np.array(train_labels).mean(),\n",
    "        'test_size': len(y),\n",
    "        'test_frate': np.array(y).mean(),\n",
    "        'hyper_params': None,\n",
    "        'theta': None,\n",
    "        'gamma': None\n",
    "    }\n",
    "\n",
    "    \n",
    "    df_results = df_results.append(evaluation_results, ignore_index=True)\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0589a004-6045-471e-9576-38a30b6aec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_book_W1(fraudTrain, fraudrate, n, prev_results=None):\n",
    "    if prev_results is None:\n",
    "        df_results = pd.DataFrame(columns=[\n",
    "            'model', 'time', 'acc', 'pre', 'rec', 'f1', 'auc', 'graph_based', \n",
    "            'method', 'throw_rate', 'train_size', 'train_cols', 'train_frate', \n",
    "            'test_size', 'test_frate', 'hyper_params'\n",
    "        ])\n",
    "    else:\n",
    "        df_results = prev_results\n",
    "    \n",
    "    dfrate = throw(fraudTrain, fraudrate)\n",
    "    df_tr, df_tst = sklearn.model_selection.train_test_split(dfrate)\n",
    "        \n",
    "    dfn = fraudTrain[::n]\n",
    "    dfnn = dfn[~dfn.index.isin(df_tr.index)]\n",
    "    dfnn = dfnn.reset_index(drop=True)\n",
    "    df_trn, df_tstn = sklearn.model_selection.train_test_split(dfnn)\n",
    "   \n",
    "    df2, mask = concat(df_tr, df_tstn)\n",
    "    df2['index'] = df2.index\n",
    "    df = df2.reset_index()\n",
    "\n",
    "    G_df = build_graph_tripartite(df, nx.Graph())\n",
    "\n",
    "    train_edges, test_edges, train_labels, y = train_test_split(list(range(len(G_df.edges))), \n",
    "                                                                 list(nx.get_edge_attributes(G_df, \"label\").values()), \n",
    "                                                                 test_size=0.20, \n",
    "                                                                 random_state=42)\n",
    "\n",
    "    edgs = list(G_df.edges)\n",
    "    train_graph = G_df.edge_subgraph([edgs[x] for x in train_edges]).copy()\n",
    "    train_graph.add_nodes_from(list(set(G_df.nodes) - set(train_graph.nodes)))\n",
    "\n",
    "    node2vec_train = Node2Vec(train_graph, weight_key='weight')\n",
    "    model_train = node2vec_train.fit(window=10)\n",
    "\n",
    "    \n",
    "    #classes = [HadamardEmbedder]#, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\n",
    "    #evaluation_results = {}\n",
    "    \n",
    "    \n",
    "    embeddings_train = WeightedL1Embedder(keyed_vectors=model_train.wv)\n",
    "\n",
    "    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n",
    "    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "    rf.fit(train_embeddings, train_labels)\n",
    "\n",
    "    yhat = rf.predict(test_embeddings)\n",
    "    acc = metrics.accuracy_score(y, yhat)\n",
    "    pre = metrics.precision_score(y, yhat)\n",
    "    rec = metrics.recall_score(y, yhat)\n",
    "    f1 = metrics.f1_score(y, yhat)\n",
    "    auc = metrics.roc_auc_score(y, yhat)\n",
    "\n",
    "    \n",
    "    result = {\n",
    "        'model': 'bipartite',\n",
    "        'time': None,\n",
    "        'acc': acc,\n",
    "        'pre': pre,\n",
    "        'rec': rec,\n",
    "        'f1': f1,\n",
    "        'auc': auc,\n",
    "        'graph_based': True,\n",
    "        'method': 'WeightedL1Embedder',\n",
    "        'throw_rate': df.is_fraud.mean(),\n",
    "        'train_size': len(train_labels),\n",
    "        'train_cols': 'amt',\n",
    "        'train_frate': np.array(train_labels).mean(),\n",
    "        'test_size': len(y),\n",
    "        'test_frate': np.array(y).mean(),\n",
    "        'hyper_params': None,\n",
    "        'theta': None,\n",
    "        'gamma': None\n",
    "    }\n",
    "\n",
    "    \n",
    "    df_results = df_results.append(evaluation_results, ignore_index=True)\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "27dc4420-2aa5-49bb-9fa8-f8b70508702c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_book_W2(fraudTrain, fraudrate, n, prev_results=None):\n",
    "    if prev_results is None:\n",
    "        df_results = pd.DataFrame(columns=[\n",
    "            'model', 'time', 'acc', 'pre', 'rec', 'f1', 'auc', 'graph_based', \n",
    "            'method', 'throw_rate', 'train_size', 'train_cols', 'train_frate', \n",
    "            'test_size', 'test_frate', 'hyper_params'\n",
    "        ])\n",
    "    else:\n",
    "        df_results = prev_results\n",
    "    \n",
    "    dfrate = throw(fraudTrain, fraudrate)\n",
    "    df_tr, df_tst = sklearn.model_selection.train_test_split(dfrate)\n",
    "        \n",
    "    dfn = fraudTrain[::n]\n",
    "    dfnn = dfn[~dfn.index.isin(df_tr.index)]\n",
    "    dfnn = dfnn.reset_index(drop=True)\n",
    "    df_trn, df_tstn = sklearn.model_selection.train_test_split(dfnn)\n",
    "   \n",
    "    df2, mask = concat(df_tr, df_tstn)\n",
    "    df2['index'] = df2.index\n",
    "    df = df2.reset_index()\n",
    "\n",
    "    G_df = build_graph_tripartite(df, nx.Graph())\n",
    "\n",
    "    train_edges, test_edges, train_labels, y = train_test_split(list(range(len(G_df.edges))), \n",
    "                                                                 list(nx.get_edge_attributes(G_df, \"label\").values()), \n",
    "                                                                 test_size=0.20, \n",
    "                                                                 random_state=42)\n",
    "\n",
    "    edgs = list(G_df.edges)\n",
    "    train_graph = G_df.edge_subgraph([edgs[x] for x in train_edges]).copy()\n",
    "    train_graph.add_nodes_from(list(set(G_df.nodes) - set(train_graph.nodes)))\n",
    "\n",
    "    node2vec_train = Node2Vec(train_graph, weight_key='weight')\n",
    "    model_train = node2vec_train.fit(window=10)\n",
    "\n",
    "    \n",
    "    #classes = [HadamardEmbedder]#, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\n",
    "    #evaluation_results = {}\n",
    "    \n",
    "    \n",
    "    embeddings_train = WeightedL2Embedder(keyed_vectors=model_train.wv)\n",
    "\n",
    "    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n",
    "    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "    rf.fit(train_embeddings, train_labels)\n",
    "\n",
    "    yhat = rf.predict(test_embeddings)\n",
    "    acc = metrics.accuracy_score(y, yhat)\n",
    "    pre = metrics.precision_score(y, yhat)\n",
    "    rec = metrics.recall_score(y, yhat)\n",
    "    f1 = metrics.f1_score(y, yhat)\n",
    "    auc = metrics.roc_auc_score(y, yhat)\n",
    "\n",
    "    \n",
    "    result = {\n",
    "        'model': 'bipartite',\n",
    "        'time': None,\n",
    "        'acc': acc,\n",
    "        'pre': pre,\n",
    "        'rec': rec,\n",
    "        'f1': f1,\n",
    "        'auc': auc,\n",
    "        'graph_based': True,\n",
    "        'method': 'WeightedL2Embedder',\n",
    "        'throw_rate': df.is_fraud.mean(),\n",
    "        'train_size': len(train_labels),\n",
    "        'train_cols': 'amt',\n",
    "        'train_frate': np.array(train_labels).mean(),\n",
    "        'test_size': len(y),\n",
    "        'test_frate': np.array(y).mean(),\n",
    "        'hyper_params': None,\n",
    "        'theta': None,\n",
    "        'gamma': None\n",
    "    }\n",
    "\n",
    "    \n",
    "    df_results = df_results.append(evaluation_results, ignore_index=True)\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce200ec-07c7-4b9b-b526-e37d1bd3fcdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
