{
 "cells": [
  {
   "cell_type": "raw",
   "id": "c8593b37-a6e6-4ccb-b9b2-349f55cb2b69",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"[FRAUD] 책_코드 함수 만들기(tri)\"\n",
    "author: \"김보람\"\n",
    "date: \"05/20/2024\"\n",
    "categories:\n",
    "  - graph\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eedf6e-cd2a-40f0-aac9-e8899d5b1acc",
   "metadata": {},
   "source": [
    "# FAURD코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8812aa9-c507-4520-9dfc-38a88301f9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import networkx as nx\n",
    "import sklearn\n",
    "import xgboost as xgb\n",
    "import pickle \n",
    "import time \n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# sklearn\n",
    "from sklearn import model_selection # split함수이용\n",
    "from sklearn import ensemble # RF,GBM\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# gnn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from node2vec import Node2Vec\n",
    "from node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from node2vec import Node2Vec\n",
    "from node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc97c744-3215-45e3-a4be-ce0ca50c8592",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train1 = pd.read_csv('~/Dropbox/Data/df_train1.csv')\n",
    "df_train2 = pd.read_csv('~/Dropbox/Data/df_train2.csv')\n",
    "df_train3 = pd.read_csv('~/Dropbox/Data/df_train3.csv')\n",
    "df_train4 = pd.read_csv('~/Dropbox/Data/df_train4.csv')\n",
    "df_train5 = pd.read_csv('~/Dropbox/Data/df_train5.csv')\n",
    "df_train6 = pd.read_csv('~/Dropbox/Data/df_train6.csv')\n",
    "df_train7 = pd.read_csv('~/Dropbox/Data/df_train7.csv')\n",
    "df_train8 = pd.read_csv('~/Dropbox/Data/df_train8.csv')\n",
    "df_test = pd.read_csv('~/Dropbox/Data/df_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b94f26f6-0235-497f-940a-71c89231dd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df1 = pd.concat([df_train1, df_test])\n",
    "_df2 = pd.concat([df_train2, df_test])\n",
    "_df3 = pd.concat([df_train3, df_test])\n",
    "_df4 = pd.concat([df_train4, df_test])\n",
    "_df5 = pd.concat([df_train5, df_test])\n",
    "_df6 = pd.concat([df_train6, df_test])\n",
    "_df7 = pd.concat([df_train7, df_test])\n",
    "_df8 = pd.concat([df_train8, df_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a5f718-5b3e-4c30-8d67-f30a737143bb",
   "metadata": {},
   "source": [
    "`-` 삼분그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de4f50b6-213c-4b04-b3bd-4ff0a8221ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_tripartite(df_input, graph_type=nx.Graph()):\n",
    "    df=df_input.copy()\n",
    "    mapping={x:node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n",
    "                                                       df[\"cc_num\"].values.tolist() +\n",
    "                                                       df[\"merchant\"].values.tolist()))}\n",
    "    df[\"in_node\"]= df[\"cc_num\"].apply(lambda x: mapping[x])\n",
    "    df[\"out_node\"]=df[\"merchant\"].apply(lambda x:mapping[x])\n",
    "    \n",
    "        \n",
    "    G=nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\\\n",
    "                        [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)\n",
    "    \n",
    "    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")\n",
    "     \n",
    "    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")\n",
    "    \n",
    "    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n",
    "    \n",
    "    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n",
    "    \n",
    "    \n",
    "    return G\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6027823d-ca18-439f-9bea-54460f7d7e70",
   "metadata": {},
   "source": [
    "- 판매자, 고객, 거래에 노드 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2be1fed8-bc99-46cd-8946-78e5edffb031",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_ti_8 = build_graph_tripartite(df_train8, nx.Graph(name=\"Tripartite Undirect\"))\n",
    "G_ti_7 = build_graph_tripartite(df_train7, nx.Graph(name=\"Tripartite Undirect\"))\n",
    "G_ti_6 = build_graph_tripartite(df_train6, nx.Graph(name=\"Tripartite Undirect\"))\n",
    "G_ti_5 = build_graph_tripartite(df_train5, nx.Graph(name=\"Tripartite Undirect\"))\n",
    "G_ti_4 = build_graph_tripartite(df_train4, nx.Graph(name=\"Tripartite Undirect\"))\n",
    "G_ti_3 = build_graph_tripartite(df_train3, nx.Graph(name=\"Tripartite Undirect\"))\n",
    "G_ti_2 = build_graph_tripartite(df_train2, nx.Graph(name=\"Tripartite Undirect\"))\n",
    "G_ti_1 = build_graph_tripartite(df_train1, nx.Graph(name=\"Tripartite Undirect\"))\n",
    "G_ti_test = build_graph_tripartite(df_test, nx.Graph(name=\"Tripartite Undirect\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "381d8fb4-2c79-4683-9778-2287093d86f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0x7f91b62b31c0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_ti_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1907003-405e-42cd-b957-8acdc3f2baa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_node_classification(G_train, G_test, embedding_dimension=128, random_state=42):\n",
    "    # Ensure the same set of nodes for both train and test graphs\n",
    "    common_nodes = set(G_train.nodes).intersection(G_test.nodes)\n",
    "    G_train = G_train.subgraph(common_nodes).copy()\n",
    "    G_test = G_test.subgraph(common_nodes).copy()\n",
    "\n",
    "    node2vec_train = Node2Vec(G_train, dimensions=embedding_dimension, weight_key='weight')\n",
    "    model_train = node2vec_train.fit(window=10)\n",
    "    \n",
    "    classes = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\n",
    "    evaluation_results = {}\n",
    "    \n",
    "    train_labels = [G_train.edges[edge]['label'] for edge in G_train.edges]\n",
    "    y_test =  list(nx.get_edge_attributes(G_test, \"label\").values())\n",
    "    \n",
    "    for cl in classes:\n",
    "        embeddings_train = cl(keyed_vectors=model_train.wv)\n",
    "\n",
    "        train_embeddings = [embeddings_train[str(edge[0]), str(edge[1])] for edge in G_train.edges]\n",
    "        test_embeddings = [embeddings_train[str(edge[0]), str(edge[1])] for edge in G_test.edges]\n",
    "\n",
    "        rf = RandomForestClassifier(n_estimators=1000, random_state=random_state)\n",
    "        rf.fit(train_embeddings, train_labels)\n",
    "\n",
    "        yhat = rf.predict(test_embeddings)\n",
    "        acc = metrics.accuracy_score(y_test, yhat)\n",
    "        pre = metrics.precision_score(y_test, yhat)\n",
    "        rec = metrics.recall_score(y_test, yhat)\n",
    "        f1 = metrics.f1_score(y_test, yhat)\n",
    "        auc = metrics.roc_auc_score(y_test, yhat)\n",
    "        \n",
    "        evaluation_results[cl.__name__] = {\"accuracy\": acc, \"precision\": pre, \"recall\": rec, \"f1-score\": f1, \"auc\": auc}\n",
    "\n",
    "    return evaluation_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73cd268c-c32a-4607-a0eb-8e70cff8666c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e59250f7534d4ad1a8947bb1bed451c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/10021 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|██████████| 10/10 [00:18<00:00,  1.90s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluate_node_classification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG_ti_8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG_ti_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 25\u001b[0m, in \u001b[0;36mevaluate_node_classification\u001b[0;34m(G_train, G_test, embedding_dimension, random_state)\u001b[0m\n\u001b[1;32m     22\u001b[0m rf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m     23\u001b[0m rf\u001b[38;5;241m.\u001b[39mfit(train_embeddings, train_labels)\n\u001b[0;32m---> 25\u001b[0m yhat \u001b[38;5;241m=\u001b[39m \u001b[43mrf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m acc \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39maccuracy_score(y_test, yhat)\n\u001b[1;32m     27\u001b[0m pre \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39mprecision_score(y_test, yhat)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:820\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    800\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;124;03m    Predict class for X.\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;124;03m        The predicted classes.\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 820\u001b[0m     proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    822\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    823\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39margmax(proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:862\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    860\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    861\u001b[0m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[0;32m--> 862\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[1;32m    865\u001b[0m n_jobs, _, _ \u001b[38;5;241m=\u001b[39m _partition_estimators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:602\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;124;03mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001b[39;00m\n\u001b[1;32m    601\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 602\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc):\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 565\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/utils/validation.py:902\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    900\u001b[0m     \u001b[38;5;66;03m# If input is 1D raise error\u001b[39;00m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 902\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    903\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    904\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    905\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    906\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m    907\u001b[0m         )\n\u001b[1;32m    909\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    912\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    913\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "evaluate_node_classification(G_ti_8, G_ti_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4501b4ee-835b-41d9-8fc9-5270bc1f7f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a72f246-5dec-45e6-abeb-d1e589475f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.time()\n",
    "evaluate_node_classification(G_ti_7, G_ti_test)\n",
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffee1761-39e5-4028-88b2-92f9b351258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.time()\n",
    "evaluate_node_classification(G_ti_6, G_ti_test)\n",
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911f1fb6-6dfa-414c-91e6-08a379d73a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.time()\n",
    "evaluate_node_classification(G_ti_5, G_ti_test)\n",
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a982428d-6c6c-4109-bd5e-0bb7de386092",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.time()\n",
    "evaluate_node_classification(G_ti_4, G_ti_test)\n",
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0525ba-e575-4be7-b08e-cb44e4bc08ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.time()\n",
    "evaluate_node_classification(G_ti_3, G_ti_test)\n",
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09058ac-4e76-41a4-928c-5f332f6edcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.time()\n",
    "evaluate_node_classification(G_ti_2, G_ti_test)\n",
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0e50e2-02b6-4165-95c7-2d1bcfaa011c",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.time()\n",
    "evaluate_node_classification(G_ti_1, G_ti_test)\n",
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299103c8-4dcf-4405-9b29-6f9b4ed31fac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
