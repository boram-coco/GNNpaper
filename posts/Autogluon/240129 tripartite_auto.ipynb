{
 "cells": [
  {
   "cell_type": "raw",
   "id": "c8593b37-a6e6-4ccb-b9b2-349f55cb2b69",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"[Autogluon] tripartite\"\n",
    "author: \"김보람\"\n",
    "date: \"01/27/2024\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e03341d-db6a-473a-91fa-7d878f69f879",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a397e5a9-c47a-4db7-b73f-bc9a2d965f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from node2vec import Node2Vec\n",
    "from node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n",
    "\n",
    "# sklearn\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "from node2vec import Node2Vec\n",
    "from node2vec.edges import AverageEmbedder\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd595016-eff7-4fa7-b416-ee5f7071b200",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def throw(df, fraud_rate):  # 사기 거래 비율에 맞춰 버려지는 함수!\n",
    "        df1 = df[df['is_fraud'] == 1].copy()\n",
    "        df0 = df[df['is_fraud'] == 0].copy()\n",
    "        df0_downsample = (len(df1) * (1-fraud_rate)) / (len(df0) * fraud_rate)\n",
    "        df0_down = df0.sample(frac=df0_downsample, random_state=42)\n",
    "        df_p = pd.concat([df1, df0_down])\n",
    "        return df_p\n",
    "    \n",
    "    def split_dataframe(data_frame, test_fraud_rate, test_rate=0.3):\n",
    "        n = len(data_frame)\n",
    "    \n",
    "        # 사기 거래와 정상 거래를 분리\n",
    "        fraud_data = data_frame[data_frame['is_fraud'] == 1]\n",
    "        normal_data = data_frame[data_frame['is_fraud'] == 0]\n",
    "\n",
    "        # 테스트 데이터 크기 계산\n",
    "        test_samples = int(test_fraud_rate * (n * test_rate))\n",
    "        remaining_test_samples = int(n * test_rate) - test_samples\n",
    "    \n",
    "        # 사기 거래 및 정상 거래에서 무작위로 테스트 데이터 추출\n",
    "        test_fraud_data = fraud_data.sample(n=test_samples, replace=False)\n",
    "        test_normal_data = normal_data.sample(n=remaining_test_samples, replace=False)\n",
    "\n",
    "        # 테스트 데이터 합치기\n",
    "        test_data = pd.concat([test_normal_data, test_fraud_data])\n",
    "\n",
    "        # 훈련 데이터 생성\n",
    "        train_data = data_frame[~data_frame.index.isin(test_data.index)]\n",
    "\n",
    "        return train_data, test_data\n",
    "    \n",
    "    def concat(df_tr, df_tst):   \n",
    "        df = pd.concat([df_tr, df_tst])\n",
    "        train_mask = np.concatenate((np.full(len(df_tr), True), np.full(len(df_tst), False)))    # index꼬이는거 방지하기 위해서? ★ (이거,, 훔,,?(\n",
    "        test_mask =  np.concatenate((np.full(len(df_tr), False), np.full(len(df_tst), True))) \n",
    "        mask = (train_mask, test_mask)\n",
    "        return df, mask\n",
    "        \n",
    "    def evaluation(y, yhat):\n",
    "        metrics = [sklearn.metrics.accuracy_score,\n",
    "                   sklearn.metrics.precision_score,\n",
    "                   sklearn.metrics.recall_score,\n",
    "                   sklearn.metrics.f1_score,\n",
    "                   sklearn.metrics.roc_auc_score]\n",
    "        return pd.DataFrame({m.__name__:[m(y,yhat).round(6)] for m in metrics})\n",
    "        \n",
    "    def compute_time_difference(group):\n",
    "        n = len(group)\n",
    "        result = []\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                time_difference = abs((group.iloc[i].trans_date_trans_time - group.iloc[j].trans_date_trans_time).total_seconds())\n",
    "                result.append([group.iloc[i].name, group.iloc[j].name, time_difference])\n",
    "        return result\n",
    "\n",
    "    def edge_index_save(df, unique_col, theta, gamma):\n",
    "        groups = df.groupby(unique_col)\n",
    "        edge_index = np.array([item for sublist in (compute_time_difference(group) for _, group in groups) for item in sublist])\n",
    "        edge_index = edge_index.astype(np.float64)\n",
    "        filename = f\"edge_index_attempt{self.save_attempt}_{str(unique_col).replace(' ', '').replace('_', '')}.npy\"\n",
    "        \n",
    "        while os.path.exists(filename):\n",
    "            self.save_attempt += 1\n",
    "            filename = f\"edge_index_attempt{self.save_attempt}_{str(unique_col).replace(' ', '').replace('_', '')}.npy\"\n",
    "        np.save(filename, edge_index)\n",
    "        #tetha = edge_index_plust_itme[:,].mean()\n",
    "    \n",
    "        \n",
    "        edge_index[:,2] = (np.exp(-edge_index[:,2]/(theta)) != 1)*(np.exp(-edge_index[:,2]/(theta))).tolist()\n",
    "        edge_index = torch.tensor([(int(row[0]), int(row[1])) for row in edge_index if row[2] > gamma], dtype=torch.long).t()\n",
    "        return edge_index\n",
    "    \n",
    "    def edge_index(df, unique_col, theta, gamma):\n",
    "        groups = df.groupby(unique_col)\n",
    "        edge_index = np.array([item for sublist in (compute_time_difference(group) for _, group in groups) for item in sublist])\n",
    "        edge_index = edge_index.astype(np.float64)\n",
    "       # filename = f\"edge_index_attempt{self.save_attempt}_{str(unique_col).replace(' ', '').replace('_', '')}.npy\"\n",
    "        \n",
    "        # while os.path.exists(filename):\n",
    "        #     self.save_attempt += 1\n",
    "        #     filename = f\"edge_index_attempt{self.save_attempt}_{str(unique_col).replace(' ', '').replace('_', '')}.npy\"\n",
    "        # np.save(filename, edge_index)\n",
    "        #tetha = edge_index_plust_itme[:,].mean()\n",
    "    \n",
    "        \n",
    "        edge_index[:,2] = (np.exp(-edge_index[:,2]/(theta)) != 1)*(np.exp(-edge_index[:,2]/(theta))).tolist()\n",
    "        edge_index = torch.tensor([(int(row[0]), int(row[1])) for row in edge_index if row[2] > gamma], dtype=torch.long).t()\n",
    "        return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edd7fc11-5bd3-4f15-a76b-5bd25039d058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_tripartite(df_input, graph_type=nx.Graph()):\n",
    "    df=df_input.copy()\n",
    "    mapping={x:node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n",
    "                                                       df[\"cc_num\"].values.tolist() +\n",
    "                                                       df[\"merchant\"].values.tolist()))}\n",
    "    df[\"in_node\"]= df[\"cc_num\"].apply(lambda x: mapping[x])\n",
    "    df[\"out_node\"]=df[\"merchant\"].apply(lambda x:mapping[x])\n",
    "    \n",
    "        \n",
    "    G=nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\\\n",
    "                        [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)\n",
    "    \n",
    "    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")\n",
    "     \n",
    "    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")\n",
    "    \n",
    "    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n",
    "    \n",
    "    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n",
    "    \n",
    "    \n",
    "    return G\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "167a4fe6-4f40-4536-b4b5-1b000d7e5f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\n",
    "fraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552a6fef-5587-42d3-91c8-14a972055459",
   "metadata": {},
   "source": [
    "# (throw 0.3 /split 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "521ef51e-b176-4ad6-86e8-d56760eae0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = throw(fraudTrain, 0.3)\n",
    "df_tr, df_tst = split_dataframe(df, 0.05)\n",
    "df2, mask = concat(df_tr, df_tst)\n",
    "df2['index'] = df2.index\n",
    "df3 = df2.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5009e55c-ddce-4e97-88be-4f51f5024fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_down = build_graph_tripartite(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9cbb281-384e-4e9b-8fdf-8ddf6f0bd25e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 40040)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(len(G_down.edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd4d688a-3112-40c9-9f78-afb58275427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_edges, test_edges, train_labels, test_labels = train_test_split(list(range(len(G_down.edges))), \n",
    "                                                                      list(nx.get_edge_attributes(G_down, \"label\").values()), \n",
    "                                                                      test_size=0.30, \n",
    "                                                                      random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfe29674-c331-4333-94e4-ae4b9bfcb532",
   "metadata": {},
   "outputs": [],
   "source": [
    "edgs = list(G_down.edges)\n",
    "train_graph = G_down.edge_subgraph([edgs[x] for x in train_edges]).copy()\n",
    "train_graph.add_nodes_from(list(set(G_down.nodes) - set(train_graph.nodes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6062b793-e4ec-4cb9-ab77-2a5c05af9456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a94c9a439742d09f83d33cd2ed39d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/21656 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating walks (CPU: 1): 100%|██████████| 10/10 [01:02<00:00,  6.20s/it]\n"
     ]
    }
   ],
   "source": [
    "node2vec_train = Node2Vec(train_graph, weight_key='weight')\n",
    "model_train = node2vec_train.fit(window=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222d7680-d3a4-4aa6-80af-4b4fb1f42443",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2862e87a-f4c5-4243-9753-3cae800c79e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_train = AverageEmbedder(keyed_vectors=model_train.wv) \n",
    "train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n",
    "test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10618caf-0acc-4589-b271-bf8f0d48ebc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28028, 128)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_embeddings).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "232a78d5-5606-48e3-8e35-d93d66637f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40040, 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(edgs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "561c12e0-34b9-495b-97f3-983d5275ef0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28028,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a041121-9744-493a-84df-3b64370a5185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_0</th>\n",
       "      <th>X_1</th>\n",
       "      <th>X_2</th>\n",
       "      <th>X_3</th>\n",
       "      <th>X_4</th>\n",
       "      <th>X_5</th>\n",
       "      <th>X_6</th>\n",
       "      <th>X_7</th>\n",
       "      <th>X_8</th>\n",
       "      <th>X_9</th>\n",
       "      <th>...</th>\n",
       "      <th>X_118</th>\n",
       "      <th>X_119</th>\n",
       "      <th>X_120</th>\n",
       "      <th>X_121</th>\n",
       "      <th>X_122</th>\n",
       "      <th>X_123</th>\n",
       "      <th>X_124</th>\n",
       "      <th>X_125</th>\n",
       "      <th>X_126</th>\n",
       "      <th>X_127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.294095</td>\n",
       "      <td>-0.182587</td>\n",
       "      <td>0.247678</td>\n",
       "      <td>0.590049</td>\n",
       "      <td>0.006470</td>\n",
       "      <td>-0.142112</td>\n",
       "      <td>0.089056</td>\n",
       "      <td>-0.001260</td>\n",
       "      <td>0.094326</td>\n",
       "      <td>0.135021</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069856</td>\n",
       "      <td>0.066224</td>\n",
       "      <td>0.115723</td>\n",
       "      <td>0.147979</td>\n",
       "      <td>-0.197021</td>\n",
       "      <td>-0.095803</td>\n",
       "      <td>-0.337333</td>\n",
       "      <td>-0.125295</td>\n",
       "      <td>-0.009825</td>\n",
       "      <td>0.151540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.272624</td>\n",
       "      <td>0.151767</td>\n",
       "      <td>-0.154251</td>\n",
       "      <td>0.323210</td>\n",
       "      <td>0.926657</td>\n",
       "      <td>-0.067704</td>\n",
       "      <td>0.017721</td>\n",
       "      <td>-0.107441</td>\n",
       "      <td>0.145443</td>\n",
       "      <td>0.302530</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003853</td>\n",
       "      <td>-0.062410</td>\n",
       "      <td>0.533643</td>\n",
       "      <td>-0.343930</td>\n",
       "      <td>-0.080773</td>\n",
       "      <td>0.252463</td>\n",
       "      <td>-0.586127</td>\n",
       "      <td>-0.611270</td>\n",
       "      <td>0.173824</td>\n",
       "      <td>-0.255820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.285497</td>\n",
       "      <td>-0.727879</td>\n",
       "      <td>0.088415</td>\n",
       "      <td>0.351048</td>\n",
       "      <td>0.179241</td>\n",
       "      <td>-0.211548</td>\n",
       "      <td>0.396390</td>\n",
       "      <td>0.184747</td>\n",
       "      <td>0.019989</td>\n",
       "      <td>0.056321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.329557</td>\n",
       "      <td>-0.137155</td>\n",
       "      <td>-0.492804</td>\n",
       "      <td>-0.395840</td>\n",
       "      <td>-0.124599</td>\n",
       "      <td>0.071007</td>\n",
       "      <td>0.092745</td>\n",
       "      <td>0.113933</td>\n",
       "      <td>0.531759</td>\n",
       "      <td>0.185540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.101396</td>\n",
       "      <td>-0.118513</td>\n",
       "      <td>0.023900</td>\n",
       "      <td>0.479202</td>\n",
       "      <td>0.010183</td>\n",
       "      <td>0.010264</td>\n",
       "      <td>0.090974</td>\n",
       "      <td>-0.374058</td>\n",
       "      <td>-0.035802</td>\n",
       "      <td>0.005411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.445272</td>\n",
       "      <td>0.161917</td>\n",
       "      <td>0.132266</td>\n",
       "      <td>-0.353145</td>\n",
       "      <td>0.408945</td>\n",
       "      <td>0.278389</td>\n",
       "      <td>-0.149341</td>\n",
       "      <td>-0.426027</td>\n",
       "      <td>0.157640</td>\n",
       "      <td>0.036185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.335867</td>\n",
       "      <td>0.193802</td>\n",
       "      <td>0.017849</td>\n",
       "      <td>-0.094391</td>\n",
       "      <td>0.406467</td>\n",
       "      <td>-0.252247</td>\n",
       "      <td>0.253800</td>\n",
       "      <td>0.060329</td>\n",
       "      <td>-0.227593</td>\n",
       "      <td>0.060363</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042921</td>\n",
       "      <td>-0.296897</td>\n",
       "      <td>-0.108547</td>\n",
       "      <td>-0.168377</td>\n",
       "      <td>0.239888</td>\n",
       "      <td>0.338059</td>\n",
       "      <td>-0.371277</td>\n",
       "      <td>0.179102</td>\n",
       "      <td>-0.186001</td>\n",
       "      <td>0.128560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12007</th>\n",
       "      <td>0.042202</td>\n",
       "      <td>-0.018433</td>\n",
       "      <td>0.708589</td>\n",
       "      <td>-0.087046</td>\n",
       "      <td>0.239773</td>\n",
       "      <td>-0.049197</td>\n",
       "      <td>0.437209</td>\n",
       "      <td>0.417002</td>\n",
       "      <td>-0.341246</td>\n",
       "      <td>0.192022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.255594</td>\n",
       "      <td>0.440218</td>\n",
       "      <td>-0.155139</td>\n",
       "      <td>-0.615026</td>\n",
       "      <td>0.221697</td>\n",
       "      <td>0.187364</td>\n",
       "      <td>0.244558</td>\n",
       "      <td>-0.492887</td>\n",
       "      <td>0.498050</td>\n",
       "      <td>0.081703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12008</th>\n",
       "      <td>-0.265624</td>\n",
       "      <td>-0.221091</td>\n",
       "      <td>0.016436</td>\n",
       "      <td>-0.082620</td>\n",
       "      <td>0.160059</td>\n",
       "      <td>-0.097688</td>\n",
       "      <td>0.372506</td>\n",
       "      <td>0.080932</td>\n",
       "      <td>-0.232221</td>\n",
       "      <td>0.103701</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106539</td>\n",
       "      <td>0.294270</td>\n",
       "      <td>-0.140846</td>\n",
       "      <td>0.088624</td>\n",
       "      <td>-0.184099</td>\n",
       "      <td>0.296281</td>\n",
       "      <td>-0.292350</td>\n",
       "      <td>0.041315</td>\n",
       "      <td>0.421299</td>\n",
       "      <td>-0.014916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12009</th>\n",
       "      <td>-0.040139</td>\n",
       "      <td>0.002283</td>\n",
       "      <td>-0.052671</td>\n",
       "      <td>0.268232</td>\n",
       "      <td>0.253181</td>\n",
       "      <td>0.208248</td>\n",
       "      <td>0.081583</td>\n",
       "      <td>0.019004</td>\n",
       "      <td>-0.129444</td>\n",
       "      <td>0.090210</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.112681</td>\n",
       "      <td>-0.216101</td>\n",
       "      <td>-0.273943</td>\n",
       "      <td>-0.158223</td>\n",
       "      <td>0.288049</td>\n",
       "      <td>0.343018</td>\n",
       "      <td>0.022950</td>\n",
       "      <td>-0.497030</td>\n",
       "      <td>0.158982</td>\n",
       "      <td>-0.103655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12010</th>\n",
       "      <td>0.108170</td>\n",
       "      <td>-0.334442</td>\n",
       "      <td>-0.155009</td>\n",
       "      <td>-0.043705</td>\n",
       "      <td>0.576253</td>\n",
       "      <td>-0.157641</td>\n",
       "      <td>-0.352357</td>\n",
       "      <td>-0.275126</td>\n",
       "      <td>-0.084098</td>\n",
       "      <td>-0.305660</td>\n",
       "      <td>...</td>\n",
       "      <td>0.417730</td>\n",
       "      <td>-0.285950</td>\n",
       "      <td>-0.008567</td>\n",
       "      <td>-0.083101</td>\n",
       "      <td>0.029371</td>\n",
       "      <td>0.271623</td>\n",
       "      <td>-0.458983</td>\n",
       "      <td>0.137005</td>\n",
       "      <td>-0.205865</td>\n",
       "      <td>0.410773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12011</th>\n",
       "      <td>-0.267466</td>\n",
       "      <td>0.094266</td>\n",
       "      <td>0.158908</td>\n",
       "      <td>0.479838</td>\n",
       "      <td>0.401144</td>\n",
       "      <td>-0.120019</td>\n",
       "      <td>0.683475</td>\n",
       "      <td>0.051481</td>\n",
       "      <td>-0.131274</td>\n",
       "      <td>0.665234</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423447</td>\n",
       "      <td>0.033082</td>\n",
       "      <td>-0.014823</td>\n",
       "      <td>-0.390924</td>\n",
       "      <td>0.166589</td>\n",
       "      <td>-0.030113</td>\n",
       "      <td>-0.244755</td>\n",
       "      <td>0.128242</td>\n",
       "      <td>-0.013727</td>\n",
       "      <td>0.101052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12012 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            X_0       X_1       X_2       X_3       X_4       X_5       X_6  \\\n",
       "0      0.294095 -0.182587  0.247678  0.590049  0.006470 -0.142112  0.089056   \n",
       "1     -0.272624  0.151767 -0.154251  0.323210  0.926657 -0.067704  0.017721   \n",
       "2     -0.285497 -0.727879  0.088415  0.351048  0.179241 -0.211548  0.396390   \n",
       "3      0.101396 -0.118513  0.023900  0.479202  0.010183  0.010264  0.090974   \n",
       "4      0.335867  0.193802  0.017849 -0.094391  0.406467 -0.252247  0.253800   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "12007  0.042202 -0.018433  0.708589 -0.087046  0.239773 -0.049197  0.437209   \n",
       "12008 -0.265624 -0.221091  0.016436 -0.082620  0.160059 -0.097688  0.372506   \n",
       "12009 -0.040139  0.002283 -0.052671  0.268232  0.253181  0.208248  0.081583   \n",
       "12010  0.108170 -0.334442 -0.155009 -0.043705  0.576253 -0.157641 -0.352357   \n",
       "12011 -0.267466  0.094266  0.158908  0.479838  0.401144 -0.120019  0.683475   \n",
       "\n",
       "            X_7       X_8       X_9  ...     X_118     X_119     X_120  \\\n",
       "0     -0.001260  0.094326  0.135021  ...  0.069856  0.066224  0.115723   \n",
       "1     -0.107441  0.145443  0.302530  ...  0.003853 -0.062410  0.533643   \n",
       "2      0.184747  0.019989  0.056321  ...  0.329557 -0.137155 -0.492804   \n",
       "3     -0.374058 -0.035802  0.005411  ...  0.445272  0.161917  0.132266   \n",
       "4      0.060329 -0.227593  0.060363  ... -0.042921 -0.296897 -0.108547   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "12007  0.417002 -0.341246  0.192022  ...  0.255594  0.440218 -0.155139   \n",
       "12008  0.080932 -0.232221  0.103701  ...  0.106539  0.294270 -0.140846   \n",
       "12009  0.019004 -0.129444  0.090210  ... -0.112681 -0.216101 -0.273943   \n",
       "12010 -0.275126 -0.084098 -0.305660  ...  0.417730 -0.285950 -0.008567   \n",
       "12011  0.051481 -0.131274  0.665234  ...  0.423447  0.033082 -0.014823   \n",
       "\n",
       "          X_121     X_122     X_123     X_124     X_125     X_126     X_127  \n",
       "0      0.147979 -0.197021 -0.095803 -0.337333 -0.125295 -0.009825  0.151540  \n",
       "1     -0.343930 -0.080773  0.252463 -0.586127 -0.611270  0.173824 -0.255820  \n",
       "2     -0.395840 -0.124599  0.071007  0.092745  0.113933  0.531759  0.185540  \n",
       "3     -0.353145  0.408945  0.278389 -0.149341 -0.426027  0.157640  0.036185  \n",
       "4     -0.168377  0.239888  0.338059 -0.371277  0.179102 -0.186001  0.128560  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "12007 -0.615026  0.221697  0.187364  0.244558 -0.492887  0.498050  0.081703  \n",
       "12008  0.088624 -0.184099  0.296281 -0.292350  0.041315  0.421299 -0.014916  \n",
       "12009 -0.158223  0.288049  0.343018  0.022950 -0.497030  0.158982 -0.103655  \n",
       "12010 -0.083101  0.029371  0.271623 -0.458983  0.137005 -0.205865  0.410773  \n",
       "12011 -0.390924  0.166589 -0.030113 -0.244755  0.128242 -0.013727  0.101052  \n",
       "\n",
       "[12012 rows x 128 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08b5173f-3d18-4491-be7c-8d08961113a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30128205128205127"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7d713a9c-cfa1-4196-b425-f063454bf771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02355977355977356"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5e25fd79-73c1-4406-96f9-dad016b867a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_0</th>\n",
       "      <th>X_1</th>\n",
       "      <th>X_2</th>\n",
       "      <th>X_3</th>\n",
       "      <th>X_4</th>\n",
       "      <th>X_5</th>\n",
       "      <th>X_6</th>\n",
       "      <th>X_7</th>\n",
       "      <th>X_8</th>\n",
       "      <th>X_9</th>\n",
       "      <th>...</th>\n",
       "      <th>X_119</th>\n",
       "      <th>X_120</th>\n",
       "      <th>X_121</th>\n",
       "      <th>X_122</th>\n",
       "      <th>X_123</th>\n",
       "      <th>X_124</th>\n",
       "      <th>X_125</th>\n",
       "      <th>X_126</th>\n",
       "      <th>X_127</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.018232</td>\n",
       "      <td>-0.461368</td>\n",
       "      <td>0.364313</td>\n",
       "      <td>-0.003355</td>\n",
       "      <td>0.447681</td>\n",
       "      <td>0.124173</td>\n",
       "      <td>-0.003152</td>\n",
       "      <td>-0.540774</td>\n",
       "      <td>-0.572652</td>\n",
       "      <td>0.302768</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.409031</td>\n",
       "      <td>0.276566</td>\n",
       "      <td>-0.532460</td>\n",
       "      <td>0.310427</td>\n",
       "      <td>-0.138094</td>\n",
       "      <td>-0.232335</td>\n",
       "      <td>-0.699818</td>\n",
       "      <td>0.409554</td>\n",
       "      <td>0.513786</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.034503</td>\n",
       "      <td>-0.591312</td>\n",
       "      <td>0.221111</td>\n",
       "      <td>0.221223</td>\n",
       "      <td>0.456697</td>\n",
       "      <td>0.206631</td>\n",
       "      <td>0.071338</td>\n",
       "      <td>-0.419212</td>\n",
       "      <td>-0.801116</td>\n",
       "      <td>0.093788</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.397874</td>\n",
       "      <td>-0.322119</td>\n",
       "      <td>-0.167821</td>\n",
       "      <td>-0.249518</td>\n",
       "      <td>0.307504</td>\n",
       "      <td>-0.631245</td>\n",
       "      <td>-0.300779</td>\n",
       "      <td>0.297532</td>\n",
       "      <td>-0.434563</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.310014</td>\n",
       "      <td>-0.523434</td>\n",
       "      <td>-0.253751</td>\n",
       "      <td>0.291488</td>\n",
       "      <td>-0.216207</td>\n",
       "      <td>-0.602615</td>\n",
       "      <td>0.851062</td>\n",
       "      <td>0.502921</td>\n",
       "      <td>-0.013592</td>\n",
       "      <td>-0.049582</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.690101</td>\n",
       "      <td>0.033092</td>\n",
       "      <td>-0.025023</td>\n",
       "      <td>0.303007</td>\n",
       "      <td>-0.769665</td>\n",
       "      <td>0.134720</td>\n",
       "      <td>0.556271</td>\n",
       "      <td>0.101577</td>\n",
       "      <td>0.475444</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.491736</td>\n",
       "      <td>-0.378155</td>\n",
       "      <td>-0.069874</td>\n",
       "      <td>-0.418389</td>\n",
       "      <td>0.708743</td>\n",
       "      <td>-0.091546</td>\n",
       "      <td>0.106801</td>\n",
       "      <td>-0.590884</td>\n",
       "      <td>0.115419</td>\n",
       "      <td>0.375492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555193</td>\n",
       "      <td>-0.788328</td>\n",
       "      <td>-0.293919</td>\n",
       "      <td>0.354652</td>\n",
       "      <td>-0.004965</td>\n",
       "      <td>-0.360878</td>\n",
       "      <td>-0.673302</td>\n",
       "      <td>0.952318</td>\n",
       "      <td>-0.829513</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.509173</td>\n",
       "      <td>-0.160158</td>\n",
       "      <td>0.732356</td>\n",
       "      <td>0.799263</td>\n",
       "      <td>0.406524</td>\n",
       "      <td>0.346506</td>\n",
       "      <td>0.278970</td>\n",
       "      <td>0.135962</td>\n",
       "      <td>-0.022850</td>\n",
       "      <td>0.728933</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.419749</td>\n",
       "      <td>-0.317320</td>\n",
       "      <td>-0.089520</td>\n",
       "      <td>0.018165</td>\n",
       "      <td>0.111126</td>\n",
       "      <td>0.156540</td>\n",
       "      <td>-0.480898</td>\n",
       "      <td>0.906297</td>\n",
       "      <td>0.375273</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28023</th>\n",
       "      <td>-1.031186</td>\n",
       "      <td>-0.789458</td>\n",
       "      <td>-0.395662</td>\n",
       "      <td>0.672432</td>\n",
       "      <td>0.366671</td>\n",
       "      <td>-0.042877</td>\n",
       "      <td>0.584821</td>\n",
       "      <td>-1.050437</td>\n",
       "      <td>-0.178776</td>\n",
       "      <td>1.148389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035637</td>\n",
       "      <td>-0.534154</td>\n",
       "      <td>-0.465015</td>\n",
       "      <td>-0.292141</td>\n",
       "      <td>-0.529951</td>\n",
       "      <td>0.083408</td>\n",
       "      <td>-0.534623</td>\n",
       "      <td>-0.033971</td>\n",
       "      <td>-0.392283</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28024</th>\n",
       "      <td>0.582753</td>\n",
       "      <td>-0.374445</td>\n",
       "      <td>0.245931</td>\n",
       "      <td>-0.342081</td>\n",
       "      <td>0.360757</td>\n",
       "      <td>-0.232758</td>\n",
       "      <td>0.543620</td>\n",
       "      <td>0.188705</td>\n",
       "      <td>0.140474</td>\n",
       "      <td>0.003210</td>\n",
       "      <td>...</td>\n",
       "      <td>0.469309</td>\n",
       "      <td>-0.733129</td>\n",
       "      <td>-0.328261</td>\n",
       "      <td>0.185925</td>\n",
       "      <td>0.491741</td>\n",
       "      <td>-0.889476</td>\n",
       "      <td>-0.515644</td>\n",
       "      <td>0.718754</td>\n",
       "      <td>-0.470877</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28025</th>\n",
       "      <td>-0.073720</td>\n",
       "      <td>-0.696957</td>\n",
       "      <td>0.398181</td>\n",
       "      <td>0.148028</td>\n",
       "      <td>0.527938</td>\n",
       "      <td>0.282207</td>\n",
       "      <td>0.083161</td>\n",
       "      <td>0.223829</td>\n",
       "      <td>0.746264</td>\n",
       "      <td>-0.078440</td>\n",
       "      <td>...</td>\n",
       "      <td>0.403734</td>\n",
       "      <td>-0.565365</td>\n",
       "      <td>-0.134230</td>\n",
       "      <td>0.076866</td>\n",
       "      <td>-0.147996</td>\n",
       "      <td>-0.263811</td>\n",
       "      <td>-0.342502</td>\n",
       "      <td>0.232086</td>\n",
       "      <td>0.396120</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28026</th>\n",
       "      <td>0.198808</td>\n",
       "      <td>-0.342205</td>\n",
       "      <td>-0.171712</td>\n",
       "      <td>-0.427701</td>\n",
       "      <td>0.376993</td>\n",
       "      <td>-0.368098</td>\n",
       "      <td>0.268516</td>\n",
       "      <td>-0.081111</td>\n",
       "      <td>-0.124265</td>\n",
       "      <td>0.539024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.235571</td>\n",
       "      <td>0.433356</td>\n",
       "      <td>-0.304768</td>\n",
       "      <td>-0.276453</td>\n",
       "      <td>0.025376</td>\n",
       "      <td>-0.091774</td>\n",
       "      <td>-1.139718</td>\n",
       "      <td>-0.064087</td>\n",
       "      <td>-0.382336</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28027</th>\n",
       "      <td>-0.071535</td>\n",
       "      <td>-0.478938</td>\n",
       "      <td>0.958107</td>\n",
       "      <td>-0.103668</td>\n",
       "      <td>0.735266</td>\n",
       "      <td>0.264131</td>\n",
       "      <td>0.526417</td>\n",
       "      <td>-0.046119</td>\n",
       "      <td>0.337727</td>\n",
       "      <td>0.551102</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.100332</td>\n",
       "      <td>-0.451198</td>\n",
       "      <td>-0.238998</td>\n",
       "      <td>-0.315951</td>\n",
       "      <td>-0.040852</td>\n",
       "      <td>-0.560907</td>\n",
       "      <td>0.301089</td>\n",
       "      <td>-0.071838</td>\n",
       "      <td>0.057509</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28028 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            X_0       X_1       X_2       X_3       X_4       X_5       X_6  \\\n",
       "0     -0.018232 -0.461368  0.364313 -0.003355  0.447681  0.124173 -0.003152   \n",
       "1     -0.034503 -0.591312  0.221111  0.221223  0.456697  0.206631  0.071338   \n",
       "2     -0.310014 -0.523434 -0.253751  0.291488 -0.216207 -0.602615  0.851062   \n",
       "3     -0.491736 -0.378155 -0.069874 -0.418389  0.708743 -0.091546  0.106801   \n",
       "4     -0.509173 -0.160158  0.732356  0.799263  0.406524  0.346506  0.278970   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "28023 -1.031186 -0.789458 -0.395662  0.672432  0.366671 -0.042877  0.584821   \n",
       "28024  0.582753 -0.374445  0.245931 -0.342081  0.360757 -0.232758  0.543620   \n",
       "28025 -0.073720 -0.696957  0.398181  0.148028  0.527938  0.282207  0.083161   \n",
       "28026  0.198808 -0.342205 -0.171712 -0.427701  0.376993 -0.368098  0.268516   \n",
       "28027 -0.071535 -0.478938  0.958107 -0.103668  0.735266  0.264131  0.526417   \n",
       "\n",
       "            X_7       X_8       X_9  ...     X_119     X_120     X_121  \\\n",
       "0     -0.540774 -0.572652  0.302768  ... -0.409031  0.276566 -0.532460   \n",
       "1     -0.419212 -0.801116  0.093788  ... -0.397874 -0.322119 -0.167821   \n",
       "2      0.502921 -0.013592 -0.049582  ... -0.690101  0.033092 -0.025023   \n",
       "3     -0.590884  0.115419  0.375492  ...  0.555193 -0.788328 -0.293919   \n",
       "4      0.135962 -0.022850  0.728933  ... -0.419749 -0.317320 -0.089520   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "28023 -1.050437 -0.178776  1.148389  ...  0.035637 -0.534154 -0.465015   \n",
       "28024  0.188705  0.140474  0.003210  ...  0.469309 -0.733129 -0.328261   \n",
       "28025  0.223829  0.746264 -0.078440  ...  0.403734 -0.565365 -0.134230   \n",
       "28026 -0.081111 -0.124265  0.539024  ...  0.235571  0.433356 -0.304768   \n",
       "28027 -0.046119  0.337727  0.551102  ... -0.100332 -0.451198 -0.238998   \n",
       "\n",
       "          X_122     X_123     X_124     X_125     X_126     X_127  label  \n",
       "0      0.310427 -0.138094 -0.232335 -0.699818  0.409554  0.513786      1  \n",
       "1     -0.249518  0.307504 -0.631245 -0.300779  0.297532 -0.434563      0  \n",
       "2      0.303007 -0.769665  0.134720  0.556271  0.101577  0.475444      0  \n",
       "3      0.354652 -0.004965 -0.360878 -0.673302  0.952318 -0.829513      0  \n",
       "4      0.018165  0.111126  0.156540 -0.480898  0.906297  0.375273      0  \n",
       "...         ...       ...       ...       ...       ...       ...    ...  \n",
       "28023 -0.292141 -0.529951  0.083408 -0.534623 -0.033971 -0.392283      0  \n",
       "28024  0.185925  0.491741 -0.889476 -0.515644  0.718754 -0.470877      1  \n",
       "28025  0.076866 -0.147996 -0.263811 -0.342502  0.232086  0.396120      0  \n",
       "28026 -0.276453  0.025376 -0.091774 -1.139718 -0.064087 -0.382336      0  \n",
       "28027 -0.315951 -0.040852 -0.560907  0.301089 -0.071838  0.057509      1  \n",
       "\n",
       "[28028 rows x 129 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fbe11c2-48d5-402c-902d-230591d21f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240129_072006/\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240129_072006/\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #38~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov  2 18:01:13 UTC 2\n",
      "Disk Space Avail:   597.99 GB / 982.82 GB (60.8%)\n",
      "Train Data Rows:    28028\n",
      "Train Data Columns: 128\n",
      "Label Column: label\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [1, 0]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    5155.63 MB\n",
      "\tTrain Data (Original)  Memory Usage: 14.35 MB (0.3% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 128 | ['X_0', 'X_1', 'X_2', 'X_3', 'X_4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 128 | ['X_0', 'X_1', 'X_2', 'X_3', 'X_4', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t128 features in original data used to generate 128 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 14.35 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.31s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.08919651776794633, Train Rows: 25528, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "Exception ignored on calling ctypes callback function: <function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.<locals>.match_module_callback at 0x7f77c8288f70>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "\t0.7884\t = Validation score   (accuracy)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.72s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "Exception ignored on calling ctypes callback function: <function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.<locals>.match_module_callback at 0x7f77c8288f70>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n",
      "    self._make_module_from_path(filepath)\n",
      "  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n",
      "    module = module_class(filepath, prefix, user_api, internal_api)\n",
      "  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 606, in __init__\n",
      "    self.version = self.get_version()\n",
      "  File \"/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/threadpoolctl.py\", line 646, in get_version\n",
      "    config = get_config().split()\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n",
      "\t0.7908\t = Validation score   (accuracy)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.53s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\t0.8216\t = Validation score   (accuracy)\n",
      "\t5.64s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_error: 0.1784\n",
      "[2000]\tvalid_set's binary_error: 0.1736\n",
      "[3000]\tvalid_set's binary_error: 0.1736\n",
      "[4000]\tvalid_set's binary_error: 0.168\n",
      "[5000]\tvalid_set's binary_error: 0.1628\n",
      "[6000]\tvalid_set's binary_error: 0.1644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.8376\t = Validation score   (accuracy)\n",
      "\t35.19s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.8076\t = Validation score   (accuracy)\n",
      "\t13.37s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.808\t = Validation score   (accuracy)\n",
      "\t17.13s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t0.8228\t = Validation score   (accuracy)\n",
      "\t21.58s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.8024\t = Validation score   (accuracy)\n",
      "\t2.14s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.8032\t = Validation score   (accuracy)\n",
      "\t2.4s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t0.8508\t = Validation score   (accuracy)\n",
      "\t33.83s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.83\t = Validation score   (accuracy)\n",
      "\t16.11s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.8516\t = Validation score   (accuracy)\n",
      "\t53.0s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's binary_error: 0.1732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.8308\t = Validation score   (accuracy)\n",
      "\t29.28s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.8704\t = Validation score   (accuracy)\n",
      "\t0.99s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 234.95s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240129_072006/\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>roc_auc_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.711788</td>\n",
       "      <td>0.777385</td>\n",
       "      <td>0.06079</td>\n",
       "      <td>0.112763</td>\n",
       "      <td>0.526642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy_score  precision_score  recall_score  f1_score  roc_auc_score\n",
       "0        0.711788         0.777385       0.06079  0.112763       0.526642"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embeddings_train = AverageEmbedder(keyed_vectors=model_train.wv) \n",
    "# train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n",
    "# test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n",
    "\n",
    "\n",
    "# DataFrame 생성\n",
    "columns = [f'X_{i}' for i in range(np.array(train_embeddings).shape[1])]\n",
    "df_data = pd.DataFrame(data=train_embeddings, columns=columns)\n",
    "\n",
    "df_labels = pd.DataFrame(data=train_labels, columns=['label'])\n",
    "\n",
    "# DataFrame 합치기\n",
    "df = pd.concat([df_data, df_labels], axis=1)\n",
    "\n",
    "\n",
    "label = np.array(train_labels)\n",
    "\n",
    "predictr = TabularPredictor(label='label')\n",
    "\n",
    "predictr.fit(df) \n",
    "\n",
    "test = np.array(test_embeddings)\n",
    "\n",
    "columns = [f'X_{i}' for i in range(test.shape[1])]\n",
    "\n",
    "# DataFrame 생성\n",
    "test_df = pd.DataFrame(data=test, columns=columns)\n",
    "\n",
    "y = np.array(test_labels)\n",
    "\n",
    "yhat = predictr.predict(test_df)\n",
    "\n",
    "evaluation(y,yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d96706-6fd7-45e1-ae49-7513f28694e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
